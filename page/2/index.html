<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="google-site-verification" content="BTo06tdvlac_Dho4-PFTLmDqjKXr1KtOzavpD8XDA5k" />
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jiechu520.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="日常笔记">
<meta property="og:type" content="website">
<meta property="og:title" content="CJ blog">
<meta property="og:url" content="https://jiechu520.github.io/page/2/index.html">
<meta property="og:site_name" content="CJ blog">
<meta property="og:description" content="日常笔记">
<meta property="og:locale">
<meta property="article:author" content="Jie Chu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jiechu520.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CJ blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">CJ blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">不知名算法工程师</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jie Chu</p>
  <div class="site-description" itemprop="description">日常笔记</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2022/05/10/NIMA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/10/NIMA/" class="post-title-link" itemprop="url">NIMA——Neural Image Assessment</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-10 21:00:00" itemprop="dateCreated datePublished" datetime="2022-05-10T21:00:00+08:00">2022-05-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-05-11 23:08:00" itemprop="dateModified" datetime="2022-05-11T23:08:00+08:00">2022-05-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="NIMA：Neural-Image-Assessment"><a href="#NIMA：Neural-Image-Assessment" class="headerlink" title="NIMA：Neural Image Assessment"></a>NIMA：Neural Image Assessment</h1><h2 id="1、introduction"><a href="#1、introduction" class="headerlink" title="1、introduction"></a>1、introduction</h2><ol>
<li><p>基于CNN的方法相比较早期基于手工制作的特征的做法，性能有显著的提升；</p>
</li>
<li><p>AVA 数据集，图片审美视觉分析的标准数据集；</p>
</li>
<li><p>深度CNN非常适合审美评估任务【1】【2】；他们的双列 CNN 由四个卷积层和两个全连接层组成，其输入是调整大小的图像和大小为 224 × 224 的裁剪窗口；</p>
</li>
<li><p>之前大多是通过分类或者回归的方法预测人类评估的分数；</p>
</li>
<li><p>kong【3】训练了一个基于 AlexNet 的 CNN 来学习两个输入图像的审美分数差异，从而间接优化排名相关性。</p>
</li>
<li><p>NIMA的目标是预测与人类评分的相关性，而不是将图片分类或者回归到平均分。提出了EMD loss，它显示了有序类分类的性能提升。实验也表明，这种方法也更准确地预测了平均分。</p>
</li>
</ol>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h2 id="2、proposed-method"><a href="#2、proposed-method" class="headerlink" title="2、proposed method"></a>2、proposed method</h2><p>模型建立在图片分类模型的结构基础上。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026114849878.png" alt="image-20231026114849878"></p>
<p>在训练阶段，输入图像被重新缩放为256<em>256，然后随机提取 224\</em>224的裁剪，这减少潜在的过度拟合问题。</p>
<h2 id="3、实验"><a href="#3、实验" class="headerlink" title="3、实验"></a>3、实验</h2><p>基线CNN权重通过在ImageNet上训练来初始化，最后一个全连接层是随机初始化。权重和偏置动量设置为0.9,基线CNN最后一层dropout应用0.75。基线CNN层和最后一个全连接层的学习率分别设置为3<em>10e-7,3\</em>10e-6。在基线CNN层上设置较低的学习率会导致使用sgd时更容易和更快优化。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>【1】Deep multi-patch aggregation network for image style, aesthetics, and quality estimation</p>
<p>【2】Rating image aesthetics using deep learning</p>
<p>【3】S. Kong, X. Shen, Z. Lin, R. Mech, and C. Fowlkes, “Photo aesthetics ranking network with attributes and content adaptation,” in European Conference on Computer Vision. Springer, 2016, pp. 662–679. 1, 2, 6, 7</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2022/04/19/Vit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/19/Vit/" class="post-title-link" itemprop="url">VIT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2022-04-19 21:00:00 / Modified: 23:08:00" itemprop="dateCreated datePublished" datetime="2022-04-19T21:00:00+08:00">2022-04-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ViT-Vision-Transformer"><a href="#ViT-Vision-Transformer" class="headerlink" title="ViT(Vision Transformer)"></a>ViT(Vision Transformer)</h1><h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><p>受到NLP领域中Transformer成功应用的启发，Vit算法中尝试将标准的Transformer结构直接应用于图像，并对整个图像分类流程进行最少的修改。</p>
<p><strong>Vit原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破Transformer的归纳偏置的限制，可以在下游任务中获得较好的迁移效果。</strong></p>
<p>但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）， $f(g(x)=g(f(x)))$ ，其中g代表卷积操作，f代表平移操作。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。</p>
<h2 id="模型结构与实现"><a href="#模型结构与实现" class="headerlink" title="模型结构与实现"></a>模型结构与实现</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/ViT.png" alt="图1 ViT算法结构示意图"></p>
<h3 id="1-图像分块嵌入"><a href="#1-图像分块嵌入" class="headerlink" title="1.图像分块嵌入"></a>1.图像分块嵌入</h3><p>transformer的输入是一个二维矩阵，因此在ViT算法中，首先要做的是如何将一个$H\times W\times C$ 的三维图像转换为$N\times D$ 的二维输入。</p>
<p>iT中的具体实现方式为：将 $H×W×C$ 的图像，变为一个 $N×(P^2\times C)$ 的序列。这个序列可以看作是一系列展平的图像块，也就是将图像切分成小块后，再将其展平。该序列中一共包含了 $N=HW/P^2$ 个图像块，每个图像块的维度则是 $(P^2\times C)$。其中 $P$ 是图像块的大小，$C$ 是通道数量。经过如上变换，就可以将 $N$ 视为sequence的长度了。</p>
<p>但是，此时每个图像块的维度是 $(P2\times C)$，而我们实际需要的向量维度是 $D$，因此我们还需要对图像块进行 Embedding。这里 Embedding 的方式非常简单，只需要对每个 $(P^2\times C)$的图像块做一个线性变换，将维度压缩为 $D$ 即可。</p>
<h3 id="2-多头注意力"><a href="#2-多头注意力" class="headerlink" title="2.多头注意力"></a>2.多头注意力</h3><p>将图像转换成序列之后，就可以将其输入到 Transformer 中结构中进行特征提取了。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/Multi-head_Attention.jpg" alt="图4 多头注意力"></p>
<h3 id="3-MLP"><a href="#3-MLP" class="headerlink" title="3.MLP"></a>3.MLP</h3><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/MLP.png" alt="图7 多层感知机"></p>
<h3 id="4-DropPath"><a href="#4-DropPath" class="headerlink" title="4.DropPath"></a>4.DropPath</h3><p>除了以上重要模块意外，代码实现过程中还使用了DropPath（Stochastic Depth）来代替传统的Dropout结构，DropPath可以理解为一种特殊的 Dropout。其作用是在训练过程中随机丢弃子图层（randomly drop a subset of layers），而在预测时正常使用完整的 Graph。</p>
<p><strong>参考文献</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2022/04/11/resnet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/11/resnet/" class="post-title-link" itemprop="url">resnet解析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-04-11 21:03:00" itemprop="dateCreated datePublished" datetime="2022-04-11T21:03:00+08:00">2022-04-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-11 23:03:00" itemprop="dateModified" datetime="2023-04-11T23:03:00+08:00">2023-04-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="resnet"><a href="#resnet" class="headerlink" title="resnet"></a>resnet</h1><h3 id="1-1-batch-normalization原理"><a href="#1-1-batch-normalization原理" class="headerlink" title="1.1 batch normalization原理"></a>1.1 batch normalization原理</h3><p>提出背景：在训练层数较深的神经网络时，参数的变化导致每一层的输入分布会发生变化，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难——internal Covariate Shift</p>
<p>问题：上层网络需要不停调整来适应输入分布的变化，导致网络学习速度的降低；网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度。</p>
<p>减缓internal covariate shift的方法：</p>
<ul>
<li>白化：对输入数据分布进行变换，具有相同的均值和方差；</li>
<li><strong>batch normalization</strong>：由于白化过程计算成本太高，且改变了网络每一层的分布。</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701153501142.png" alt="image-20220701153501142"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701153552148.png" alt="image-20220701153552148"></p>
<h3 id="1-2-resnet-结构"><a href="#1-2-resnet-结构" class="headerlink" title="1.2  resnet 结构"></a>1.2  resnet 结构</h3><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701154852179.png" alt="image-20220701154852179"></p>
<p><strong>Stage 0</strong></p>
<ul>
<li><code>(3,224,224)</code>指输入<code>INPUT</code>的通道数(channel)、高(height)和宽(width)，即<code>(C,H,W)</code>。现假设输入的高度和宽度相等，所以用<code>(C,W,W)</code>表示。</li>
<li>该stage中第1层包括3个先后操作</li>
</ul>
<ol>
<li><code>CONV</code><br> <code>CONV</code>是卷积（Convolution）的缩写，<code>7×7</code>指卷积核大小，<code>64</code>指<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=卷积核&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">卷积核</a>的数量（即该卷积层输出的通道数），<code>/2</code>指卷积核的步长为2。</li>
<li><code>BN</code><br> <code>BN</code>是Batch Normalization的缩写，即常说的BN层。 </li>
<li><code>RELU</code><br> <code>RELU</code>指ReLU激活函数。</li>
</ol>
<ul>
<li><p>该stage中第2层为<code>MAXPOOL</code>，即最大池化层，其kernel大小为<code>3×3</code>、步长为<code>2</code>。</p>
</li>
<li><p><code>(64,56,56)</code>是该stage输出的通道数(channel)、高(height)和宽(width)，其中<code>64</code>等于该stage第1层卷积层中卷积核的数量，<code>56</code>等于<code>224/2/2</code>（步长为2会使输入尺寸减半）。 </p>
</li>
</ul>
<p>总体来讲，在<em>Stage 0</em>中，形状为<code>(3,224,224)</code>的输入先后经过卷积层、<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=BN层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">BN层</a>、ReLU激活函数、<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=MaxPooling层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">MaxPooling层</a>得到了形状为<code>(64,56,56)</code>的输出。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2022/03/20/Inception%E7%B3%BB%E5%88%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/20/Inception%E7%B3%BB%E5%88%97/" class="post-title-link" itemprop="url">Inception系列</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-03-20 11:03:00" itemprop="dateCreated datePublished" datetime="2022-03-20T11:03:00+08:00">2022-03-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-03-22 13:03:00" itemprop="dateModified" datetime="2022-03-22T13:03:00+08:00">2022-03-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>一般来说，提升神经网络性能最直接的办法就是增加网络的尺寸，包括增加网络的深度和宽度两个方面。但这种方式存在一些问题：</p>
<ol>
<li><p>参数太多，如果训练数据集有限，很容易产生过拟合；</p>
</li>
<li><p>网络越大，参数越多，计算复杂度越高，难以应用；</p>
</li>
<li><p>网络越深，容易出现梯度弥散问题，难以优化模型；</p>
</li>
</ol>
<h2 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception-v1"></a>Inception-v1</h2><p>深度研究 Inception Net模型之前，必须了解 Inception 网络的一个重要概念：</p>
<p><strong>1×1卷积</strong>：1×1 卷积简单地将输入像素及其所有相应通道映射到输出像素。1×1卷积作为降维模块，一定程度上减少的计算量。</p>
<ul>
<li>例如，在不使用1×1卷积的情况下执行5×5卷积，如下所示：</li>
</ul>
<p><img title="" src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/inceptionNet.png" alt="" data-align="center"></p>
<p>FLOPs运算次数：（14×14×48）×（5×5×480）= 112.9M</p>
<ul>
<li>使用 1×1 卷积：</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/inceptionNet2.png" title="" alt="" data-align="center"></p>
<p>FLOPs运算次数</p>
<p>1×1卷积的操作数= （14×14×16）×（1×1×480）=1.5M<br>5×5卷积的操作数= （14×14×48）×（5×5×16 ) = 3.8M<br>相加后，1.5M + 3.8M = 5.3M</p>
<p>因此1×1卷积可以帮助减少模型大小，这可以在某种程度上帮助减少过度拟合问题；</p>
<p><strong>降维初始模型</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/modified.png" title="" alt="" data-align="center"></p>
<h2 id="Inception-v2"><a href="#Inception-v2" class="headerlink" title="Inception v2"></a>Inception v2</h2><h3 id="Inception-v1架构的问题"><a href="#Inception-v1架构的问题" class="headerlink" title="Inception v1架构的问题"></a>Inception v1架构的问题</h3><p>Inception V1有时会使用5×5等卷积，导致输入维度大幅下降。这导致神经网络使用一些精度下降。其背后的原因是，如果输入维度下降得太快，神经网络容易丢失信息。 此外，与3×3相比，<br>当我们使用更大的卷积（如5×5 ）时，复杂度也会降低. 我们可以在因式分解方面走得更远，即我们可以将3×3卷积分成<strong>1×3</strong>的非对称卷积，然后是3×1卷积。这相当于滑动一个具有与3×3卷积相同感受野但比*3×3便宜33%的两层网络。当输入维度很大但仅当输入大小为mxm时，这种分解不适用于早期层（m 在 12 到 20 之间）。根据 Inception V1 架构，辅助分类器提高了网络的收敛性。他们认为，通过将有用的梯度推到较早的层（以减少损失），它可以帮助减少深层网络中梯度消失问题的影响。但是，这篇论文的作者发现这个分类器在训练的早期并没有很好地提高收敛性。</p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>在BN的论文里，作者提出了Internal Covariate Shift这个问题，即在训练神经网络的过程中，因为前一层的参数变化而导致每层的输入分布都在不断变化（the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change）。这使得我们需要更低的学习率和更小心地进行参数初始化，导致我们难以充分构建一个具有饱满地非线性结构的模型，而这个现象就被称作Internal Covariate Shift。为了解决这个问题，Google提出了Batch Normalization（批规范化）。即在每次梯度下降前，对每个mini-batch做归一化操作来降低数据分布的影响。</p>
<h3 id="小卷积和代替大卷积核"><a href="#小卷积和代替大卷积核" class="headerlink" title="小卷积和代替大卷积核"></a>小卷积和代替大卷积核</h3><p><img title="" src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/5x5replaced3x3.png" alt="灯箱" data-align="center"></p>
<p>该架构还将 nXn 分解转换为<em>1xn</em>和 nx1 分解。正如我们上面讨论的，3×3 卷积可以转换为<em>1×3 ，然后是 3×1 卷积，与<em>*3×3</em></em>相比，计算复杂度降低了 33% 。</p>
<p><img title="" src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/1xn.png" alt="灯箱" data-align="center"></p>
<h2 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception v3"></a>Inception v3</h2><p>Inception v3 类似于 Inception v2，并做了以下改动：</p>
<ul>
<li><p>使用 RMSprop 优化器；</p>
</li>
<li><p>辅助分类器全连接层的BN；</p>
</li>
<li><p>使用 7×7 分解卷积；</p>
</li>
</ul>
<h2 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception v4"></a>Inception v4</h2><p>将 Inception 模块和残差连接 结合起来。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA57qi6bKk6bG85LiO57u_6am0,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt=""></p>
<p>EmbOding!9196#CN360</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Jie Chu</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
