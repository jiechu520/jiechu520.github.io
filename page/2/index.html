<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="google-site-verification" content="BTo06tdvlac_Dho4-PFTLmDqjKXr1KtOzavpD8XDA5k" />
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jiechu520.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="日常笔记">
<meta property="og:type" content="website">
<meta property="og:title" content="CJ blog">
<meta property="og:url" content="https://jiechu520.github.io/page/2/index.html">
<meta property="og:site_name" content="CJ blog">
<meta property="og:description" content="日常笔记">
<meta property="og:locale">
<meta property="article:author" content="Jie Chu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jiechu520.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CJ blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">CJ blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">不知名算法工程师</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jie Chu</p>
  <div class="site-description" itemprop="description">日常笔记</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2023/01/09/MAE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/01/09/MAE/" class="post-title-link" itemprop="url">MAE——Masked Autoencoders</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-01-09 21:00:00" itemprop="dateCreated datePublished" datetime="2023-01-09T21:00:00+08:00">2023-01-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-01-09 23:08:00" itemprop="dateModified" datetime="2022-01-09T23:08:00+08:00">2022-01-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="MAE：Masked-Autoencoders"><a href="#MAE：Masked-Autoencoders" class="headerlink" title="MAE：Masked Autoencoders"></a>MAE：Masked Autoencoders</h1><p>code: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/mae">https://github.com/facebookresearch/mae</a></p>
<p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.06377">https://arxiv.org/abs/2111.06377</a></p>
<h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240111174241212.png" alt="image-20240111174241212"></p>
<p><strong>以一定比例随机mask掉图片中的一些图像块然后重建这些部分像素值。</strong></p>
<p>主要特点有两个：</p>
<ol>
<li><strong>非对称</strong>的编、解码器设计</li>
<li>使用<strong>较高(如 75%)的掩码率</strong>(mask 比例)</li>
</ol>
<p>第1点所述的“非对称”主要体现在 <strong>输入形式</strong> 与 <strong>网络结构</strong> 上：编码器(Encoder)仅对可见(un-masked)的图像块进行编码，而解码器(Decoder)的输入则是所有的图像块；同时，Decoder 可以是比较轻量的(比如 Encoder 通常是多层堆叠的 Transformer，而 Decoder 仅需较少层甚至1层就 ok)。这也表明 Encoder 与 Decoder 之间是<strong>解耦</strong>的。</p>
<p>第2点是该工作的一个重要发现：不同于 NLP，<strong>在 CV 中可能要配合较高的 mask 比例才能作为“有效”的自监督代理任务。“有效”指的是任务本身足够困难，这样模型才能学到有效的潜在特征表示。</strong></p>
<p>由于 Encoder 仅处理 un-masked 的 patch(占所有输入的少数)，因此，尽管其本身网络结构比较重载，但依然能够高效训练，特别是对于大模型，能够加速3倍以上，同时配合较高的掩码率，还能够涨点。</p>
<p>作者从一个问题出来解释选择这样mask策略以及模型设计的理由，<strong>“为什么 masked autoencoding 在CV中应用比较少相较于NLP？”</strong> 作者提炼了以下三点：</p>
<ul>
<li>架构差异：之前CNN的架构不适合使用</li>
<li>信息密度不同: 语言的信息密度比较高，将句子中的少量词语抹去再让模型预测被抹去的这些词是比较困难的任务；但对于图像则相反，它在空间中是冗余的，对于图片中的某个部分，模型很容易由其相邻的图像块推断出来，因此在CV中，mask的比例应该更高，才能使任务本身具有足够的挑战性，从而使模型学到良好的潜在特征表示。</li>
<li>解码的目标不一致：CV 和 NLP 在解码器的设计上应该有不一样的考虑：NLP 解码输出的是对应被 mask 掉的词语，本身包含了丰富的语义信息；而 CV 要重建的是被 mask 掉的图像块(像素值)，是低语义的。因此，NLP 的解码器可以很简单，比如 BERT，严格来说它并没有解码器，最后用 MLP 也可以搞定。因为来自编码器的特征也是高度语义的，与需要解码的目标之间的 gap 较小；而 CV 的解码器设计则需要“谨慎”考虑了，因为它要将<strong>来自编码器的高级语义特征解码至低级语义层级</strong>。</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240111174251713.png" alt="image-20240111174251713"></p>
<h2 id="2、具体方法"><a href="#2、具体方法" class="headerlink" title="2、具体方法"></a>2、具体方法</h2><p>MAE的特点主要是：<font color=red><strong>高掩码率</strong>的<strong>随机</strong> mask 策略、<strong>非对称</strong>的编、解码器设计 以及 重建的目标是<strong>像素</strong>值</font></p>
<h3 id="2-1-masked-策略"><a href="#2-1-masked-策略" class="headerlink" title="2.1 masked 策略"></a>2.1 masked 策略</h3><p>沿袭 ViT 的做法，将图像分成一块块(ViT 中是 16x16 大小)不重叠的 patch，然后使用服从<strong>均匀分布(uniform distribution)</strong>的采样策略对这些 patches 随机采样一部分，同时 mask 掉余下的另一部分。被 mask 掉的 patches 占所有 patches 的大部分(实验效果发现最好的比例是 75%)，它们不会输入到 Encoder。</p>
<p>作者实验发现：无论是finetune，还是 fine-tune 还是 linear-probe(微调方法，将最后一层替换成线性层，微调时冻结其他层，只训练这个线性层)，75%都是一个比较好的比例。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240115201547788.png" alt="image-20240115201547788"></p>
<h3 id="2-2-Encoder"><a href="#2-2-Encoder" class="headerlink" title="2.2 Encoder"></a>2.2 Encoder</h3><p><strong>Encoder只处理 un-masked 的patches</strong>。Encoder可以是Vit或者其他backbone。图像划分成patch采用Vit的做法。</p>
<p>先将图像从 (B,C,H,W) reshape 成 (B,N,PxPxC)，其中 N 和 P 分别为 patch 数量 和 patch 大小( $H/P \times W/P$)，也就是<strong>将3通道的图像转换成 N 个 维度大小为 PxPxC 的向量</strong>；然后，<strong>通过线性映射(linear projection，可以是全连接层)将其嵌入(embed)到指定的维度空间大小</strong>，记为 ‘dim’(从 PxPxC project 到 dim)，转换成为 <strong>token</strong>(B,N,dim)；最后再<strong>加上位置嵌入(position embedding)</strong>，从而为各个 patch 添加位置信息。<strong>位置嵌入是所有图像共享的、可学习的</strong>，shape 与 每张图的 token 相对应，即：(N,dim)。</p>
<p>由于 un-masked patches 占所有 patches 的少数，计算消耗和空间需求都减少了，因此可以训练很大的 Encoder。</p>
<h3 id="2-3-Decoder"><a href="#2-3-Decoder" class="headerlink" title="2.3 Decoder"></a>2.3 Decoder</h3><p>Decoder 不仅需要处理经过 Encoder 编码的 un-masked 的 tokens，还需要处理 masked tokens。但请注意，<strong>masked token 并非由之前 mask 掉的 patch 经过 embedding 转换而来，而是可学习的、所有 masked patches 都共享的1个向量，对，仅仅就是1个！</strong></p>
<p>通过 position embedding 来区分各个 masked patch 对应的 token。</p>
<h3 id="2-4-loss"><a href="#2-4-loss" class="headerlink" title="2.4 loss"></a>2.4 loss</h3><p>MAE 预训练任务的目标是重建像素值，并且仅仅是 masked patches 的像素值，也就是<strong>仅对 mask 掉的部分计算 loss</strong>，而 loss 就是很大众的 MSE。为何仅计算 mask 部分的 loss？实验结果发现这样做模型的性能会更好，而如果对所有 patches 都计算 loss 的话会掉点。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h2><p><strong>mask采样策略</strong></p>
<p>作者通过实验比较，最终选择了服从均匀分布的随机采样，以下是详细实验结果：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116104651814.png" alt="image-20240116104651814"></p>
<p><strong>Decoder设计</strong></p>
<p>作者还研究了Decoder的设计，下图展示了 Decoder 的深度和宽度对于 ft 和 linear probe 的影响。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116104906404.png" alt="image-20240116104906404"></p>
<p>Decoder 的深度和宽度对于 linear probe 有较为明显的影响，但对于 fine-tune 的影响却不那么突出。究其本质，原因是是<strong>预训练任务(图像重建)与下游任务(图像识别)之间存在着 gap</strong>。</p>
<p><strong>encoder为什么不用 masked tokens</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116105137183.png" alt="image-20240116105137183"></p>
<p>原因是下游任务并不存在这些 masked tokens，上下游任务之间存在gap。如果Encoder 也对 masked tokens 进行编码，会进一步将这种 gap 的影响“扩散”至下游任务中。</p>
<p><strong>各种重建目标的比较</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116105418257.png" alt="image-20240116105418257"></p>
<p><strong>数据增强</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116105510461.png" alt="image-20240116105510461"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2022/06/21/DARN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/21/DARN/" class="post-title-link" itemprop="url">图片美观度模型</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-21 21:00:00" itemprop="dateCreated datePublished" datetime="2022-06-21T21:00:00+08:00">2022-06-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-06-22 23:08:00" itemprop="dateModified" datetime="2022-06-22T23:08:00+08:00">2022-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="360图片搜索—图片美观度模型"><a href="#360图片搜索—图片美观度模型" class="headerlink" title="360图片搜索—图片美观度模型"></a>360图片搜索—图片美观度模型</h1><p><a target="_blank" rel="noopener" href="https://github.com/lmm360/Image-aesthetic-assessment">code和数据集地址</a></p>
<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>在图片搜索中，用户希望搜索的图片不但和自己的查询相关，并且在视觉感官、图片质量等方面也要比较满意。此时，就需要我们计算图片图片的在美学上的特征分值，加到排序模型中，将高相关性、高美学质量的图像排在检索结果的前面。</p>
<p>但是现有的图片质量模型主要考虑图像的质量，如像素，清晰度、有无噪声，在图像的美学特征，如构图、色彩、内容和谐等考虑的权重偏低。</p>
<h2 id="二、相关研究"><a href="#二、相关研究" class="headerlink" title="二、相关研究"></a>二、相关研究</h2><p>大部分的工作将这种美观度评估的任务形式化为回归或者分类问题。开源的数据集有 AVA【1】, AADB【2】等。代表的工作有 NIMA模型【3】和DARN模型【4】。360美观度模型正是在DARN模型的基础上改进得到的。</p>
<p><strong>NIMA模型</strong>利用了AVA数据集，AVA数据集包含了约 25w张图像，由业余摄影师根据审美品质进行评分，200人评分的平均分就是该张图像的最终的分数。</p>
<p><strong>NIMA模型</strong>目标是预测与人类评分的相关性，而不是将图片分类或者回归到平均分。提出了<strong>EMD loss</strong>，它显示了有序类分类的性能提升。下面是模型结构。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026144748579.png" alt="image-20231026144748579"></p>
<p>开源的数据集AVA、AADB图像与图片搜索的图像在种类和美学质量分布上有很大的差异，所以不能再公开的数据集上训练模型。最好的方式使构建自己的数据集，但是像AVA数据集的构建，需要一批具有美学专业知识的标注人员对每张图片进行标注。标注成本和难度都有很大</p>
<p>Microsoft提出了<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.00309">《An Universal Image Attractiveness Ranking Framework》</a>，很好的解决美学数据集构建难的问题，并提出了新的美学模型——DARN模型。下面介绍DARN模型以及360美观度模型在此之上的改进。</p>
<h2 id="三、基于DARN的美观度模型"><a href="#三、基于DARN的美观度模型" class="headerlink" title="三、基于DARN的美观度模型"></a>三、基于DARN的美观度模型</h2><h3 id="1、数据集构建"><a href="#1、数据集构建" class="headerlink" title="1、数据集构建"></a>1、数据集构建</h3><p>数据集的构建过程如下：</p>
<p>1、在图片搜索中高中低频queey中各选择2000条query。</p>
<p>2、根据query在图片搜索返回结果中，top20里面，top40-60里面各抽取两张图片。</p>
<p>3、根据Swiss-system tournament规则、对这4张图像进行三轮标注。</p>
<p>4、每一轮标注，将同一query下4张图片组成两对，每一对图像由7个人标注人员去进行投票：</p>
<p>左边更好，左边好一点，一样好，右边好一点，右边更好。</p>
<p>具体标注可以参考下图：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026154542693.png" alt="image-20231026154542693"></p>
<p>这样标注的好处是，标注人员无需掌握专业知识，“比较”相对于给出一个直接的分值更简单，多人投票也降低主观偏好带来的偏差。7个标注人员对image1和image2进行投票的结果中，左边更好，左边好一点，一样好，右边好一点，右边更好的人数分别为0、1、1、4、1，则这对图像的label为：</p>
<script type="math/tex; mode=display">
l(image1,image2)=(0,1,1,4,1)</script><h3 id="2、DARN模型"><a href="#2、DARN模型" class="headerlink" title="2、DARN模型"></a>2、DARN模型</h3><p>我们的标注数据是pair对，模型结构如下：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026163835872.png" alt="image-20231026163835872"></p>
<p>左边的网络和右边的网络是参数共享的，图像对输入到网络中，经过 Deep CNN（如resnet50）的特征抽取，再经过三个fc层，然后计算最终输出的方差和均值。</p>
<p>假设每张图像都是由很多的专业的美学专家评分得到的，那么根据中心极限定理，图像美观度分值$X$服从正态分布，AVA数据基本就符合正态分布。所以<strong>最终计算得到的均值和方差就可以看作是模型计算的该图像的平均美观度分值和方差</strong>。</p>
<p>对于一个pair对 $[x_i,x_j]$, 它们的 label 记为${0,1,2,3,4}$ 分别对应着左边更好，左边好一点，一样好，右边好一点，右边更好。对于我们的标注数据有：</p>
<script type="math/tex; mode=display">
\sum_{y=0}^4{P_{ij}(xi-xj)=y}=1</script><p>直觉上，图像通过模型得到的美观度分值有以下两个特点：</p>
<ul>
<li>图像越美观，模型得到美观度分数均值越大。</li>
<li>图像对的label应该与美观度分数一致。如，$u_i&gt;&gt;u_j$,那么标注图像 $i$ 好一点或者更好的人数应该更多。如果 $u_i=u_j$,则大部分人标注的 label 是 “一样好”。</li>
</ul>
<p>上面说到我们假设美观度分值服从正态分布，那么两个美观度分值的差也服从正态分布，记$x_{i}^{left}$,$x_{j}^{right}$分别表示图像对中左边图像美观度和右边图像美观度（左边和右边只是为了对应我们数据的标注label），他们美观度的差值 $\Delta X$ 服从 $N(x,u_i-u_j,\sigma_{1}^{2}+\sigma_{2}^{2})$。</p>
<p>然后我们定义4个可以学习的边界值 ${b_i}_{i=0}^{3}$，这4个boundaries将 x 轴分成5部分，也就将正态分布和x轴围成的区域分成了5个部分，这5个部分就对应了 我们设定的 五个label {左边更好，左边好一点，一样好，右边好一点，右边更好}，我们定义 $p_{i}^{j}，j={0,1,2,3}$表示第$i$对图像对被标注lable $j$ 的概率。$\Delta u_i $和 $\Delta \sigma_i$ 表示第$i$对图像对美观度分值差的均值和方差。于是我们有：</p>
<script type="math/tex; mode=display">
p_{i}^{j}=\displaystyle \int^{b_j}_{b_{j-1}}N(x;\Delta u_i,\Delta \sigma_i)dx</script><p>形象点可以看下图：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026175218691.png" alt="image-20231026175218691"></p>
<p>上面这张图显示了标签如何随着图像对的分数差值系统的变化。当右侧图像的平均得分远高于左侧图像时，得分差的分布将向右移动，导致标签“右侧更好”的概率更高。当左右之间的分数差异变小时，分布向左移动并导致标签发生变化。因此，当两个图像的分数相等时，中间桶中的面积最大，这意味着这对图像最有可能被标记为“一样好”。</p>
<p>至此我们可以计算出模型对第 $i$ 对图像对的美观度分值的差的分布 $p_i=(p_{i}^{0},p_{i}^{1},p_{i}^{2},p_{i}^{3},p_{i}^{4})$ ，而我们</p>
<p>图像对的真实label 为 $l=(n_{i}^{0},n_{i}^{1},n_{i}^{2},n_{i}^{3},n_{i}^{4})$, 因此我们可以定义一个对数最大似然损失函数：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026180559585.png" alt="image-20231026180559585"></p>
<p>从上面的过程可以看出，由于预先假设图像的美学评分服从正态分布，而正态分布可以由均值和方差两个参数控制，所以严格的说，DARN模型预测的不是美学评分，而是美学评分分布。<strong>在实际应用时，可以直接用均值作为该图像的美学评分</strong>。</p>
<h3 id="3、实验改进"><a href="#3、实验改进" class="headerlink" title="3、实验改进"></a>3、实验改进</h3><p>实验的过程中，根据我们的数据集和训练情况对DARN模型做了一些改进和调整。</p>
<p>1、 原始DARN模型中，deep CNN以及以后所有FC层，激活函数都是 Relu。缺少带有归一化能力的激活函数和norm操作，则很可能随着<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=W方差&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;83896463&quot;}">W方差</a>的增大，出现个别节点数值过大的问题，导致训练时loss很可能出现nan的情况。所以将第一个FC层的激活函数改为 tanh。</p>
<p>2、在计算均值之前加上softplus激活函数。使得最后推理的结果都是正的，且减小方差。模型中计算正态分布在某一个区间的积分，需要保证 $\sigma &gt;0$，才能让正态分布函数有意义，但是原始论文中：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026182412236.png" alt="image-20231026182412236"></p>
<p>使用的是Relu激活函数，有可能训练得到 $\sigma = 0$。</p>
<p>3、训练中对边界值进行截断，防止训练中部分 $\Delta u$过大，产生nan值。</p>
<p>4、最后一层维度通过实验调整为 16。</p>
<p>5、边界值的初始化值对于模型训练影响很大，我们数据集下初始化值为（-0.75，-0.25， 0.25, 0.75）</p>
<p>6、deep CNN 由 resnet50 调整为 swin transformer。</p>
<h2 id="四、结果展示"><a href="#四、结果展示" class="headerlink" title="四、结果展示"></a>四、结果展示</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image2022-11-3_15-17-31.png" alt="image2022-11-3_15-17-31"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image2022-11-3_15-21-28.png" alt="image2022-11-3_15-21-28"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image2022-11-3_15-24-26.png" alt="image2022-11-3_15-19-28"></p>
<p>该美观度模型计算的特征应用在360图片搜索的排序中，离线ndcg指标和在线Ctr均得到了提升。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>【1】N. Murray, L. Marchesotti, and F. Perronnin. Ava: A large-scale database for aesthetic visual analysis. In Computer Vision and Pattern Recognition (CVPR), pages 2408–2415,Providence, RI, USA, 2012. IEEE.</p>
<p>【2】S. Kong, X. Shen, Z. Lin, R. Mech, and C. Fowlkes. Photo aesthetics ranking network with attributes and content adapta-tion. In European Conference on Computer Vision (ECCV),<br>page 662679, Amsterdam, The Netherlands, 2016. Springer.</p>
<p>【3】H. Talebi and P . Milanfar. Nima: Neural image assess-ment. IEEE Transactions on Image Processing, 27:3998–4011, 2017</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2022/05/10/NIMA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/10/NIMA/" class="post-title-link" itemprop="url">NIMA——Neural Image Assessment</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-10 21:00:00" itemprop="dateCreated datePublished" datetime="2022-05-10T21:00:00+08:00">2022-05-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-05-11 23:08:00" itemprop="dateModified" datetime="2022-05-11T23:08:00+08:00">2022-05-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="NIMA：Neural-Image-Assessment"><a href="#NIMA：Neural-Image-Assessment" class="headerlink" title="NIMA：Neural Image Assessment"></a>NIMA：Neural Image Assessment</h1><h2 id="1、introduction"><a href="#1、introduction" class="headerlink" title="1、introduction"></a>1、introduction</h2><ol>
<li><p>基于CNN的方法相比较早期基于手工制作的特征的做法，性能有显著的提升；</p>
</li>
<li><p>AVA 数据集，图片审美视觉分析的标准数据集；</p>
</li>
<li><p>深度CNN非常适合审美评估任务【1】【2】；他们的双列 CNN 由四个卷积层和两个全连接层组成，其输入是调整大小的图像和大小为 224 × 224 的裁剪窗口；</p>
</li>
<li><p>之前大多是通过分类或者回归的方法预测人类评估的分数；</p>
</li>
<li><p>kong【3】训练了一个基于 AlexNet 的 CNN 来学习两个输入图像的审美分数差异，从而间接优化排名相关性。</p>
</li>
<li><p>NIMA的目标是预测与人类评分的相关性，而不是将图片分类或者回归到平均分。提出了EMD loss，它显示了有序类分类的性能提升。实验也表明，这种方法也更准确地预测了平均分。</p>
</li>
</ol>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h2 id="2、proposed-method"><a href="#2、proposed-method" class="headerlink" title="2、proposed method"></a>2、proposed method</h2><p>模型建立在图片分类模型的结构基础上。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026114849878.png" alt="image-20231026114849878"></p>
<p>在训练阶段，输入图像被重新缩放为256<em>256，然后随机提取 224\</em>224的裁剪，这减少潜在的过度拟合问题。</p>
<h2 id="3、实验"><a href="#3、实验" class="headerlink" title="3、实验"></a>3、实验</h2><p>基线CNN权重通过在ImageNet上训练来初始化，最后一个全连接层是随机初始化。权重和偏置动量设置为0.9,基线CNN最后一层dropout应用0.75。基线CNN层和最后一个全连接层的学习率分别设置为3<em>10e-7,3\</em>10e-6。在基线CNN层上设置较低的学习率会导致使用sgd时更容易和更快优化。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>【1】Deep multi-patch aggregation network for image style, aesthetics, and quality estimation</p>
<p>【2】Rating image aesthetics using deep learning</p>
<p>【3】S. Kong, X. Shen, Z. Lin, R. Mech, and C. Fowlkes, “Photo aesthetics ranking network with attributes and content adaptation,” in European Conference on Computer Vision. Springer, 2016, pp. 662–679. 1, 2, 6, 7</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2022/04/19/Vit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/19/Vit/" class="post-title-link" itemprop="url">VIT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2022-04-19 21:00:00 / Modified: 23:08:00" itemprop="dateCreated datePublished" datetime="2022-04-19T21:00:00+08:00">2022-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ViT-Vision-Transformer"><a href="#ViT-Vision-Transformer" class="headerlink" title="ViT(Vision Transformer)"></a>ViT(Vision Transformer)</h1><h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><p>受到NLP领域中Transformer成功应用的启发，Vit算法中尝试将标准的Transformer结构直接应用于图像，并对整个图像分类流程进行最少的修改。</p>
<p><strong>Vit原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破Transformer的归纳偏置的限制，可以在下游任务中获得较好的迁移效果。</strong></p>
<p>但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）， $f(g(x)=g(f(x)))$ ，其中g代表卷积操作，f代表平移操作。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。</p>
<h2 id="模型结构与实现"><a href="#模型结构与实现" class="headerlink" title="模型结构与实现"></a>模型结构与实现</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/ViT.png" alt="图1 ViT算法结构示意图"></p>
<h3 id="1-图像分块嵌入"><a href="#1-图像分块嵌入" class="headerlink" title="1.图像分块嵌入"></a>1.图像分块嵌入</h3><p>transformer的输入是一个二维矩阵，因此在ViT算法中，首先要做的是如何将一个$H\times W\times C$ 的三维图像转换为$N\times D$ 的二维输入。</p>
<p>iT中的具体实现方式为：将 $H×W×C$ 的图像，变为一个 $N×(P^2\times C)$ 的序列。这个序列可以看作是一系列展平的图像块，也就是将图像切分成小块后，再将其展平。该序列中一共包含了 $N=HW/P^2$ 个图像块，每个图像块的维度则是 $(P^2\times C)$。其中 $P$ 是图像块的大小，$C$ 是通道数量。经过如上变换，就可以将 $N$ 视为sequence的长度了。</p>
<p>但是，此时每个图像块的维度是 $(P2\times C)$，而我们实际需要的向量维度是 $D$，因此我们还需要对图像块进行 Embedding。这里 Embedding 的方式非常简单，只需要对每个 $(P^2\times C)$的图像块做一个线性变换，将维度压缩为 $D$ 即可。</p>
<h3 id="2-多头注意力"><a href="#2-多头注意力" class="headerlink" title="2.多头注意力"></a>2.多头注意力</h3><p>将图像转换成序列之后，就可以将其输入到 Transformer 中结构中进行特征提取了。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/Multi-head_Attention.jpg" alt="图4 多头注意力"></p>
<h3 id="3-MLP"><a href="#3-MLP" class="headerlink" title="3.MLP"></a>3.MLP</h3><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/MLP.png" alt="图7 多层感知机"></p>
<h3 id="4-DropPath"><a href="#4-DropPath" class="headerlink" title="4.DropPath"></a>4.DropPath</h3><p>除了以上重要模块意外，代码实现过程中还使用了DropPath（Stochastic Depth）来代替传统的Dropout结构，DropPath可以理解为一种特殊的 Dropout。其作用是在训练过程中随机丢弃子图层（randomly drop a subset of layers），而在预测时正常使用完整的 Graph。</p>
<p><strong>参考文献</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2022/04/19/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/19/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/" class="post-title-link" itemprop="url">多模态论文综述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2022-04-19 21:00:00 / Modified: 23:08:00" itemprop="dateCreated datePublished" datetime="2022-04-19T21:00:00+08:00">2022-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%9A%E6%A8%A1%E6%80%81/" itemprop="url" rel="index"><span itemprop="name">多模态</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="多模态论文综述"><a href="#多模态论文综述" class="headerlink" title="多模态论文综述"></a>多模态论文综述</h1><p>持续更新中。。。</p>
<h2 id="1-ALBEF"><a href="#1-ALBEF" class="headerlink" title="1. ALBEF"></a>1. ALBEF</h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h3><p>在Vision-and-Language Pre-training (VLP) 已有工作中，主要是靠一个预训练的图像检测器来提取图像特征，并用多模态的encoder编码器将图像特征和文本特征融合，然后使用“完形填空、图文匹配”等下游任务来训练多模态encoder。</p>
<p>但是这些工作存在着一下问题：</p>
<ol>
<li>用于抽取图像特征和文本特征的模型是分别单独训练的，也就是说他们的特征不在一个空间内，这就是使得多模态encoder不能有效的学习如何去融合两种特征；</li>
<li>使用目标检测模型作为图像特征的抽取器是一件很“昂贵”的事情，在预训练阶段，需要标注物体的box位置，在预测阶段需要有高分辨率的图片；</li>
<li>目前被广泛使用的图像-文本数据集存在着固有的噪声，这些噪声会影响模型性能。</li>
</ol>
<p>基于这些问题，这篇文章提出了ALBEF模型。</p>
<h3 id="1-2-方法"><a href="#1-2-方法" class="headerlink" title="1.2 方法"></a>1.2 方法</h3><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%B1%8F2024-01-29%2023.26.57.png" alt="截屏2024-01-29 23.26.57"></p>
<p>如上图所示，ALBEF使用ViT-B/16（12层的transformer）作为图像输入的编码器，并用ImageNet-1k上预训练的权重初始化它；使用6层的transfomer作为文本输入的编码器，并用 $Bert_{base}$ 的前6层作为初始化；使用  $Bert_{base}$  的后6层权重作为模态融合层的初始化。通过多模态编码器各层的交叉注意，实现图像特征与文本特征的融合。</p>
<p>预训练的任务：</p>
<p><strong>ITC</strong>：ITC任务想要解决“研究动机”中提出的第一个问题，通过该任务使得图像编码器的向量空间与文本编码器的向量空间做对齐。</p>
<p><strong>ITM</strong>：构建负样本的时候用到了ITC任务，在一个batch里，通过计算特征相似度，寻找一张图片除它本身对应的文本之外相似度最高的文本作为负样本。这样就能构建一批hard negatives，从而提升训练难度。</p>
<p><strong>MLM</strong>：完形填空</p>
<h3 id="1-3-总结"><a href="#1-3-总结" class="headerlink" title="1.3 总结"></a>1.3 总结</h3><ol>
<li>通过ITC任务实现了图像向量和文本向量的对齐；</li>
<li>使用VIT代替了图像检测模块，抽取图像特征，减少了图像的标注成本；</li>
<li>通过Momentum Distillation的方式，降低了训练数据中噪声的影响。</li>
</ol>
<h2 id="2-VLMO"><a href="#2-VLMO" class="headerlink" title="2. VLMO"></a>2. VLMO</h2><h3 id="2-1-背景"><a href="#2-1-背景" class="headerlink" title="2.1 背景"></a>2.1 背景</h3><p>第一个动机是<strong>多模态的模型呈现了两个不同的发展方向，但都有各自的缺点。</strong>一种是CLIP，在图文检索上效果比较好，推理速度快，但是这种方法在视觉推理类任务上表现不理想。一个可能的原因是双编码器的结构在编码阶段缺乏模态之间的交互，这样模型在编码阶段并不能充分利用另一个模态的信息。另一种是以ViL-BERT为代表的融合类模型，它们一般会通过一个融合模块来融合文本模态和图像模态编码的特征。但是这类算法在进行视觉推理时的速度却非常慢。</p>
<p>第二个动机是：多模态的数据集不够用，但是分开来看，视觉和NLP各自都有比较大的数据集。</p>
<p>考虑到以上的原因，VLMO提出了一个混合模态专家 Transformer （Mixture-of-Modality-Experts Transformer)。另一个贡献是多模态数据的预训练策略。</p>
<h3 id="2-2-方法"><a href="#2-2-方法" class="headerlink" title="2.2 方法"></a>2.2 方法</h3><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%B1%8F2024-01-30%2023.58.45.png" alt="截屏2024-01-30 23.58.45"></p>
<p>我们从下向上看这个图，首先它的输入是一个多模态的输入。接着是个一个共享的多头自注意力层，这一部分主要的作用是供不同模特的输入特征进行交互。再往上是一个模态混合专家（Swithcing Modality Expert，SME），SME的作用是根据输入数据的类型选择训练不同的由前馈神经网络（Feed Forward Network，FFN）。根据输入数据的三种类型，SME也是由三个不同的专家组成，分别是视觉专家（V-FFN），语言专家（L-FFN），视觉-语言专家（VL-FFN）。</p>
<p>因为涉及了多个模态的数据和专家，VLMo的预训练任务也分成了几种不同的类型：对纯图像数据的训练，对纯文本数据的训练，以及混合模态数据的训练，包括对比学习、图像-文本对匹配、掩码语言模型三个任务，如上图右边所示。</p>
<p><strong>分阶段预训练的策略</strong></p>
<p>因为MoME-Transformer中使用了参数共享的MSA以及参数独立的三个专家，并且VLMo中也引入了三种不同输入数据的预训练任务，因此我们需要一个合适的调度算法来训练模型，VLMo将其命名为阶段预训练（Stagewise Pre-training）。</p>
<p>在进行模型预训练时，一个常见的策略是先使用易获得的海量数据进行训练，使模型优化到一个比较好的参数值后，再在比较难获得的数据上进行微调，从而使得模型在样本数较少的数据上也能获得不错的泛化能力。基于这个思想，VLMo提出的阶段预训练如下图所示。它首先冻结语言专家和视觉-语言专家的参数，通过MIM预训练任务在纯图像数据上训练视觉专家。然后再冻结共享的MSA以及视觉专家和视觉-语言专家的参数，通过MLM预训练任务在纯文本数据上训练语言专家。最后它再解冻所有参数，通过图文数据集的三个任务训练所有的参数。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%B1%8F2024-01-31%2000.01.45.png" alt="截屏2024-01-31 00.01.45"></p>
<p><strong>可以看到在视觉数据上训练的self-attention直接拿来做文本训练，且不需要再 finetune</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%B1%8F2024-01-31%2000.04.25.png" alt="截屏2024-01-31 00.04.25"></p>
<p>从实验结果来看，效果也是比较好的。</p>
<h3 id="2-3-总结"><a href="#2-3-总结" class="headerlink" title="2.3 总结"></a>2.3 总结</h3><p>VLMO 提出了一个可切换不同结果的名为 MoME-Transformer的结构，解决了双编码器结构模型以及特征融合结构模型的问题，并在VQA，VR任务达到了主流效果。</p>
<h2 id="3-BLIP"><a href="#3-BLIP" class="headerlink" title="3. BLIP"></a>3. BLIP</h2><h3 id="3-1-背景"><a href="#3-1-背景" class="headerlink" title="3.1 背景"></a>3.1 背景</h3><p>从模型的角度，最近的一些方法，只使用 encoder-only的模型无法直接运用到 text generaton的任务中如 image captioning；使用 encoder-decoder 结构的模型又无法直接适用于 image-text retrieval 任务。</p>
<p>从数据的角度，大部分方法的数据都是比较 noise的。</p>
<p>BLIP 引入了 encoder-decoder的多模态混合结构 MED（Multimodal mixture of Encoder-Decoder），能够有效地进行多任务雨训练和迁移学习。</p>
<h3 id="3-2-方法"><a href="#3-2-方法" class="headerlink" title="3.2 方法"></a>3.2 方法</h3><p>MED包括两个单模态编码器（lmage Encoder，Text Encoder），一个以图像为基础的编码器（image-grounded text encoder）和一个以图像为基础的解码器（image-grounded text decoder）。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%B1%8F2024-01-31%2023.30.15.png" alt="截屏2024-01-31 23.30.15"></p>
<p><em>通过三个损失函数联合进行预训练：*</em></p>
<p><strong>(i)图像-文本对比损失 ITC</strong>（Image-Text Contrastive Loss）：针对图像编码器和文本编码器，通过正负图文对的对比学习，<strong>来对齐图像和文本的潜在特征空间。</strong></p>
<p><strong>(ii)图像-文本匹配损失 ITM</strong>（Image-Text Matching Loss）：针对以图像为基础的文本编码器，通过对图文匹配性进行二分类，<strong>建模图文多模态信息的相关性。</strong></p>
<p><strong>(iii)语言建模损失 LM</strong>（Language Modeling Loss ）：针对以图像为基础的文本解码器，通过交叉熵损失进行优化，<strong>训练模型以自回归的方式生成目标caption。</strong></p>
<p>网络上获得的大量图文对，通常包含许多不准确甚至错误的信息，为了有效利用这种形态的数据，BLIP提出caption生成和过滤模块<strong>CapFilt</strong>（Captioning and Filtering），首先从噪声图文对中学习，然后生成和过滤产生新的数据集，再去迭代优化原模型。</p>
<p><strong>CapFilt包含</strong>两个模块：<strong>一个是captioner</strong>，给网络图像生成caption，<strong>另一个是Filter</strong>，过滤原始网络文本和合成文本中的噪声caption。实验结果表明，通过captioner和filter的协作，<strong>BLIP模型能够在各种下游任务上取得了稳定的性能提升</strong>，包括图像-文本检索、图像标题、视觉问答、视觉推理和视觉对话。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%B1%8F2024-01-31%2023.33.55.png" alt="截屏2024-01-31 23.33.55"></p>
<p>Captioner和Filter都是从预训练的模型初始化的，并在人工标注数据集上单独进行微调。</p>
<p><strong>（i）Captioner是image-grounded text decoder，</strong>它在人工标注数据集上以LM为目标进行微调，对给定的图像进行文本解码，这里给定网络图片，Captioner生成合成caption 。</p>
<p><strong>（ii）Filter是image-grounded text encoder</strong>，它根据<strong>ITC</strong>和<strong>ITM</strong>的目标进行微调，以学习文本是否与图像匹配，去除原始网络文本和合成文本中的噪音文本。</p>
<p><strong>（iii）Bootstrap过程，</strong>Captioner生成的图文对与Filter过滤后的网络图文，再加上人工标注的图文对结合起来，形成一个新的数据集，重新预训练一个新模型。</p>
<h3 id="3-3-总结"><a href="#3-3-总结" class="headerlink" title="3.3 总结"></a>3.3 总结</h3><p>BLIP提出了一个encoder-decoder结构，统一了理解和生成的任务，并且提出了一个用训练好的模型去清洗数据反过来再去迭代原模型的方法。</p>
<h2 id="4-CoCa"><a href="#4-CoCa" class="headerlink" title="4. CoCa"></a>4. CoCa</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%B1%8F2024-02-01%2000.13.24.png" alt="截屏2024-02-01 00.13.24"></p>
<p>CoCa结构与ALBEF类似，但是将 encoder换成了 decoder，并且通过 ITC和LM 的任务进行预训练。</p>
<blockquote>
<p>（1）Autoregressive decoder 的设计可以在几乎不增加计算量的前提下结合 full</p>
<p>sentence 的对比loss和 subword-level 的 LM loss。（2）CoCa的另一个动机是simVLM证明了 生成任务可以简单高效地训练多模态模型且具备 zero-shot的能力。</p>
</blockquote>
<p>CoCa构建在encoder-decoder基础上，不过这里将text decoder均分成两个部分：unimodal text decoder和multimodal text decoder。然后增加一个cls token在文本的最后，unimodal text decoder不参与对图像特征的cross-attention，这样cls token经过unimodal text decoder之后就能够得到整个句子的全局特征。同时采用attention pooling对image encoder得到特征提取图像的全局特征，两个全局特征就可以实现图像-文本的对比学习，这里的attention pooling其实就是一个multi-head attention，只不过key和value是image encoder得到的特征，而query是预先定义的一个可训练的embedding，由于我们只需要提取一个全局特征，所以只需要定义一个query就好了。</p>
<p>multimodal text decoder将用来执行生成任务，这里也通过一个attention pooling对image encoder得到的特征进行提取，不过这里query数量定义为256，这样attention pooling可以得到256个特征，它作为multimodal text decoder的cross-attention的输入。</p>
<p>CoCa训练使用的数据比较大，最后得到的效果也刷新的当时很多项多模态任务的榜单。</p>
<h2 id="5-BEiT-V3"><a href="#5-BEiT-V3" class="headerlink" title="5 BEiT V3"></a>5 BEiT V3</h2><h3 id="5-1-背景"><a href="#5-1-背景" class="headerlink" title="5.1 背景"></a>5.1 背景</h3><p>无论是 NLP，CV 还是 多模态领域，模型大一统是大势所趋，也就是在超大的数据集上做大规模预训练，一旦模型训练好了之后，就可以直接应用到下游任务中，成为一个通用的 Foundation Model。Beit V3正是朝着这个目标，对之前的工作进行总结和改进之后实现的。</p>
<p>主要亮点是 Beit v3 直接将图像和文本以相同的方式处理，并通过一个预训练任务进行训练，也就是 mask data modeling。</p>
<h3 id="5-2-方法"><a href="#5-2-方法" class="headerlink" title="5.2 方法"></a>5.2 方法</h3><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/1928412-20230319083441947-1188926581.png" alt=""></p>
<p>模型结构采用和 VLMO 相同的 MOME，训练目标是 mask data modeling，可能是遮住了图像，可能是遮住了文本，模型训练学习如何去恢复它就可以。</p>
<p>下游任务实现框架：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/1928412-20230319083441599-652241990.png" alt=""></p>
<h3 id="5-3-总结"><a href="#5-3-总结" class="headerlink" title="5.3 总结"></a>5.3 总结</h3><p>目标函数不是越多越好，要看目标函数是否有互补的特性。数据的质量也很关键。</p>
<h2 id="6-BLIP2"><a href="#6-BLIP2" class="headerlink" title="6 BLIP2"></a>6 BLIP2</h2><h3 id="6-1-背景"><a href="#6-1-背景" class="headerlink" title="6.1 背景"></a>6.1 背景</h3><p>使用大规模模型和数据集进行端到端训练需要<strong>较高的计算成本。</strong></p>
<p>为了利用预先训练好的单模态模型进行视觉语言预训练，促进<strong>跨模态对齐是关键</strong>。然而，由于 LLM 在单模态预训练期间没有看到图像，因此冻结它们使视觉-语言对齐特别有挑战。现有方法诉诸于图像到文本的生成损失，作者表明这不足以弥补模态间的差距。</p>
<p>为了实现与冻结的单模态模型的有效视觉-语言对齐，作者提出了一种通过<strong>两阶段预训练策略</strong>进行预训练的 Query Transformer（Q-Former）。如下图 Figure 1 所示，Q-Former 是一个轻量级的 Transformer，它使用一组可学习的Query 向量来从冻结的 image encoder 中提取视觉特征。它充当了冻结的 image encoder 和冻结的 LLM 之间的信息屏障（bottleneck），为 LLM 提供最有用的视觉特征，以输出所需的文本。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240221232233626.png" alt="image-20240221232233626"></p>
<h3 id="6-2-方法"><a href="#6-2-方法" class="headerlink" title="6.2 方法"></a>6.2 方法</h3><h4 id="6-2-1-模型结构"><a href="#6-2-1-模型结构" class="headerlink" title="6.2.1 模型结构"></a>6.2.1 模型结构</h4><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240221232331395.png" alt="image-20240221232331395"></p>
<p>模型中唯一可训练的模块就是提出的 Q-Former，它能够从图像编码器中提取<strong>固定数量的输出特征</strong>，<strong>与输入图像分辨率无关</strong>。Q-Former 的结构如上图所示，其由两个共享 Self Attention的 Transformer 子模块组成（也就是说，图中橙色的 Self Attention 是共享的，灰色的 Cross Attention、紫色的 Feed Forward 和绿色的 Feed Forward 都是独立的）：</p>
<ul>
<li>Q-Former 左侧为 image transformer：与冻结的 image encoder 交互以进行视觉特征提取</li>
<li>Q-Former 右侧为 text transformer：可以用文本 encoder 和 文本 decoder</li>
</ul>
<p>在 Q-Former 中，作者额外创建了一组可学习的 Query embedding 作为 image transformer 的输入。这些 Query embedding 在 Self Attention 层相互交叉，并通过 Cross attention 层（每隔一个 transformer block 有一个 Cross attention）与冻结的 image encoder 输出的 image embedding 进行交叉。此外，这些 Query embedding 还通过相同的 Self Attention 与 text embedding 相交叉。作者使用 Bert Base 的预训练权重来初始化 Q-Former，其中的 Cross Attention 是随机初始化的，Q-Former 总共包含 188M 个参数（包括 Query embedding）。</p>
<p>根据预训练任务不同，作者会使用不同的 Self Attention Mask 来控制 Query embedding 和 text embedding 的交互。</p>
<h4 id="6-2-2-第一阶段-表征学习阶段"><a href="#6-2-2-第一阶段-表征学习阶段" class="headerlink" title="6.2.2 第一阶段-表征学习阶段"></a>6.2.2 第一阶段-表征学习阶段</h4><p>作者将 Q-Former 连接到冻结的 Image encoder 上，使用图像文本对进行预训练。目标是训练 Q-Former，以便 Query 可以学习提取对文本信息量最大的视觉表征。受 BLIP 的启发，作者联合了三个具有相同输入格式和模型参数的预训练目标。每个目标在 Query 和 Text 之间采用不同的 Attention Mask 策略来控制它们之间的交互。分别是：</p>
<h5 id="6-2-2-1-ITC-图像文本对比学习"><a href="#6-2-2-1-ITC-图像文本对比学习" class="headerlink" title="6.2.2.1 ITC 图像文本对比学习"></a>6.2.2.1 ITC 图像文本对比学习</h5><p>ITC 的目的是使图像表征和文本表征对齐，以便最大化它们之间的交互信息。作者使用对比损失，通过对比正对和负对的相似性来实现这一点（正对的距离尽量近，负对的距离尽量远）。具体来说，作者将 image transformer 的输出 Query 表征 Z 与 text transformer 输出的文本表征 t 对齐，其中 t 对应 [CLS] Token 的输出 embedding。由于 Z 包含多个输出 embedding（32 个 Query，对应 32 个 embedding 向量），因此作者首先计算每个 Query 表征与 t 之间的成对相似度（32 个），然后选择最高的一个作为图像-文本相似度。为了避免信息泄露，作者采用了单模态的 Self-Attention Mask，也就是如下图红框所示 Mask，其不允许 Query 和 Text 相互看到。其中的负例都从 batch 数据中选择（图像-文本是成对存在的，每个图像都有 1 个正对，其余图像对应的文本都可以作为负对，也就是每个图像有 batch size - 1 个负样本），而不是 BLIP 中的动量队列.</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240222000314502.png" alt="image-20240222000314502"></p>
<h5 id="6-2-2-2-ITG-基于图像文本生成"><a href="#6-2-2-2-ITG-基于图像文本生成" class="headerlink" title="6.2.2.2 ITG 基于图像文本生成"></a>6.2.2.2 ITG 基于图像文本生成</h5><p>ITG 的目的是以给定输入图像作为条件来训练 Q-Former 生成文本。由于 Q-Former 的架构不允许 Text Token 与 image encoder 之间直接交互，因此必须先由 Query 和 image encoder 交互提取生成文本所需的信息，然后通过 Self-Attention 传递给 Text Token。也就是说，Query 被强制提取有关文本的所有信息的视觉特征。作者采用多模态因果自注意力掩码（Multi-modal Causal Self-Attention Mask）来控制 Query-Text 之间的交互。如下图红框内所示，类似于 UniLM 中使用的 Mask，Query 可以相互关注到，但不能关注到 Text。每个 Text Token 都可以关注到所有 Query Token，以及之前的 Text Token。此外作者还将 [CLS] Token 换成了 [DEC] Token，作为发出解码任务信息的第一个 Text Token。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/640.png" alt="图片"></p>
<h5 id="6-2-2-3-ITM-图像文本匹配"><a href="#6-2-2-3-ITM-图像文本匹配" class="headerlink" title="6.2.2.3 ITM 图像文本匹配"></a>6.2.2.3 ITM 图像文本匹配</h5><p>ITM 的目的是学习图像和文本之间的细粒度对齐。这是一个二元分类任务，要求模型预测图像-文本对是正（匹配）还是负（不匹配）。此时作者使用双向自注意力掩码（Bi-directional Self-Attention Mask），如下图红框内所示，也就是 Query 和 Text 都可以相互看到。因此 Query 表征 Z 可以捕获到多模态信息。之后，作者将 Z 中的每个 embedding（32）都输入到二元分类 Linear 层以获得 logit，并将所有 Query 的 logit 平均输出为匹配分数。作者采用了 [2107.07651] Align before Fuse: Vision and Language Representation Learning with Momentum Distillation 中的 hard 负样本挖掘策略来创建信息丰富的负对。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/640-20240222000447400.png" alt="图片"></p>
<h4 id="6-2-3-第二阶段"><a href="#6-2-3-第二阶段" class="headerlink" title="6.2.3 第二阶段"></a>6.2.3 第二阶段</h4><p>在第二阶段的预训练阶段，作者将 Q-Former（带有冻结的 image encoder）连接到冻结的 LLM，以获得 LLM 强大的语言生成能力。如下图 Figure 3 所示，作者将 Q-former 的输出 Z 通过全连接（FC）投影到与 LLM 的 text embedding 相同的维度中（类似 LLaVA-1.5 中直接将 image encoder 的输出通过一个 MLP 投影到 text embedding 相同维度，并实现模态对齐）。然后，将投影的 Query embedding 附加到输入的 text embedding 之前。它们可以充当 soft visual prompts，也就是使 LLM 以 Q-Former 提取的视觉表征为条件。由于 Q-Former 已经经过预训练以提取语言相关（language-informative）的视觉表征，因此它有效的充当了信息瓶颈（bottleneck），将最有用的信息提供给 LLM，同时删除不相关的视觉信息。这减轻了 LLM 学习视觉-语言对齐的负担，从而减轻了灾难性遗忘问题。</p>
<p>作者尝试了两种类型的 LLM：</p>
<ul>
<li>Decoder-Only 的 LLM：使用语言建模损失进行预训练，其中冻结的 LLM 的任务是生成基于 Q-Former 的视觉表征的文本。</li>
<li>Encoder + Decoder 的 LLM：使用 prefix 语言建模损失进行预训练，将文本分成两部分，prefix 文本与视觉表征连接在一起作为 LLM Eecoder 的输入，suffix 文本用作 LLM Decoder 的生成目标。</li>
<li><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240222000603115.png" alt="image-20240222000603115"></li>
</ul>
<h3 id="6-3-总结"><a href="#6-3-总结" class="headerlink" title="6.3 总结"></a>6.3 总结</h3><p>本文中作者提出了 BLIP-2，这是一种通用且计算高效的视觉-语言预训练方法，它利用了冻结的预训练 image encoder 和 LLM。BLIP-2 在各种视觉语言任务上实现了 SOTA，同时在预训练期间具有很少的可训练参数。BLIP-2 还展示了 zero-shot 指示图像到文本生成的能力。作者认为 BLIP-2 是构建多模态对话式 AI 代理的重要一步。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2022/04/11/resnet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/11/resnet/" class="post-title-link" itemprop="url">resnet解析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-04-11 21:03:00" itemprop="dateCreated datePublished" datetime="2022-04-11T21:03:00+08:00">2022-04-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-11 23:03:00" itemprop="dateModified" datetime="2023-04-11T23:03:00+08:00">2023-04-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="resnet"><a href="#resnet" class="headerlink" title="resnet"></a>resnet</h1><h3 id="1-1-batch-normalization原理"><a href="#1-1-batch-normalization原理" class="headerlink" title="1.1 batch normalization原理"></a>1.1 batch normalization原理</h3><p>提出背景：在训练层数较深的神经网络时，参数的变化导致每一层的输入分布会发生变化，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难——internal Covariate Shift</p>
<p>问题：上层网络需要不停调整来适应输入分布的变化，导致网络学习速度的降低；网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度。</p>
<p>减缓internal covariate shift的方法：</p>
<ul>
<li>白化：对输入数据分布进行变换，具有相同的均值和方差；</li>
<li><strong>batch normalization</strong>：由于白化过程计算成本太高，且改变了网络每一层的分布。</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701153501142.png" alt="image-20220701153501142"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701153552148.png" alt="image-20220701153552148"></p>
<h3 id="1-2-resnet-结构"><a href="#1-2-resnet-结构" class="headerlink" title="1.2  resnet 结构"></a>1.2  resnet 结构</h3><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701154852179.png" alt="image-20220701154852179"></p>
<p><strong>Stage 0</strong></p>
<ul>
<li><code>(3,224,224)</code>指输入<code>INPUT</code>的通道数(channel)、高(height)和宽(width)，即<code>(C,H,W)</code>。现假设输入的高度和宽度相等，所以用<code>(C,W,W)</code>表示。</li>
<li>该stage中第1层包括3个先后操作</li>
</ul>
<ol>
<li><code>CONV</code><br> <code>CONV</code>是卷积（Convolution）的缩写，<code>7×7</code>指卷积核大小，<code>64</code>指<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=卷积核&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">卷积核</a>的数量（即该卷积层输出的通道数），<code>/2</code>指卷积核的步长为2。</li>
<li><code>BN</code><br> <code>BN</code>是Batch Normalization的缩写，即常说的BN层。 </li>
<li><code>RELU</code><br> <code>RELU</code>指ReLU激活函数。</li>
</ol>
<ul>
<li><p>该stage中第2层为<code>MAXPOOL</code>，即最大池化层，其kernel大小为<code>3×3</code>、步长为<code>2</code>。</p>
</li>
<li><p><code>(64,56,56)</code>是该stage输出的通道数(channel)、高(height)和宽(width)，其中<code>64</code>等于该stage第1层卷积层中卷积核的数量，<code>56</code>等于<code>224/2/2</code>（步长为2会使输入尺寸减半）。 </p>
</li>
</ul>
<p>总体来讲，在<em>Stage 0</em>中，形状为<code>(3,224,224)</code>的输入先后经过卷积层、<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=BN层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">BN层</a>、ReLU激活函数、<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=MaxPooling层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">MaxPooling层</a>得到了形状为<code>(64,56,56)</code>的输出。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2022/03/20/Inception%E7%B3%BB%E5%88%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/20/Inception%E7%B3%BB%E5%88%97/" class="post-title-link" itemprop="url">Inception系列</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-03-20 11:03:00" itemprop="dateCreated datePublished" datetime="2022-03-20T11:03:00+08:00">2022-03-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-03-22 13:03:00" itemprop="dateModified" datetime="2022-03-22T13:03:00+08:00">2022-03-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>一般来说，提升神经网络性能最直接的办法就是增加网络的尺寸，包括增加网络的深度和宽度两个方面。但这种方式存在一些问题：</p>
<ol>
<li><p>参数太多，如果训练数据集有限，很容易产生过拟合；</p>
</li>
<li><p>网络越大，参数越多，计算复杂度越高，难以应用；</p>
</li>
<li><p>网络越深，容易出现梯度弥散问题，难以优化模型；</p>
</li>
</ol>
<h2 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception-v1"></a>Inception-v1</h2><p>深度研究 Inception Net模型之前，必须了解 Inception 网络的一个重要概念：</p>
<p><strong>1×1卷积</strong>：1×1 卷积简单地将输入像素及其所有相应通道映射到输出像素。1×1卷积作为降维模块，一定程度上减少的计算量。</p>
<ul>
<li>例如，在不使用1×1卷积的情况下执行5×5卷积，如下所示：</li>
</ul>
<p><img title="" src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/inceptionNet.png" alt="" data-align="center"></p>
<p>FLOPs运算次数：（14×14×48）×（5×5×480）= 112.9M</p>
<ul>
<li>使用 1×1 卷积：</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/inceptionNet2.png" title="" alt="" data-align="center"></p>
<p>FLOPs运算次数</p>
<p>1×1卷积的操作数= （14×14×16）×（1×1×480）=1.5M<br>5×5卷积的操作数= （14×14×48）×（5×5×16 ) = 3.8M<br>相加后，1.5M + 3.8M = 5.3M</p>
<p>因此1×1卷积可以帮助减少模型大小，这可以在某种程度上帮助减少过度拟合问题；</p>
<p><strong>降维初始模型</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/modified.png" title="" alt="" data-align="center"></p>
<h2 id="Inception-v2"><a href="#Inception-v2" class="headerlink" title="Inception v2"></a>Inception v2</h2><h3 id="Inception-v1架构的问题"><a href="#Inception-v1架构的问题" class="headerlink" title="Inception v1架构的问题"></a>Inception v1架构的问题</h3><p>Inception V1有时会使用5×5等卷积，导致输入维度大幅下降。这导致神经网络使用一些精度下降。其背后的原因是，如果输入维度下降得太快，神经网络容易丢失信息。 此外，与3×3相比，<br>当我们使用更大的卷积（如5×5 ）时，复杂度也会降低. 我们可以在因式分解方面走得更远，即我们可以将3×3卷积分成<strong>1×3</strong>的非对称卷积，然后是3×1卷积。这相当于滑动一个具有与3×3卷积相同感受野但比*3×3便宜33%的两层网络。当输入维度很大但仅当输入大小为mxm时，这种分解不适用于早期层（m 在 12 到 20 之间）。根据 Inception V1 架构，辅助分类器提高了网络的收敛性。他们认为，通过将有用的梯度推到较早的层（以减少损失），它可以帮助减少深层网络中梯度消失问题的影响。但是，这篇论文的作者发现这个分类器在训练的早期并没有很好地提高收敛性。</p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>在BN的论文里，作者提出了Internal Covariate Shift这个问题，即在训练神经网络的过程中，因为前一层的参数变化而导致每层的输入分布都在不断变化（the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change）。这使得我们需要更低的学习率和更小心地进行参数初始化，导致我们难以充分构建一个具有饱满地非线性结构的模型，而这个现象就被称作Internal Covariate Shift。为了解决这个问题，Google提出了Batch Normalization（批规范化）。即在每次梯度下降前，对每个mini-batch做归一化操作来降低数据分布的影响。</p>
<h3 id="小卷积和代替大卷积核"><a href="#小卷积和代替大卷积核" class="headerlink" title="小卷积和代替大卷积核"></a>小卷积和代替大卷积核</h3><p><img title="" src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/5x5replaced3x3.png" alt="灯箱" data-align="center"></p>
<p>该架构还将 nXn 分解转换为<em>1xn</em>和 nx1 分解。正如我们上面讨论的，3×3 卷积可以转换为<em>1×3 ，然后是 3×1 卷积，与<em>*3×3</em></em>相比，计算复杂度降低了 33% 。</p>
<p><img title="" src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/1xn.png" alt="灯箱" data-align="center"></p>
<h2 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception v3"></a>Inception v3</h2><p>Inception v3 类似于 Inception v2，并做了以下改动：</p>
<ul>
<li><p>使用 RMSprop 优化器；</p>
</li>
<li><p>辅助分类器全连接层的BN；</p>
</li>
<li><p>使用 7×7 分解卷积；</p>
</li>
</ul>
<h2 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception v4"></a>Inception v4</h2><p>将 Inception 模块和残差连接 结合起来。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA57qi6bKk6bG85LiO57u_6am0,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt=""></p>
<p>EmbOding!9196#CN360</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Jie Chu</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
