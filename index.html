<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="google-site-verification" content="BTo06tdvlac_Dho4-PFTLmDqjKXr1KtOzavpD8XDA5k" />
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jiechu520.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="日常笔记">
<meta property="og:type" content="website">
<meta property="og:title" content="CJ blog">
<meta property="og:url" content="https://jiechu520.github.io/index.html">
<meta property="og:site_name" content="CJ blog">
<meta property="og:description" content="日常笔记">
<meta property="og:locale">
<meta property="article:author" content="Jie Chu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jiechu520.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CJ blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">CJ blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">不知名算法工程师</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jie Chu</p>
  <div class="site-description" itemprop="description">日常笔记</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2024/01/16/Personalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/16/Personalization/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-01-16 19:47:56 / Modified: 19:47:54" itemprop="dateCreated datePublished" datetime="2024-01-16T19:47:56+08:00">2024-01-16</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Latest-Personnalization-Overview"><a href="#Latest-Personnalization-Overview" class="headerlink" title="Latest Personnalization Overview"></a>Latest Personnalization Overview</h1><h2 id="ZipLoRA"><a href="#ZipLoRA" class="headerlink" title="ZipLoRA"></a>ZipLoRA</h2><blockquote>
<p>time: 2023.11</p>
<p>source: google</p>
<p>title: ZipLoRA: any subject in any style by Effectively merging loras</p>
</blockquote>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%9B%BE20231205114657.png" alt="截图20231205114657"></p>
<p>ziplora主要基于一些事实的观察：</p>
<ol>
<li>不通过sd1.x， sdxl能够仅通过一张图片就能学习到图片的风格；</li>
<li>lora 权重是稀疏的，大多数值都很小，对生成质量和逼真度影响很小；</li>
<li>两个独立训练的 LoRA 的权重矩阵的列彼此之间可能具有不同程度的“对齐”，例如通过余弦相似度来测量。 我们发现直接对具有高余弦相似度的列求和会降低合并模型的性能；</li>
</ol>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231205150545862.png" alt="image-20231205150545862"></p>
<p>ziplora通过优化以下的损失：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231205151130045.png" alt="image-20231205151130045"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231205150554367.png" alt="image-20231205150554367"></p>
<p>前两项是，最小化合并后的lora与内容lora、风格lora之间的差异，来保留合并后的lora生成参考风格和内容能力；</p>
<p>最后一项是，最小化内容和风格lora之间的余弦相似度；</p>
<p>训练的时候，只优化 合并系数 <script type="math/tex">m_c,m_s</script>, 文章只需要100次参数更新就能达到很好的效果。</p>
<h2 id="AnyText"><a href="#AnyText" class="headerlink" title="AnyText"></a>AnyText</h2><blockquote>
<p>time: 2023.12</p>
<p>source: alibaba</p>
<p>title：ANYTEXT: MULTILINGUAL VISUAL TEXT GENERATION AND EDITING</p>
<p>code：<a target="_blank" rel="noopener" href="https://github.com/tyxsspa/AnyText">https://github.com/tyxsspa/AnyText</a></p>
<p>paper：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.03054">https://arxiv.org/abs/2311.03054</a></p>
</blockquote>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>作者介绍了一下现有开源文生图模型生成特定文字效果比较差及其原因，主要由以下三点：</p>
<ol>
<li>模型的训练数据集，如 LAION-5B 缺乏文本内容的手动注释或者OCR结果。</li>
<li>开源的模型使用的文本编码器采用基于词汇的分词器，无法直接访问字符。</li>
<li>大多数扩散模型的loss中缺乏对文本区域的专门监督。</li>
</ol>
<p>作者提出的 AnyText 框架，在 text-control diffusion pipeline的基础上，增加了两个组件：辅助潜在模块将文本字形、位置和遮罩图像等辅助信息编码到潜在空间中以辅助文本生成和编辑；文本嵌入模块采用 OCR 模型将笔画信息编码为嵌入，然后与来自分词器的图像caption embedding 融合，以呈现与背景无缝混合的文本；最后，引入图像空间中的文本感知损失以进一步提高书写准确性。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116113204422.png" alt="image-20240116113204422"></p>
<p>模型的loss包含两个部分：<img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116113426377.png" alt="image-20240116113426377"></p>
<p>第一部分损失如下：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116113559130.png" alt="image-20240116113559130"></p>
<p>第二部分损失是 text perceptual loss，利用 文字的位置条件 $lp$，准确定位到生成文本的区域，利用 PP-OCRv3 模型通过裁剪、仿射变换、填充和归一化等操作对位置 $l_p$ 处的原始图像$x_0$ 和 去噪重建后图像 $x^\prime_0$ 进行处理, 利用全连接层之前的特征图 $\hat{m}_p$ 和 $\hat{m}_{p}^\prime$分别表示原始图像和预测图像中位置 p 处的文本书写信息。文本感知损失表示为</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116115109747.png" alt="image-20240116115109747"></p>
<p>Auxiliary latent module 利用三种类型的辅助条件来产生 latent feature amp $z_a$ 分别是，字形 $l_g$，位置 $l_p$，masked image $l_m$。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116114237703.png" alt="image-20240116114237703"></p>
<p>$f$ 是一个卷积的fusion layer。$z_a$ 的  channels 数量和 $z_t$ 一致。</p>
<p>Text embedding module 将字形线渲染到图像中，利用预先训练的视觉模型，PP-OCRv3 的识别模型对字形信息进行编码，并从caption标记中替换它们的嵌入，然后一起送到 基于 Transformer 的文本编码器中。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>论文开源了一个数据集用于文本生成 AnyWord-3M</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116141058257.png" alt="image-20240116141058257"></p>
<p>可以看到 OCR 的信息对于效果提升是最大的。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2024/01/02/ECLIPSE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/02/ECLIPSE/" class="post-title-link" itemprop="url">ECLIPSE——A Resource-Efficient Text-to-Image Prior for Image Generations</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-01-02 21:00:00 / Modified: 23:08:00" itemprop="dateCreated datePublished" datetime="2024-01-02T21:00:00+08:00">2024-01-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ECLIPSE-A-Resource-Efficient-Text-to-Image-Prior-for-Image-Generations"><a href="#ECLIPSE-A-Resource-Efficient-Text-to-Image-Prior-for-Image-Generations" class="headerlink" title="ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations"></a>ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations</h1><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240104202314415.png" alt="image-20240104202314415"></p>
<h2 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h2><p>目前 unCLIP 类型的文生图模型，如 DALLE2，kandinsky2.2， Karlo，<font color=red>先验模型参数太大，对计算资源和训练数据的要求比较高</font>。论文利用对比学习的方法，来训练先验模型，仅使用 3.3%参数和2.8%的数据进行训练就能超过baseline的先验模型，且可以和预训练的扩散图像解码器搭配使用。</p>
<p><strong>现有 diffusion prior model 的问题</strong></p>
<p>论文通过实验说明 prior model steps的增加并不能提高最终生成图片的质量。因此扩散的训练方式存在很多不必要的计算。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240105191501550.png" alt="image-20240105191501550"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240105191509560.png" alt="image-20240105191509560"></p>
<p>图a是</p>
<h2 id="2、方法"><a href="#2、方法" class="headerlink" title="2、方法"></a>2、方法</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240105115308506.png" alt="image-20240105115308506"></p>
<p>论文采用非扩散的训练方式，目标函数有两个，第一个目标函数如下：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240105193153414.png" alt="image-20240105193153414"></p>
<p>将文本embedding投影到视觉embedding上，通过以上近似扩散先验模型的目标函数来实现。这里没有 CFG。但这种相当于直接学习一个函数将文本embedding映射成视觉embedding，泛化性可能比较差。</p>
<p>第二个目标函数如下：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240105194510830.png" alt="image-20240105194510830"></p>
<p>利用对比损失来对齐图像和文本。</p>
<p>最终的损失函数为：<img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240105194826376.png" alt="image-20240105194826376">$\lambda$ 设置为 0.2</p>
<h2 id="3、实验"><a href="#3、实验" class="headerlink" title="3、实验"></a>3、实验</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240105195206469.png" alt="image-20240105195206469"></p>
<p>可以看到 eclipse的方式大大减少了使用的数据量和模型的参数。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240105195231282.png" alt="image-20240105195231282"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240105195311653.png" alt="image-20240105195311653"></p>
<p>对比损失对于图像和文本之间的对齐是有帮助的。</p>
<h2 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a>4、总结</h2><p>unclip的模型结构，decoder的训练直接通过一个image encoder来编码图像得到image embedding，先验模型的任务理论上就是学习一个模型来对齐文本和图像之间的embedding，对齐的越好，那么对于decoder生成越有帮助。并非只能通过扩散的方式来学习先验模型。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2023/12/30/%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/30/%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B/" class="post-title-link" itemprop="url">文生图模型演进</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-30 21:00:00 / Modified: 23:08:00" itemprop="dateCreated datePublished" datetime="2023-12-30T21:00:00+08:00">2023-12-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>此文是为了更好的理解 stable diffusion以及DALL-E 3等最新的图像生成模型，回顾一下在它们之前更早的模型。stable diffusion的作者也是 VQ-GAN的作者，DALL-E3之前还有 DALL-E，DALL-E2。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/640.png" alt="图片"></p>
<h2 id="2-AE"><a href="#2-AE" class="headerlink" title="2. AE"></a>2. AE</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-120f54237d6bc3529d54270139ea8276_r.jpg" alt="img"></p>
<p><strong>作用</strong>：可以用于学习<font color=red> 无标签数据的有效编码</font>, 学习对高维数据的进行低维表示，常用于<font color=red>降维</font>，数据去噪，特征学习。</p>
<p><strong>基本思想</strong>：AE（auto-encoder）是很早之前的技术了，思路也非常简单：用一个编码器（Encoder）把输入编码为 latent vector；然后用 Decoder 将其解码为重建图像，<font color=red>希望重建后的图像与输入图像越接近越好</font>。通常 latent vector 的维度比输入、输出的维度小，因此称之为 bottleneck。AE 是一个自重建的过程，所以叫做“自-编码器”。</p>
<p><strong>特点</strong>：但是模型在 Latent Space 没有增加任何的约束或者正则化，意味着<font color=red>不知道 Latent Space 是如何构建的</font>, 所以很难使用 latent space 来采样生成一个新的图像。</p>
<h2 id="DAE"><a href="#DAE" class="headerlink" title="DAE"></a>DAE</h2><p>DAE（Denoising autoencoder）将原始输入图像进行一定程度的打乱，得到 corrupted input。然后把后者输入AE，目标仍然是希望重建后的图像与原始输入越接近越好。DAE 的效果很不错，原因之一就是图像的冗余度太高了，即使添加了噪声，模型依然能抓取它的特征。而这种方式增强了模型的鲁棒性，防止过拟合。</p>
<h2 id="3-VAE"><a href="#3-VAE" class="headerlink" title="3. VAE"></a>3. VAE</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-9fdb71e7e0b359f900a54878be5f2477_r.jpg" alt="img"></p>
<p><strong>论文</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6114">[1312.6114] Auto-Encoding Variational Bayes</a></p>
<p><strong>作用</strong>：除了AE的作用之外，还广泛应用于生成新的、与训练数据相似但不完全相同的样本。</p>
<p><strong>基本思想</strong>：VAE（Variational autoencoder）仍然由一个编码器和一个解码器构成，并且目标仍然是重建原始输入。但中间不再是学习 latent vector z ，而是学习它的后验分布 $p(z|x)$ ，并假设它遵循<font color=red>多维高斯分布</font>。具体来说，编码器得到两个输出，并分别作为高斯分布的均值和协方差矩阵的对角元（假设协方差矩阵是对角矩阵）。然后在这个高斯分布中采样，送入解码器。实际工程实现中，会用到重参数化（reparameterization）的技巧。</p>
<p><strong>特点</strong>：VAE训练目标除了重构误差，还包括最小化隐空间的KL散度，以确保隐空间与标准正态分布接近。但VAE最大的问题也是这个，使用了固定的先验分布。</p>
<h2 id="4-VQ-VAE"><a href="#4-VQ-VAE" class="headerlink" title="4. VQ-VAE"></a>4. VQ-VAE</h2><blockquote>
<p><strong>VQ(vector quantization)</strong>是一种数据压缩和量化的技术，它可以将连续的向量映射到一组离散的具有代表性的向量中。在深度学习领域，VQ通常用来将连续的隐空间表示映射到一个有限的、离散的 <strong>codebook</strong> 中。</p>
</blockquote>
<p>VAE 具有一个最大的问题就是使用了固定的先验（高斯分布），其次是使用了<strong>连续的中间表征，导致模型的可控性差</strong>。为了解决这个问题，VQ-VAE（Vector Quantized Variational Autoencoder）选择使用<strong>离散的中间表征</strong>，同时，通常会使用一个自回归模型来学习先验（例如 PixelCNN），在训练完成后，用来采样得到 $z_e$。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-956ed05aed4340fd44c2a2853800edf0_r.jpg" alt="img"></p>
<p>图像首先经过encoder，得到$z_e$,它是$H\times W$个 $D$ 维向量。$e_1,e_2,…,e_k$是 $K$ 个 $D$ 维向量，称为codebook。 对于 $z_e$ 中的每个 $D$ 维向量，都可以在 codebook 中找到最接近的 $e_i$, 构成 $z_q$, 这就是decoder的输入。一般 $k=8192, D=512 or 768$。</p>
<p>从 $z_e(x)$ 到 $z_q(x)$ 这个变化可以看成一个聚类，$e_1,e_2,…,e_k$ 可以看作 K 个聚类中心。这样把 encoder 得到的 embedding 离散化了，只由聚类中心表示。</p>
<h3 id="4-1-VQ-VAE的训练"><a href="#4-1-VQ-VAE的训练" class="headerlink" title="4.1 VQ-VAE的训练"></a>4.1 VQ-VAE的训练</h3><p>在VQ中使用 Argmin来获取最小的距离，这一步是不可导的，因为无法将 Decoder 和 Encoder联合训练，针对这个问题，作者添加了一个trick，如上图红线所示：直接将 $z_q(x)$的梯度cooy给$Z_e(x)$, 而不是给 codebook里面的embedding。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E6%8E%A8%E6%88%AA%E5%9B%BE_20240112000325.jpg" alt="推推截图_20240112000325"></p>
<p>VQ-VAE的loss如上所示，分成三部分，第一项用来训练 encoder和decoder，第二项叫 codebook loss,只训练 codebook，让codebook中的embedding向各自最近的$Z_e(x)$靠近VQGAN。第三项叫 commitment loss,只训练encoder, 目的是encourage the output of encoder to stay close to the chosen codebook vector to prevent it from flucturating too frequently from one code vector to another, 即防止encoder的输出频繁在各个codebook embedding之间跳</p>
<p>具体代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VectorQuantizer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_embeddings, embedding_dim, commitment_cost</span>):</span><br><span class="line">        <span class="built_in">super</span>(VectorQuantizer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self._embedding_dim = embedding_dim</span><br><span class="line">        self._num_embeddings = num_embeddings</span><br><span class="line">        </span><br><span class="line">        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)</span><br><span class="line">        self._embedding.weight.data.uniform_(-<span class="number">1</span>/self._num_embeddings, <span class="number">1</span>/self._num_embeddings)</span><br><span class="line">        self._commitment_cost = commitment_cost</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># convert inputs from BCHW -&gt; BHWC</span></span><br><span class="line">        inputs = inputs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        input_shape = inputs.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Flatten input</span></span><br><span class="line">        flat_input = inputs.view(-<span class="number">1</span>, self._embedding_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate distances</span></span><br><span class="line">        distances = (torch.<span class="built_in">sum</span>(flat_input**<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">                    + torch.<span class="built_in">sum</span>(self._embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">                    - <span class="number">2</span> * torch.matmul(flat_input, self._embedding.weight.t()))</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Encoding</span></span><br><span class="line">        encoding_indices = torch.argmin(distances, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        encodings = torch.zeros(encoding_indices.shape[<span class="number">0</span>], self._num_embeddings, device=inputs.device)</span><br><span class="line">        encodings.scatter_(<span class="number">1</span>, encoding_indices, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Quantize and unflatten</span></span><br><span class="line">        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loss</span></span><br><span class="line">        e_latent_loss = F.mse_loss(quantized.detach(), inputs)</span><br><span class="line">        q_latent_loss = F.mse_loss(quantized, inputs.detach())</span><br><span class="line">        loss = q_latent_loss + self._commitment_cost * e_latent_loss</span><br><span class="line">        </span><br><span class="line">        quantized = inputs + (quantized - inputs).detach()</span><br><span class="line">        avg_probs = torch.mean(encodings, dim=<span class="number">0</span>)</span><br><span class="line">        perplexity = torch.exp(-torch.<span class="built_in">sum</span>(avg_probs * torch.log(avg_probs + <span class="number">1e-10</span>)))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert quantized from BHWC -&gt; BCHW</span></span><br><span class="line">        <span class="keyword">return</span> loss, quantized.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous(), perplexity, encodings</span><br></pre></td></tr></table></figure>
<h3 id="4-2-VQ-VAE-PixelCNN"><a href="#4-2-VQ-VAE-PixelCNN" class="headerlink" title="4.2 VQ-VAE + PixelCNN"></a>4.2 VQ-VAE + PixelCNN</h3><p>有了上述的 VQ-VAE 模型，可以很容易实现图像<strong>压缩、重建</strong>的目的，但是无法生成新的图像数据。当然可以随机生成 Index，然后对应生成量化后的 latent code，进而使用 Decoder 来生成输出图像。但是这样的 latent code 完全没有全局信息甚至局部信息，因为每个位置都是随机生成的。因此，作者引入了 PixelCNN 来自回归的生成考虑了全局信息的 latent code，进而可以生成更真实的图像，如下图所示：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240112001504547.png" alt="image-20240112001504547"></p>
<p>PixelCNN 和 VQ-VAE 的一作是同一个人，来自 Google DeepMind，对应的论文为：Conditional Image Generation with PixelCNN Decoders。此处我们不再对 PixelCNN 展开，只需要知道它是一个自回归生成模型，可以逐个像素的生成，因为其是自回归模型，所以每个位置都能看到之前位置的信息，这样生成的 latent code 能够更全面的考虑到空间信息，有助于提高模型生成图像的质量和多样性。</p>
<h3 id="4-3-VQ-VAE-2"><a href="#4-3-VQ-VAE-2" class="headerlink" title="4.3 VQ-VAE-2"></a>4.3 VQ-VAE-2</h3><p>VQ-VAE-2 的模型结构如下图所示，以 256x256 的图像压缩重建为例：</p>
<ul>
<li>训练阶段：其首先使用 Encoder 将图像压缩到 Bottom Level，对应大小为 64x64，然后进一步使用 Encoder 压缩到 Top Level，大小为 32x32。重建时，首先将 32x32 的表征经过 VQ 量化为 latent code，然后经过 Decoder 重建 64x64 的压缩图像，再经过 VQ 和 Decoder 重建 256x256 的图像。</li>
<li>推理阶段（图像生成）：使用 PixelCNN 首先生成 Top Level 的离散 latent code，然后作为条件输入 PixelCNN 以生成 Bottom Level 的更高分辨率的离散 latent code。之后使用两个 Level 的离散 latent code 生成最终的图像。</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/640-20240112002031130.png" alt="图片"></p>
<p>当然，基于这个思想作者也进一步验证了使用 3 个 Level 来生成 1024x1024 分辨率的图像，相应的压缩分辨率分别为 128x128、64x64、32x32。</p>
<h2 id="5-VQ-GAN"><a href="#5-VQ-GAN" class="headerlink" title="5 VQ-GAN"></a>5 VQ-GAN</h2><h3 id="5-1-概述"><a href="#5-1-概述" class="headerlink" title="5.1 概述"></a>5.1 概述</h3><p>paper:<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html">https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html</a></p>
<p>code:<a target="_blank" rel="noopener" href="https://github.com/CompVis/taming-transformers">https://github.com/CompVis/taming-transformers</a></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230512112944268.png" alt="image-20230512112944268"></p>
<p>VQ-GAN 相比较 VQ-VAE 的主要改变有以下几点：</p>
<ul>
<li>引入 GAN 的思想，将 VQ-VAE 当做生成器（Generator），并加入判别器（Discriminator），以对生成图像的质量进行判断、监督，以及加入感知重建损失（不只是约束像素的差异，还约束 feature map 的差异），以此来重建更具有保真度的图片，也就学习了更丰富的 codebook。</li>
<li>将 PixelCNN 替换为性能更强大的自回归 GPT2 模型（针对不同的任务可以选择不同的规格）</li>
<li>引入滑动窗口自注意力机制，以降低计算负载，生成更大分辨率的图像。</li>
</ul>
<p>VQ-GAN 也是 stable diffusion的作者。</p>
<h3 id="5-2-方法"><a href="#5-2-方法" class="headerlink" title="5.2 方法"></a>5.2 方法</h3><p>模型结构如上图所示，实际训练的时候是分为<font color=red>两阶段训练的</font>。</p>
<p>如下图所示，第一阶段训练，相比 VQ-VAE 主要是增加 Discriminator， 以及将重建损失替换成 <font color=red> LPIPS损失：</font></p>
<ul>
<li>Discriminator：对生成的图像块进行判别，每一块都会返回 True 和 False，然后将对应的损失加入整体损失中。</li>
<li>LPIPS：除了像素级误差外，也会使用 VGG 提取 input 图像和 reconstruction 图像的多尺度 feature map，以监督对应的误差（具体可参考 lpips.py - CompVis/taming-transformers · GitHub）。</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113215830827.png" alt="image-20240113215830827"></p>
<h2 id="6-DALL-E-dVAE-DALL-E"><a href="#6-DALL-E-dVAE-DALL-E" class="headerlink" title="6. DALL-E (dVAE, DALL-E)"></a>6. DALL-E (dVAE, DALL-E)</h2><h3 id="6-1-概述"><a href="#6-1-概述" class="headerlink" title="6.1 概述"></a>6.1 概述</h3><p>DALL-E 最主要的贡献是提供了不错的文本引导图片生成的能力，其不是在 VQ-VAE 基础上修改，而是首先引入 VAE 的变种 dVAE，然后在此基础上进一步训练 DALL-E。可惜的是，OpenAI 并不 Open，只开源了 dVAE 部分模型，文本引导生成部分并没有开源，不过 Huggingface 和 Google Cloud 团队进行了复现，并发布对应的 DALL-E mini 模型。</p>
<p>DALL-E 对应的论文为：[2102.12092] Zero-Shot Text-to-Image Generation。对应的代码库为：GitHub - openai/DALL-E: PyTorch package for the discrete VAE used for DALL·E.。</p>
<p>DALL-E mini 对应的文档为：DALL-E Mini Explained，对应的代码库为：GitHub - borisdayma/dalle-mini: DALL·E Mini - Generate images from a text prompt。</p>
<h3 id="6-2-dVAE"><a href="#6-2-dVAE" class="headerlink" title="6.2 dVAE"></a>6.2 dVAE</h3><p>dVAE（discrete VAE）与VQ-VAE的区别在于<font color=red>引入 Gumbel Softmax 来训练</font>, 避免 VQ-VAE 训练中 ArgMin 不可导的问题。 </p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113222402539.png" alt="image-20240113222402539"></p>
<h3 id="6-3-模型训练"><a href="#6-3-模型训练" class="headerlink" title="6.3 模型训练"></a>6.3 模型训练</h3><p>有了 dVAE 模型之后，第二阶段就是就是训练 Transformer（此阶段会固定 dVAE），使其具备文本引导生成的能力。DALL-E 使用大规模的图像-文本对数据集进行训练，训练过程中使用 dVAE 的 Encoder 将图像编码为离散的 latent code。然后将文本输入 Transformer，并使用生成的 latent code 来作为 target 输出。以此就可以完成有监督的自回归训练。推理时只需输入文本，然后逐个生成图像对应的 Token，直到生成 1024 个，然后将其作为离散的 latent code 进一步生成最终图像。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113222517785.png" alt="image-20240113222517785"></p>
<p>最终作者在 1024 个 16G 的 V100 GPU 上完成训练，batch size 为 1024，总共更新了 430,000 次模型，也就相当于训练了 4.3 亿图像-文本对（训练集包含 250M 图像-文本对，主要是 Conceptual Captions 和 YFFCC100M）。</p>
<h3 id="6-4-DALL-E-mini-模型概述"><a href="#6-4-DALL-E-mini-模型概述" class="headerlink" title="6.4 DALL-E mini 模型概述"></a>6.4 DALL-E mini 模型概述</h3><p>如下图所示，DALL-E mini 中作者使用 VQ-GAN 替代 dVAE，使用 Encoder + Decoder 的 BART 替代 DALL-E 中 Decoder only 的 Transformer。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113222725076.png" alt="dalle-e mini infer"></p>
<p>在推理过程中，不是生成单一的图像，而是会经过采样机制生成多个 latent code，并使用 VQ-GAN 的 Decoder 生成多个候选图像，之后再使用 CLIP 提取这些图像的 embedding 和文本 embedding，之后进行比对排序，挑选出最匹配的生成结果。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113222818173.png" alt="image-20240113222818173"></p>
<h3 id="6-5-DALL-E-mini-和-DALL-E-对比"><a href="#6-5-DALL-E-mini-和-DALL-E-对比" class="headerlink" title="6.5 DALL-E mini 和 DALL-E 对比"></a>6.5 DALL-E mini 和 DALL-E 对比</h3><p>DALL-E mini 和 DALL-E 在模型、训练上都有比较大的差异，具体体现在：</p>
<ul>
<li>DALL-E 使用 12B 的 GPT-3 作为 Transformer，而 mini 使用的是 0.4B 的 BART，小 27 倍。</li>
<li>mini 中使用预训练的 VQ-GAN、BART 的 Encoder 以及 CLIP，而 DALL-E 从头开始训练，mini 训练代价更小。</li>
<li>DALL-E 使用 1024 个图像 Token，词表更小为 8192，而 mini 使用 256 个图像 Token，词表大小为 16384。</li>
<li>DALL-E 支持最多 256 个文本 Token，对应词表为 16,384，mini 支持最多 1024 文本 Token，词表大小为 50,264。</li>
<li>mini 使用的 BART 是 Encoder + Decoder 的，因此文本是使用双向编码，也就是每个文本 Token 都能看到所有文本 Token，而 DALL-E 是 Decoder only 的 GPT-3，文本 Token 只能看到之前的 Token。</li>
<li>DALL-E 使用 250M 图像-文本对训练，而 mini 只使用了 15M。</li>
</ul>
<h2 id="7-CLIP-VQ-GAN-VQGAN-CLIP"><a href="#7-CLIP-VQ-GAN-VQGAN-CLIP" class="headerlink" title="7 CLIP+VQ-GAN(VQGAN-CLIP)"></a>7 CLIP+VQ-GAN(VQGAN-CLIP)</h2><p>Katherine 等人将 VQ-GAN 和 OpenAI 发布的 CLIP 模型结合起来，利用 CLIP 的图文对齐能力来赋予 VQ-GAN 文本引导生成的能力。其最大的优势是不需要额外的预训练，也不需要对 CLIP 和 VQ-GAN 进行微调，只需在推理阶段执行少量的迭代即可实现。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113223222191.png" alt="image-20240113223222191"></p>
<p>如上图所示：使用初始图像通过 VQ-GAN 生成一个图像，然后使用 CLIP 对生成图像和 Target Text 提取 embedding，然后计算相似性，并将其误差作为反馈对隐空间的 Z-vector 进行迭代更新，直到生成图像和 Target Text 对应的 embedding 很相似为止。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.00937">https://arxiv.org/abs/1711.00937</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.05328">https://arxiv.org/abs/1606.05328</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.00446">https://arxiv.org/abs/1906.00446</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.09841">https://arxiv.org/abs/2012.09841</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/CompVis/taming-transformers">https://github.com/CompVis/taming-transformers</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.12092">https://arxiv.org/abs/2102.12092</a></li>
<li><a target="_blank" rel="noopener" href="https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained--Vmlldzo4NjIxODA">https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained--Vmlldzo4NjIxODA</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/borisdayma/dalle-mini">https://github.com/borisdayma/dalle-mini</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.08583">https://arxiv.org/abs/2204.08583</a></li>
<li><a target="_blank" rel="noopener" href="https://python.plainenglish.io/variational-autoencoder-1eb543f5f055">https://python.plainenglish.io/variational-autoencoder-1eb543f5f055</a></li>
<li><a target="_blank" rel="noopener" href="https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/">https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2023/12/20/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/20/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-20 00:09:14" itemprop="dateCreated datePublished" datetime="2023-12-20T00:09:14+08:00">2023-12-20</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2023/08/19/Stable%20DIffusion%20%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/19/Stable%20DIffusion%20%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B/" class="post-title-link" itemprop="url">Stable Diffusion 系列模型</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-19 21:00:00" itemprop="dateCreated datePublished" datetime="2023-08-19T21:00:00+08:00">2023-08-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-12-19 23:08:00" itemprop="dateModified" datetime="2023-12-19T23:08:00+08:00">2023-12-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Stable-Diffusion-系列模型"><a href="#Stable-Diffusion-系列模型" class="headerlink" title="Stable Diffusion 系列模型"></a>Stable Diffusion 系列模型</h1><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116234259305.png" alt="image-20240116234259305"></p>
<p>stable diffusion系列从 LDM 到SDXL 一直是开源工作里传播和使用最广的，大量工作都在此基础上展开。</p>
<p>从 2021年5月 openai 发表的 DM beat GANS 开始，diffusion model 的效果开始超过传统的 GAN 模型，进一步推动了 DM 在图像生成领域的应用。</p>
<p>早期的 DM 作用于像素空间，训练和推理的成本很高，为了实现在有效的计算资源上训练 DM，同时保持其质量和灵活性，作者提出将 DM 应用于强大的预训练 AutoEncoder 的隐空间（Latent Space），这也就是为什么提出的模型叫 <strong>LDM</strong>。此外，作者还在模型中引入交叉注意力层，可以将文本、边界框等条件很方便地引入到模型中，将 DM 转化为强大而灵活的生成器，实现高分辨率的生成。作者提出的 LDM 模型同样在图像修复、类别条件生成等方面取得很好的效果，同时与基于像素空间的扩散模型相比，大大降低计算要求。</p>
<h2 id="2-演进"><a href="#2-演进" class="headerlink" title="2. 演进"></a>2. 演进</h2><h3 id="2-1-Latent-Diffusion"><a href="#2-1-Latent-Diffusion" class="headerlink" title="2.1 Latent Diffusion"></a>2.1 Latent Diffusion</h3><p>Stable Diffusion 之前的版本，对应的正是论文的开源版本，位于代码库 High-Resolution Image Synthesis with Latent Diffusion Models 中。</p>
<p>该版本发布于 2022 年 4 月，主要包含三个模型：</p>
<ul>
<li>文生图模型：基于 LAION-400M 数据集训练，包含 1.45B 参数。</li>
<li>图像修复模型：指定区域进行擦除。</li>
<li>基于 ImageNet 的类别生成模型：在 ImageNet 上训练，指定类别条件生成，获得了 3.6 的 FID 分数。使用了 Classifier Free Guidance 技术。</li>
</ul>
<p>代码实现参考了 OpenAI 的 Diffusion Models Beat GANs 代码实现。</p>
<h3 id="2-2-stable-diffusion-V1"><a href="#2-2-stable-diffusion-V1" class="headerlink" title="2.2 stable diffusion V1"></a>2.2 stable diffusion V1</h3><p>V1 系列的模型结构和LDM类似基本没有变化；</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-3123494fa758b5fc404f42752e2f1101_r.jpg" alt="img"></p>
<p>该版本发布于 2022 年 8 月，该模型包含 2 个子模型：</p>
<ul>
<li>AutoEncoder 模型：U-Net，8 倍下采样，包含 860M 参数。</li>
<li>Text Encoder 模型：使用 CLIP ViT-L/14 中的 Text encoder。</li>
</ul>
<p>模型首先在 256x256 的分辨率下训练，然后在 512x512 的分辨率下微调。总共包含 4 个子版本：</p>
<ul>
<li>sd-v1-1.ckpt：<ul>
<li>在 LAION-2B-en 数据集上以 256x256 分辨率训练 237k step。</li>
<li>在 LAION-high-resolution（LAION-5B 中超过 1024x1024 分辨率的 170M 样本）上以 512x512 分辨率继续训练 194k step。</li>
</ul>
</li>
<li>sd-v1-2.ckpt：<ul>
<li>复用 sd-v1-1.ckpt，在 LAION-aesthetics v2 5+（LAION-2B-en 中美观度分数大于 5.0 的子集） 上以 512x512 分辨率继续训练 515k step。</li>
</ul>
</li>
<li>sd-v1-3.ckpt：<ul>
<li>复用 sd-v1-2.ckpt，在 LAION-aesthetics v2 5+ 上以 512x512 分辨率继续训练 195k step，使用了 Classifier Free Guidance 技术，以 10% 概率删除文本条件。</li>
</ul>
</li>
<li>sd-v1-4.ckpt：<ul>
<li>复用 sd-v1-2.ckpt，在 LAION-aesthetics v2 5+ 上以 512x512 分辨率继续训练 225k step，使用了 Classifier Free Guidance 技术，以 10% 概率删除文本条件。</li>
</ul>
</li>
</ul>
<p>对应的 FID 和 CLIP 分数如下图所示：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116235722441.png" alt="image-20240116235722441"></p>
<h3 id="2-3-Stable-Diffusion-V1-5"><a href="#2-3-Stable-Diffusion-V1-5" class="headerlink" title="2.3 Stable Diffusion V1.5"></a>2.3 Stable Diffusion V1.5</h3><p>Stable Diffusion 的 V1.5 版本，由runway发布，</p>
<p>该版本发布于 2022 年 10 月，主要包含两个模型：</p>
<ul>
<li>sd-v1-5.ckpt：<ul>
<li>复用 sd-v1-2.ckpt，在 LAION-aesthetics v2 5+ 上以 512x512 分辨率继续训练 595k step，使用了 Classifier Free Guidance 技术，以 10% 概率删除文本条件。</li>
</ul>
</li>
<li>sd-v1-5-inpainting.ckpt：<ul>
<li>复用 sd-v1-5.ckpt，在 LAION-aesthetics v2 5+ 上以 512x512 分辨率以 inpainting 训练了 440k step，使用 Classifier Free Guidance 技术，以 10% 概率删除文本条件。在 U-Net 的输入中额外加了 5 个 channel，4 个用于 masked 的图像，1 个用于 mask 本身。</li>
</ul>
</li>
</ul>
<h3 id="2-4-Stable-Diffusion-V2"><a href="#2-4-Stable-Diffusion-V2" class="headerlink" title="2.4 Stable Diffusion V2"></a>2.4 Stable Diffusion V2</h3><p>Stable Diffusion 的 V2 版本，由 Stability-AI 发布</p>
<p>V2 包含三个子版本，分别为 v2.0，v2.1 和 Stable UnCLIP 2.1：</p>
<ul>
<li>v2.0：<ul>
<li>发布于 2022 年 11 月，U-Net 模型和 V1.5 相同，Text encoder 模型换成了 OpenCLIP-ViT/H 中的 text encoder。</li>
<li>SD 2.0-base：分别率为 512x512</li>
<li>SD 2.0-v：基于 2.0-base 微调，分辨率提升到 768x768，同时利用 [2202.00512] Progressive Distillation for Fast Sampling of Diffusion Models 提出的技术大幅降低 Diffusion 的步数。</li>
<li>发布了一个文本引导的 4 倍超分模型。</li>
<li>基于 2.0-base 微调了一个深度信息引导的生成模型。</li>
<li>基于 2.0-base 微调了一个文本信息引导的修复模型。</li>
</ul>
</li>
<li>v2.1：<ul>
<li>发布于 2022 年 12 月，模型结构和参数量都和 v2.0 相同。并在 v2.0 的基础上使用 LAION 5B 数据集（较低的 NSFW 过滤约束）微调。同样包含 512x512 分辨率的 v2.1-base 和 768x768 分辨率的 v2.1-v。</li>
</ul>
</li>
<li>Stable UnCLIP 2.1：<ul>
<li>发布于 2023 年 3 月，基于 v2.1-v（768x768 分辨率） 微调，参考 OpenAI 的 DALL-E 2（也就是 UnCLIP），可以更好的实现和其他模型的联合，同样提供基于 CLIP ViT-L 的 Stable unCLIP-L 和基于 CLIP ViT-H 的 Stable unCLIP-H</li>
</ul>
</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/640-20240117000025136.png" alt="图片"></p>
<p>从结果来看，V2版本相对于V1.5提升比较明显，但是实际体验效果比较一般，而且V2版本难以finetune。</p>
<h3 id="3-4-Stable-Diffusion-XL"><a href="#3-4-Stable-Diffusion-XL" class="headerlink" title="3.4  Stable Diffusion XL"></a>3.4  Stable Diffusion XL</h3><p>Stable Diffusion 的 XL 版本，由 Stability-AI 发布，位于代码库 Generative Models by Stability AI。</p>
<p>该版本发布于 2023 年 06 月，主要包含两个模型：</p>
<ul>
<li>SDXL-base-0.9：基于多尺度分辨率训练，最大分辨率 1024x1024，包含两个 Text encoder，分别为 OpenCLIP-ViT/G 和 CLIP-ViT/L。</li>
<li>SDXL-refiner-0.9：用来生成更高质量的图像，不应直接使用，此外文本条件只使用 OpenCLIP 中的 Text encoder。</li>
</ul>
<p>2023 年 07 月发布 1.0 版本，同样对应两个模型：</p>
<ul>
<li>SDXL-base-1.0：基于 SDXL-base-0.9 改进。</li>
<li>SDXL-refiner-1.0：基于 SDXL-refiner-0.9 改进。</li>
</ul>
<p>2023 年 11 月发表 SDXL-Trubo 版本，通过引入蒸馏技术来优化加速的版本。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2023/04/21/EVA02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/21/EVA02/" class="post-title-link" itemprop="url">EVA02解析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-04-21 21:03:00" itemprop="dateCreated datePublished" datetime="2023-04-21T21:03:00+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-22 23:03:00" itemprop="dateModified" datetime="2023-04-22T23:03:00+08:00">2023-04-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="EVA-02-A-Visual-Representation-for-Neon-Genesis"><a href="#EVA-02-A-Visual-Representation-for-Neon-Genesis" class="headerlink" title="EVA-02: A Visual Representation for Neon Genesis"></a>EVA-02: A Visual Representation for Neon Genesis</h1><p>EVA02的目标是作为下一代的基于Transformer的视觉表示模型。</p>
<p>文章主要包含两个部分：1、对普通Vit的架构改进，2、MIM的预训练策略</p>
<p><strong>总结</strong></p>
<p>EVA02主要有两个改进，一是，通过实验的方法来观察采用哪些NLP方向关于Vit的改进；二是，增加视觉特征编码的容量以及增加训练的轮数和图像的size；</p>
<h2 id="1-Architecture"><a href="#1-Architecture" class="headerlink" title="1 Architecture"></a>1 Architecture</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230629182751431.png" alt="image-20230629182751431"></p>
<p>ViT主要由 MHSA（用于全局空间信息聚合）和 pointwise的FFNs（特征变换）交错组成。但是NLP方面很多针对ViT的修改没有在视觉上应用。作者做了一个实验来探索不同的修改带来的影响：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230703191529244.png" alt="image-20230703191529244"></p>
<p><strong>Gelu</strong>: $GELU(x)=x * \phi(x),x \sim N(0,1)$</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-0d8b979444f64ab49d4bc0f4199a15c2_r.jpg" alt="img"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%9B%BE20230914183157.png" alt="截图20230914183157"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-e31b1e5b4333a90fbcca03c9a863a0c5_r.jpg" alt="img"></p>
<h2 id="2-pre-trainning-strategy"><a href="#2-pre-trainning-strategy" class="headerlink" title="2 pre-trainning strategy"></a>2 pre-trainning strategy</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230703195140612.png" alt="image-20230703195140612"></p>
<p><strong>MIM teacher model变大，训练的epoch也需要变多；</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230703195449659.png" alt="image-20230703195449659"></p>
<p><strong>分辨率的增加以及在imgnet数据上的有监督ft也会增加性能</strong></p>
<p>预训练目标类似于 EVA [44]，即仅以可见图像块为条件回归屏蔽图像文本对齐的视觉特征。我们使用 [MASK] 标记破坏输入补丁，并按照 [5, 44] 使用掩码率为 40% 的分块掩码。</p>
<p>MIM 预训练的目标表示来自可公开访问的 EVA-CLIP [44] 视觉塔，具有 10 亿个参数。 EV A-02 的输出特征首先被归一化 [4]，然后通过线性层投影到与 EVA-CLIP 的视觉特征相同的维度。我们使用负余弦相似度作为损失函数。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2023/03/21/EMA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/21/EMA/" class="post-title-link" itemprop="url">EMA原理和pytorch实现</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-21 21:03:00" itemprop="dateCreated datePublished" datetime="2023-03-21T21:03:00+08:00">2023-03-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-22 23:03:00" itemprop="dateModified" datetime="2023-03-22T23:03:00+08:00">2023-03-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="EMA的原理和pytorch实现"><a href="#EMA的原理和pytorch实现" class="headerlink" title="EMA的原理和pytorch实现"></a>EMA的原理和pytorch实现</h1><h2 id="EMA-定义"><a href="#EMA-定义" class="headerlink" title="EMA 定义"></a>EMA 定义</h2><p>指数移动平均也叫权重移动平均，是一种给予近期数据更高权重的平均方法。</p>
<h2 id="在深度学习的优化中的EMA"><a href="#在深度学习的优化中的EMA" class="headerlink" title="在深度学习的优化中的EMA"></a>在深度学习的优化中的EMA</h2><p>在深度学习的优化过程中，$\theta_t$ 是 t 时刻的模型权重 weight， $\upsilon_t$ 是 t 时刻的影子权重。在梯度下降的过程中，会一直维护着这个影子权重，但是这个影子权重不会参与训练。基本的假设是<strong>模型权重在最后 n 步内，会在实际的最优点处抖动，所以我们去最后 n 步的平均，能使模型更加的鲁棒。</strong></p>
<h3 id="EMA为何有效"><a href="#EMA为何有效" class="headerlink" title="EMA为何有效"></a>EMA为何有效</h3><p>因为在训练的时候，会使用验证集来衡量模型精度，但是验证集和测试集并不完全一致，在训练后期阶段，模型可能已经在测试集最佳精度附近波动，所以使用ema的结果会比使用单一结果更可靠。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230307191607861.png" alt="image-20230307191607861"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230307191620727.png" alt="image-20230307191620727"></p>
<h3 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EMA</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, decay</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.decay = decay</span><br><span class="line">        self.shadow = &#123;&#125;</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">register</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                self.shadow[name] = param.data.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.shadow</span><br><span class="line">                new_average = (<span class="number">1.0</span> - self.decay) * param.data + self.decay * self.shadow[name]</span><br><span class="line">                self.shadow[name] = new_average.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply_shadow</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.shadow</span><br><span class="line">                self.backup[name] = param.data</span><br><span class="line">                param.data = self.shadow[name]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">restore</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.backup</span><br><span class="line">                param.data = self.backup[name]</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">ema = EMA(model, <span class="number">0.999</span>)</span><br><span class="line">ema.register()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程中，更新完参数后，同步update shadow weights</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    optimizer.step()</span><br><span class="line">    ema.update()</span><br><span class="line"></span><br><span class="line"><span class="comment"># eval前，apply shadow weights；eval之后，恢复原来模型的参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>():</span><br><span class="line">    ema.apply_shadow()</span><br><span class="line">    <span class="comment"># evaluate</span></span><br><span class="line">    ema.restore()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2023/02/24/lora/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/24/lora/" class="post-title-link" itemprop="url">Lora</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-02-24 23:00:00 / Modified: 23:53:00" itemprop="dateCreated datePublished" datetime="2023-02-24T23:00:00+08:00">2023-02-24</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h1><p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a><br>代码链接：<a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a></p>
<p>LoRA是一种finetune扩散模型的训练技术。通过对标准的checkpoint模型微小的修改，可以比checkpoint模型小10到100倍。LoRA的原理比较简单，原始全量的finetune其实就是在原始模型参数基础上加入增量$W=W_0+\Delta W$，那么我们可以通过冻结原始参数 $W_0$，并且把增量部分通过低秩分解方式进一步降低参数量级$\Delta W = A*B^T$, 原始参数的维度是 $d*d$, 则低秩分解后的参数量级是 $2*r*d$, 这里 $r&lt;&lt;d$， 因此可以起到大幅降低微调参数量级的效果。</p>
<p>和textula inversion一样，不能直接使用LoRA模型，需要和checkpoint文件一起使用。</p>
<h2 id="How-does-LoRA-work-in-Stable-diffusion？"><a href="#How-does-LoRA-work-in-Stable-diffusion？" class="headerlink" title="How does LoRA work in Stable diffusion？"></a>How does LoRA work in Stable diffusion？</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/45f7e2ce00c48b8c96e938c9fe8ed3d4.png" alt="image"></p>
<p>LoRA通过在checkpoint上做小的修改替换风格，具体而言修改的地方是UNet中的cross-attention层。该层是图像和文本prompt交界的层。LORA的作者们发现微调该部分足以实现良好的性能。</p>
<p>cross attention层的权重是一个矩阵，LoRA fine tune这些权重来微调模型。那么LoRA是怎么做到模型文件如此小？LoRA的做法是将一个权重矩阵分解为两个矩阵存储，能起到的作用可以用下图表示，参数量由(1000<em> 2000)减少到(1000</em> 2+2000*2), 大概300多倍！</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/10bbf49b2e9042779941c600a692d74dtplv-k3u1fbpfcp-zoom-in-crop-mark1512000.webp" alt="img"></p>
<p><strong>核心代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 初始化低秩矩阵A和B</span></span><br><span class="line">self.lora_A.update(nn.ModuleDict(&#123;adapter_name: nn.Linear(self.in_features, r, bias=<span class="literal">False</span>)&#125;))</span><br><span class="line">self.lora_B.update(nn.ModuleDict(&#123;adapter_name: nn.Linear(r, self.out_features, bias=<span class="literal">False</span>)&#125;))</span><br><span class="line">self.scaling[adapter_name] = lora_alpha / r</span><br><span class="line"></span><br><span class="line"><span class="comment">## 向前计算</span></span><br><span class="line">result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)</span><br><span class="line">result += (</span><br><span class="line">    self.lora_B[self.active_adapter](</span><br><span class="line">        self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))</span><br><span class="line">    )</span><br><span class="line">    * self.scaling[self.active_adapter]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>alpha参数：alpha其实是个缩放参数，本质和learning rate相同。</p>
<h2 id="cross-attention"><a href="#cross-attention" class="headerlink" title="cross attention"></a>cross attention</h2><p>cross-attention是扩散模型中关键的技术之一，在LoRA中通过微调该模块，即可微调生成图片的样式，而在Hypernetwork中使用两个带有dropout和激活函数的全链接层，分别修改cross attention中的key和value，也可以定制想要的生成风格。可见cross attention的重要性。</p>
<p>在讲cross-attention之前，先看看经典的transformer中attention的含义，attnetion实际上用了三个QKV矩阵，来计算不同token之间的彼此的依赖关系，Q和K可以用来计算当前token和其他token的相似度，这个相似度作为权值对V进行加权求和，可以作为下一层的token。更通俗点说，Q和k的作用是用来在token之间搬运信息，而value本身就是从当前token当中提取出来的信息. 比较常见的是self-attention，该注意力是一个sequence内部不同token间产生注意力，而cross-attention的区别是在不同的sequence之间产生注意力。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2023/01/21/dali%E9%A2%84%E5%A4%84%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/01/21/dali%E9%A2%84%E5%A4%84%E7%90%86/" class="post-title-link" itemprop="url">DALI加速图像数据预处理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-01-21 23:03:00" itemprop="dateCreated datePublished" datetime="2023-01-21T23:03:00+08:00">2023-01-21</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="DALI预处理加速"><a href="#DALI预处理加速" class="headerlink" title="DALI预处理加速"></a>DALI预处理加速</h1><p>NVIDIA DALI 文档：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/general/data_loading/external_input.html">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/general/data_loading/external_input.html</a></p>
<p>安装：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html#pip-official-releases">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html#pip-official-releases</a></p>
<h2 id="1、DALI-pipeline"><a href="#1、DALI-pipeline" class="headerlink" title="1、DALI pipeline"></a>1、DALI pipeline</h2><p>DALI可以选择纯CPU加载和预处理或者CPU&amp;GPU混合加载，GPU加载。</p>
<p>在DALI中，任何数据处理任务都有一个称为 Pipeline 的对象， Pipeline 对象是类的实例<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline"><code>nvidia.dali.Pipeline</code></a>或派生类。</p>
<p>可以通过以下方式定义DALI Pipeline</p>
<ol>
<li>通过实现内部使用 DALI 运算符的函数并使用<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.pipeline_def"><code>pipeline_def()</code></a>装饰器对其进行装饰。</li>
<li>通过<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline"><code>Pipeline</code></a>直接实例化对象、构建图形并使用<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline.set_outputs"><code>Pipeline.set_outputs()</code></a>.</li>
<li>通过从<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline"><code>Pipeline</code></a>类继承并覆盖<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline.define_graph"><code>Pipeline.define_graph()</code></a>（这是定义 DALI Pipelines 的传统方式）</li>
</ol>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%9B%BE20230907195707.png" alt="截图20230907195707"></p>
<h2 id="2、图像分类的pipeline示例"><a href="#2、图像分类的pipeline示例" class="headerlink" title="2、图像分类的pipeline示例"></a>2、图像分类的pipeline示例</h2><p>所有操作均在GPU上，note：<strong>使用gpu进行预处理，会占用显存，模型越大占用越多，但是GPU利用率会一直保持在100%</strong>。模型较大不推荐使用GPU加载。</p>
<p><strong>使用纯CPU操作，数据处理的速度也比 torchvision快</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TrainPipeline</span>(<span class="title class_ inherited__">Pipeline</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_threads, device_id, data_root, img_size, n_holes, length, custom_cutout=<span class="literal">False</span></span>):</span><br><span class="line">         </span><br><span class="line">        <span class="built_in">super</span>(TrainPipeline, self).__init__(batch_size, num_threads, device_id, prefetch_queue_depth=<span class="number">4</span>)</span><br><span class="line">        mode = <span class="string">&#x27;gpu&#x27;</span></span><br><span class="line">        self.decode = ops.decoders.Image(device=<span class="string">&#x27;mixed&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">        self.img_size = img_size</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># readers.File类似torchvision.datasets.ImageFolder，dali还有其他高阶API，可自行研究使用</span></span><br><span class="line">        self.<span class="built_in">input</span> = ops.readers.File(file_root=data_root, random_shuffle=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># Resize</span></span><br><span class="line">        self.resize = ops.Resize(device=mode, resize_x=<span class="built_in">int</span>(img_size*<span class="number">1.2</span>), resize_y=<span class="built_in">int</span>(img_size*<span class="number">1.2</span>))</span><br><span class="line">        <span class="comment"># Randomcrop，类似于torchvision.transforms.RandomCrop</span></span><br><span class="line">        self.randomcrop = ops.RandomResizedCrop(device=mode, size=img_size, random_area=[<span class="number">0.3</span>, <span class="number">1.0</span>])</span><br><span class="line">        <span class="comment"># CropMirrorNormalize可以实现normalize和随机水平翻转，类似于torchvision.transforms.Normalize &amp; RandomHorizontalFlip</span></span><br><span class="line">        self.normalize = ops.CropMirrorNormalize(device=mode, mean=[<span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>],</span><br><span class="line">                                                 std=[<span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>])</span><br><span class="line">        <span class="comment"># 获取随机数</span></span><br><span class="line">        self.rng1 = ops.random.Uniform()</span><br><span class="line">        self.rng2 = ops.random.CoinFlip()</span><br><span class="line">        <span class="comment"># 实例化改变图片色彩的类，类似于torchvision.transforms.ColorJitter</span></span><br><span class="line">        self.colortwist = ops.ColorTwist(device=mode)</span><br><span class="line">        <span class="comment"># 实例化旋转图像的类，类似于torchvision.transforms.RandomRotation</span></span><br><span class="line">        self.rotate = ops.Rotate(device=mode, fill_value=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># gridmask，类似于cutout这种随机遮挡块操作</span></span><br><span class="line">        self.gridmask = ops.GridMask(device=mode)</span><br><span class="line">       </span><br></pre></td></tr></table></figure>
<p>如果需要自定义数据处理的函数，可参考一下方式。以cutout为例：cutout使用的是cpu处理了，如果是gpu处理的话，<strong>需要将numpy改成cupy，DALI原生支持的操作和数据增强挺丰富的。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CUTOUT</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_holes, length</span>):</span><br><span class="line">        self.n_holes = n_holes</span><br><span class="line">        self.length = length</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, imgs</span>):</span><br><span class="line">        c, h, w = imgs.shape</span><br><span class="line">        mask = np.ones((h, w), np.float32)</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(self.n_holes):</span><br><span class="line">            y = np.random.randint(h)</span><br><span class="line">            x = np.random.randint(w)</span><br><span class="line">            y1 = np.clip(y - self.length // <span class="number">2</span>, <span class="number">0</span>, h)</span><br><span class="line">            y2 = np.clip(y + self.length // <span class="number">2</span>, <span class="number">0</span>, h)</span><br><span class="line">            x1 = np.clip(x - self.length // <span class="number">2</span>, <span class="number">0</span>, w)</span><br><span class="line">            x2 = np.clip(x + self.length // <span class="number">2</span>, <span class="number">0</span>, w)</span><br><span class="line">            mask[y1: y2, x1: x2] = <span class="number">0.</span></span><br><span class="line">        mask = np.expand_dims(mask, <span class="number">0</span>).repeat(c, axis=<span class="number">0</span>)</span><br><span class="line">        imgs = imgs * mask</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> imgs</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 然后在上面的 TrainPipeline上加上下面这行,model 是 “cpu”</span></span><br><span class="line">    self.mask = ops.PythonFunction(device=<span class="string">&quot;cpu&quot;</span>, function=CUTOUT(n_holes, length), num_outputs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>图像分类数据加载的时候的调用方式：其他的Iterator可以参考 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/plugins/pytorch_tutorials.html">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/plugins/pytorch_tutorials.html</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nvidia.dali.plugin.pytorch <span class="keyword">import</span> DALIClassificationIterator</span><br><span class="line"><span class="keyword">from</span> nvidia.dali.plugin.base_iterator <span class="keyword">import</span> LastBatchPolicy</span><br><span class="line"> </span><br><span class="line">pipe_train = TrainPipeline(batch_size, num_threads, device_id, data_root, img_size, n_holes, length,custom_cutout=custom_cutout)</span><br><span class="line">pipe_train.build()</span><br><span class="line">         </span><br><span class="line"><span class="comment"># DALIClassificationIterator: 返回pytorch tensor 形式是 (data and label) , 即DataLoader</span></span><br><span class="line">train_loader = DALIClassificationIterator(pipe_train, size=pipe_train.epoch_size(<span class="string">&#x27;Reader&#x27;</span>),last_batch_policy=LastBatchPolicy.PARTIAL, auto_reset=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiechu520.github.io/2023/01/09/MAE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/01/09/MAE/" class="post-title-link" itemprop="url">MAE——Masked Autoencoders</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-01-09 21:00:00" itemprop="dateCreated datePublished" datetime="2023-01-09T21:00:00+08:00">2023-01-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-01-09 23:08:00" itemprop="dateModified" datetime="2022-01-09T23:08:00+08:00">2022-01-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="MAE：Masked-Autoencoders"><a href="#MAE：Masked-Autoencoders" class="headerlink" title="MAE：Masked Autoencoders"></a>MAE：Masked Autoencoders</h1><p>code: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/mae">https://github.com/facebookresearch/mae</a></p>
<p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.06377">https://arxiv.org/abs/2111.06377</a></p>
<h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240111174241212.png" alt="image-20240111174241212"></p>
<p><strong>以一定比例随机mask掉图片中的一些图像块然后重建这些部分像素值。</strong></p>
<p>主要特点有两个：</p>
<ol>
<li><strong>非对称</strong>的编、解码器设计</li>
<li>使用<strong>较高(如 75%)的掩码率</strong>(mask 比例)</li>
</ol>
<p>第1点所述的“非对称”主要体现在 <strong>输入形式</strong> 与 <strong>网络结构</strong> 上：编码器(Encoder)仅对可见(un-masked)的图像块进行编码，而解码器(Decoder)的输入则是所有的图像块；同时，Decoder 可以是比较轻量的(比如 Encoder 通常是多层堆叠的 Transformer，而 Decoder 仅需较少层甚至1层就 ok)。这也表明 Encoder 与 Decoder 之间是<strong>解耦</strong>的。</p>
<p>第2点是该工作的一个重要发现：不同于 NLP，<strong>在 CV 中可能要配合较高的 mask 比例才能作为“有效”的自监督代理任务。“有效”指的是任务本身足够困难，这样模型才能学到有效的潜在特征表示。</strong></p>
<p>由于 Encoder 仅处理 un-masked 的 patch(占所有输入的少数)，因此，尽管其本身网络结构比较重载，但依然能够高效训练，特别是对于大模型，能够加速3倍以上，同时配合较高的掩码率，还能够涨点。</p>
<p>作者从一个问题出来解释选择这样mask策略以及模型设计的理由，<strong>“为什么 masked autoencoding 在CV中应用比较少相较于NLP？”</strong> 作者提炼了以下三点：</p>
<ul>
<li>架构差异：之前CNN的架构不适合使用</li>
<li>信息密度不同: 语言的信息密度比较高，将句子中的少量词语抹去再让模型预测被抹去的这些词是比较困难的任务；但对于图像则相反，它在空间中是冗余的，对于图片中的某个部分，模型很容易由其相邻的图像块推断出来，因此在CV中，mask的比例应该更高，才能使任务本身具有足够的挑战性，从而使模型学到良好的潜在特征表示。</li>
<li>解码的目标不一致：CV 和 NLP 在解码器的设计上应该有不一样的考虑：NLP 解码输出的是对应被 mask 掉的词语，本身包含了丰富的语义信息；而 CV 要重建的是被 mask 掉的图像块(像素值)，是低语义的。因此，NLP 的解码器可以很简单，比如 BERT，严格来说它并没有解码器，最后用 MLP 也可以搞定。因为来自编码器的特征也是高度语义的，与需要解码的目标之间的 gap 较小；而 CV 的解码器设计则需要“谨慎”考虑了，因为它要将<strong>来自编码器的高级语义特征解码至低级语义层级</strong>。</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240111174251713.png" alt="image-20240111174251713"></p>
<h2 id="2、具体方法"><a href="#2、具体方法" class="headerlink" title="2、具体方法"></a>2、具体方法</h2><p>MAE的特点主要是：<font color=red><strong>高掩码率</strong>的<strong>随机</strong> mask 策略、<strong>非对称</strong>的编、解码器设计 以及 重建的目标是<strong>像素</strong>值</font></p>
<h3 id="2-1-masked-策略"><a href="#2-1-masked-策略" class="headerlink" title="2.1 masked 策略"></a>2.1 masked 策略</h3><p>沿袭 ViT 的做法，将图像分成一块块(ViT 中是 16x16 大小)不重叠的 patch，然后使用服从<strong>均匀分布(uniform distribution)</strong>的采样策略对这些 patches 随机采样一部分，同时 mask 掉余下的另一部分。被 mask 掉的 patches 占所有 patches 的大部分(实验效果发现最好的比例是 75%)，它们不会输入到 Encoder。</p>
<p>作者实验发现：无论是finetune，还是 fine-tune 还是 linear-probe(微调方法，将最后一层替换成线性层，微调时冻结其他层，只训练这个线性层)，75%都是一个比较好的比例。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240115201547788.png" alt="image-20240115201547788"></p>
<h3 id="2-2-Encoder"><a href="#2-2-Encoder" class="headerlink" title="2.2 Encoder"></a>2.2 Encoder</h3><p><strong>Encoder只处理 un-masked 的patches</strong>。Encoder可以是Vit或者其他backbone。图像划分成patch采用Vit的做法。</p>
<p>先将图像从 (B,C,H,W) reshape 成 (B,N,PxPxC)，其中 N 和 P 分别为 patch 数量 和 patch 大小( $H/P \times W/P$)，也就是<strong>将3通道的图像转换成 N 个 维度大小为 PxPxC 的向量</strong>；然后，<strong>通过线性映射(linear projection，可以是全连接层)将其嵌入(embed)到指定的维度空间大小</strong>，记为 ‘dim’(从 PxPxC project 到 dim)，转换成为 <strong>token</strong>(B,N,dim)；最后再<strong>加上位置嵌入(position embedding)</strong>，从而为各个 patch 添加位置信息。<strong>位置嵌入是所有图像共享的、可学习的</strong>，shape 与 每张图的 token 相对应，即：(N,dim)。</p>
<p>由于 un-masked patches 占所有 patches 的少数，计算消耗和空间需求都减少了，因此可以训练很大的 Encoder。</p>
<h3 id="2-3-Decoder"><a href="#2-3-Decoder" class="headerlink" title="2.3 Decoder"></a>2.3 Decoder</h3><p>Decoder 不仅需要处理经过 Encoder 编码的 un-masked 的 tokens，还需要处理 masked tokens。但请注意，<strong>masked token 并非由之前 mask 掉的 patch 经过 embedding 转换而来，而是可学习的、所有 masked patches 都共享的1个向量，对，仅仅就是1个！</strong></p>
<p>通过 position embedding 来区分各个 masked patch 对应的 token。</p>
<h3 id="2-4-loss"><a href="#2-4-loss" class="headerlink" title="2.4 loss"></a>2.4 loss</h3><p>MAE 预训练任务的目标是重建像素值，并且仅仅是 masked patches 的像素值，也就是<strong>仅对 mask 掉的部分计算 loss</strong>，而 loss 就是很大众的 MSE。为何仅计算 mask 部分的 loss？实验结果发现这样做模型的性能会更好，而如果对所有 patches 都计算 loss 的话会掉点。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h2><p><strong>mask采样策略</strong></p>
<p>作者通过实验比较，最终选择了服从均匀分布的随机采样，以下是详细实验结果：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116104651814.png" alt="image-20240116104651814"></p>
<p><strong>Decoder设计</strong></p>
<p>作者还研究了Decoder的设计，下图展示了 Decoder 的深度和宽度对于 ft 和 linear probe 的影响。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116104906404.png" alt="image-20240116104906404"></p>
<p>Decoder 的深度和宽度对于 linear probe 有较为明显的影响，但对于 fine-tune 的影响却不那么突出。究其本质，原因是是<strong>预训练任务(图像重建)与下游任务(图像识别)之间存在着 gap</strong>。</p>
<p><strong>encoder为什么不用 masked tokens</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116105137183.png" alt="image-20240116105137183"></p>
<p>原因是下游任务并不存在这些 masked tokens，上下游任务之间存在gap。如果Encoder 也对 masked tokens 进行编码，会进一步将这种 gap 的影响“扩散”至下游任务中。</p>
<p><strong>各种重建目标的比较</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116105418257.png" alt="image-20240116105418257"></p>
<p><strong>数据增强</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240116105510461.png" alt="image-20240116105510461"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Jie Chu</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
