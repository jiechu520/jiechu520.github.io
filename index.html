<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="google-site-verification" content="BTo06tdvlac_Dho4-PFTLmDqjKXr1KtOzavpD8XDA5k" />
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="日常笔记">
<meta property="og:type" content="website">
<meta property="og:title" content="CJ blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="CJ blog">
<meta property="og:description" content="日常笔记">
<meta property="og:locale">
<meta property="article:author" content="Jie Chu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CJ blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">CJ blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">不知名算法工程师</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jie Chu</p>
  <div class="site-description" itemprop="description">日常笔记</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/20/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/20/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-20 00:09:14" itemprop="dateCreated datePublished" datetime="2023-12-20T00:09:14+08:00">2023-12-20</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/21/EVA02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/21/EVA02/" class="post-title-link" itemprop="url">EVA02解析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-04-21 21:03:00" itemprop="dateCreated datePublished" datetime="2023-04-21T21:03:00+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-22 23:03:00" itemprop="dateModified" datetime="2023-04-22T23:03:00+08:00">2023-04-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="EVA-02-A-Visual-Representation-for-Neon-Genesis"><a href="#EVA-02-A-Visual-Representation-for-Neon-Genesis" class="headerlink" title="EVA-02: A Visual Representation for Neon Genesis"></a>EVA-02: A Visual Representation for Neon Genesis</h1><p>EVA02的目标是作为下一代的基于Transformer的视觉表示模型。</p>
<p>文章主要包含两个部分：1、对普通Vit的架构改进，2、MIM的预训练策略</p>
<p><strong>总结</strong></p>
<p>EVA02主要有两个改进，一是，通过实验的方法来观察采用哪些NLP方向关于Vit的改进；二是，增加视觉特征编码的容量以及增加训练的轮数和图像的size；</p>
<h2 id="1-Architecture"><a href="#1-Architecture" class="headerlink" title="1 Architecture"></a>1 Architecture</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230629182751431.png" alt="image-20230629182751431"></p>
<p>ViT主要由 MHSA（用于全局空间信息聚合）和 pointwise的FFNs（特征变换）交错组成。但是NLP方面很多针对ViT的修改没有在视觉上应用。作者做了一个实验来探索不同的修改带来的影响：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230703191529244.png" alt="image-20230703191529244"></p>
<p><strong>Gelu</strong>: $GELU(x)=x * \phi(x),x \sim N(0,1)$</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-0d8b979444f64ab49d4bc0f4199a15c2_r.jpg" alt="img"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%9B%BE20230914183157.png" alt="截图20230914183157"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-e31b1e5b4333a90fbcca03c9a863a0c5_r.jpg" alt="img"></p>
<h2 id="2-pre-trainning-strategy"><a href="#2-pre-trainning-strategy" class="headerlink" title="2 pre-trainning strategy"></a>2 pre-trainning strategy</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230703195140612.png" alt="image-20230703195140612"></p>
<p><strong>MIM teacher model变大，训练的epoch也需要变多；</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230703195449659.png" alt="image-20230703195449659"></p>
<p><strong>分辨率的增加以及在imgnet数据上的有监督ft也会增加性能</strong></p>
<p>预训练目标类似于 EVA [44]，即仅以可见图像块为条件回归屏蔽图像文本对齐的视觉特征。我们使用 [MASK] 标记破坏输入补丁，并按照 [5, 44] 使用掩码率为 40% 的分块掩码。</p>
<p>MIM 预训练的目标表示来自可公开访问的 EVA-CLIP [44] 视觉塔，具有 10 亿个参数。 EV A-02 的输出特征首先被归一化 [4]，然后通过线性层投影到与 EVA-CLIP 的视觉特征相同的维度。我们使用负余弦相似度作为损失函数。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/21/EMA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/21/EMA/" class="post-title-link" itemprop="url">EMA原理和pytorch实现</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-21 21:03:00" itemprop="dateCreated datePublished" datetime="2023-03-21T21:03:00+08:00">2023-03-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-22 23:03:00" itemprop="dateModified" datetime="2023-03-22T23:03:00+08:00">2023-03-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="EMA的原理和pytorch实现"><a href="#EMA的原理和pytorch实现" class="headerlink" title="EMA的原理和pytorch实现"></a>EMA的原理和pytorch实现</h1><h2 id="EMA-定义"><a href="#EMA-定义" class="headerlink" title="EMA 定义"></a>EMA 定义</h2><p>指数移动平均也叫权重移动平均，是一种给予近期数据更高权重的平均方法。</p>
<h2 id="在深度学习的优化中的EMA"><a href="#在深度学习的优化中的EMA" class="headerlink" title="在深度学习的优化中的EMA"></a>在深度学习的优化中的EMA</h2><p>在深度学习的优化过程中，$\theta_t$ 是 t 时刻的模型权重 weight， $\upsilon_t$ 是 t 时刻的影子权重。在梯度下降的过程中，会一直维护着这个影子权重，但是这个影子权重不会参与训练。基本的假设是<strong>模型权重在最后 n 步内，会在实际的最优点处抖动，所以我们去最后 n 步的平均，能使模型更加的鲁棒。</strong></p>
<h3 id="EMA为何有效"><a href="#EMA为何有效" class="headerlink" title="EMA为何有效"></a>EMA为何有效</h3><p>因为在训练的时候，会使用验证集来衡量模型精度，但是验证集和测试集并不完全一致，在训练后期阶段，模型可能已经在测试集最佳精度附近波动，所以使用ema的结果会比使用单一结果更可靠。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230307191607861.png" alt="image-20230307191607861"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230307191620727.png" alt="image-20230307191620727"></p>
<h3 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EMA</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, decay</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.decay = decay</span><br><span class="line">        self.shadow = &#123;&#125;</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">register</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                self.shadow[name] = param.data.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.shadow</span><br><span class="line">                new_average = (<span class="number">1.0</span> - self.decay) * param.data + self.decay * self.shadow[name]</span><br><span class="line">                self.shadow[name] = new_average.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply_shadow</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.shadow</span><br><span class="line">                self.backup[name] = param.data</span><br><span class="line">                param.data = self.shadow[name]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">restore</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.backup</span><br><span class="line">                param.data = self.backup[name]</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">ema = EMA(model, <span class="number">0.999</span>)</span><br><span class="line">ema.register()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程中，更新完参数后，同步update shadow weights</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    optimizer.step()</span><br><span class="line">    ema.update()</span><br><span class="line"></span><br><span class="line"><span class="comment"># eval前，apply shadow weights；eval之后，恢复原来模型的参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>():</span><br><span class="line">    ema.apply_shadow()</span><br><span class="line">    <span class="comment"># evaluate</span></span><br><span class="line">    ema.restore()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/02/24/lora/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/24/lora/" class="post-title-link" itemprop="url">Lora</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-02-24 23:00:00 / Modified: 23:53:00" itemprop="dateCreated datePublished" datetime="2023-02-24T23:00:00+08:00">2023-02-24</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h1><p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a><br>代码链接：<a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a></p>
<p>LoRA是一种finetune扩散模型的训练技术。通过对标准的checkpoint模型微小的修改，可以比checkpoint模型小10到100倍。LoRA的原理比较简单，原始全量的finetune其实就是在原始模型参数基础上加入增量$W=W_0+\Delta W$，那么我们可以通过冻结原始参数 $W_0$，并且把增量部分通过低秩分解方式进一步降低参数量级$\Delta W = A*B^T$, 原始参数的维度是 $d*d$, 则低秩分解后的参数量级是 $2*r*d$, 这里 $r&lt;&lt;d$， 因此可以起到大幅降低微调参数量级的效果。</p>
<p>和textula inversion一样，不能直接使用LoRA模型，需要和checkpoint文件一起使用。</p>
<h2 id="How-does-LoRA-work-in-Stable-diffusion？"><a href="#How-does-LoRA-work-in-Stable-diffusion？" class="headerlink" title="How does LoRA work in Stable diffusion？"></a>How does LoRA work in Stable diffusion？</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/45f7e2ce00c48b8c96e938c9fe8ed3d4.png" alt="image"></p>
<p>LoRA通过在checkpoint上做小的修改替换风格，具体而言修改的地方是UNet中的cross-attention层。该层是图像和文本prompt交界的层。LORA的作者们发现微调该部分足以实现良好的性能。</p>
<p>cross attention层的权重是一个矩阵，LoRA fine tune这些权重来微调模型。那么LoRA是怎么做到模型文件如此小？LoRA的做法是将一个权重矩阵分解为两个矩阵存储，能起到的作用可以用下图表示，参数量由(1000<em> 2000)减少到(1000</em> 2+2000*2), 大概300多倍！</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/10bbf49b2e9042779941c600a692d74dtplv-k3u1fbpfcp-zoom-in-crop-mark1512000.webp" alt="img"></p>
<p><strong>核心代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 初始化低秩矩阵A和B</span></span><br><span class="line">self.lora_A.update(nn.ModuleDict(&#123;adapter_name: nn.Linear(self.in_features, r, bias=<span class="literal">False</span>)&#125;))</span><br><span class="line">self.lora_B.update(nn.ModuleDict(&#123;adapter_name: nn.Linear(r, self.out_features, bias=<span class="literal">False</span>)&#125;))</span><br><span class="line">self.scaling[adapter_name] = lora_alpha / r</span><br><span class="line"></span><br><span class="line"><span class="comment">## 向前计算</span></span><br><span class="line">result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)</span><br><span class="line">result += (</span><br><span class="line">    self.lora_B[self.active_adapter](</span><br><span class="line">        self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))</span><br><span class="line">    )</span><br><span class="line">    * self.scaling[self.active_adapter]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>alpha参数：alpha其实是个缩放参数，本质和learning rate相同。</p>
<h2 id="cross-attention"><a href="#cross-attention" class="headerlink" title="cross attention"></a>cross attention</h2><p>cross-attention是扩散模型中关键的技术之一，在LoRA中通过微调该模块，即可微调生成图片的样式，而在Hypernetwork中使用两个带有dropout和激活函数的全链接层，分别修改cross attention中的key和value，也可以定制想要的生成风格。可见cross attention的重要性。</p>
<p>在讲cross-attention之前，先看看经典的transformer中attention的含义，attnetion实际上用了三个QKV矩阵，来计算不同token之间的彼此的依赖关系，Q和K可以用来计算当前token和其他token的相似度，这个相似度作为权值对V进行加权求和，可以作为下一层的token。更通俗点说，Q和k的作用是用来在token之间搬运信息，而value本身就是从当前token当中提取出来的信息. 比较常见的是self-attention，该注意力是一个sequence内部不同token间产生注意力，而cross-attention的区别是在不同的sequence之间产生注意力。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/21/dali%E9%A2%84%E5%A4%84%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/01/21/dali%E9%A2%84%E5%A4%84%E7%90%86/" class="post-title-link" itemprop="url">DALI加速图像数据预处理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-01-21 23:03:00" itemprop="dateCreated datePublished" datetime="2023-01-21T23:03:00+08:00">2023-01-21</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="DALI预处理加速"><a href="#DALI预处理加速" class="headerlink" title="DALI预处理加速"></a>DALI预处理加速</h1><p>NVIDIA DALI 文档：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/general/data_loading/external_input.html">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/general/data_loading/external_input.html</a></p>
<p>安装：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html#pip-official-releases">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html#pip-official-releases</a></p>
<h2 id="1、DALI-pipeline"><a href="#1、DALI-pipeline" class="headerlink" title="1、DALI pipeline"></a>1、DALI pipeline</h2><p>DALI可以选择纯CPU加载和预处理或者CPU&amp;GPU混合加载，GPU加载。</p>
<p>在DALI中，任何数据处理任务都有一个称为 Pipeline 的对象， Pipeline 对象是类的实例<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline"><code>nvidia.dali.Pipeline</code></a>或派生类。</p>
<p>可以通过以下方式定义DALI Pipeline</p>
<ol>
<li>通过实现内部使用 DALI 运算符的函数并使用<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.pipeline_def"><code>pipeline_def()</code></a>装饰器对其进行装饰。</li>
<li>通过<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline"><code>Pipeline</code></a>直接实例化对象、构建图形并使用<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline.set_outputs"><code>Pipeline.set_outputs()</code></a>.</li>
<li>通过从<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline"><code>Pipeline</code></a>类继承并覆盖<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline.define_graph"><code>Pipeline.define_graph()</code></a>（这是定义 DALI Pipelines 的传统方式）</li>
</ol>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%9B%BE20230907195707.png" alt="截图20230907195707"></p>
<h2 id="2、图像分类的pipeline示例"><a href="#2、图像分类的pipeline示例" class="headerlink" title="2、图像分类的pipeline示例"></a>2、图像分类的pipeline示例</h2><p>所有操作均在GPU上，note：<strong>使用gpu进行预处理，会占用显存，模型越大占用越多，但是GPU利用率会一直保持在100%</strong>。模型较大不推荐使用GPU加载。</p>
<p><strong>使用纯CPU操作，数据处理的速度也比 torchvision快</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TrainPipeline</span>(<span class="title class_ inherited__">Pipeline</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_threads, device_id, data_root, img_size, n_holes, length, custom_cutout=<span class="literal">False</span></span>):</span><br><span class="line">         </span><br><span class="line">        <span class="built_in">super</span>(TrainPipeline, self).__init__(batch_size, num_threads, device_id, prefetch_queue_depth=<span class="number">4</span>)</span><br><span class="line">        mode = <span class="string">&#x27;gpu&#x27;</span></span><br><span class="line">        self.decode = ops.decoders.Image(device=<span class="string">&#x27;mixed&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">        self.img_size = img_size</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># readers.File类似torchvision.datasets.ImageFolder，dali还有其他高阶API，可自行研究使用</span></span><br><span class="line">        self.<span class="built_in">input</span> = ops.readers.File(file_root=data_root, random_shuffle=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># Resize</span></span><br><span class="line">        self.resize = ops.Resize(device=mode, resize_x=<span class="built_in">int</span>(img_size*<span class="number">1.2</span>), resize_y=<span class="built_in">int</span>(img_size*<span class="number">1.2</span>))</span><br><span class="line">        <span class="comment"># Randomcrop，类似于torchvision.transforms.RandomCrop</span></span><br><span class="line">        self.randomcrop = ops.RandomResizedCrop(device=mode, size=img_size, random_area=[<span class="number">0.3</span>, <span class="number">1.0</span>])</span><br><span class="line">        <span class="comment"># CropMirrorNormalize可以实现normalize和随机水平翻转，类似于torchvision.transforms.Normalize &amp; RandomHorizontalFlip</span></span><br><span class="line">        self.normalize = ops.CropMirrorNormalize(device=mode, mean=[<span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>],</span><br><span class="line">                                                 std=[<span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>])</span><br><span class="line">        <span class="comment"># 获取随机数</span></span><br><span class="line">        self.rng1 = ops.random.Uniform()</span><br><span class="line">        self.rng2 = ops.random.CoinFlip()</span><br><span class="line">        <span class="comment"># 实例化改变图片色彩的类，类似于torchvision.transforms.ColorJitter</span></span><br><span class="line">        self.colortwist = ops.ColorTwist(device=mode)</span><br><span class="line">        <span class="comment"># 实例化旋转图像的类，类似于torchvision.transforms.RandomRotation</span></span><br><span class="line">        self.rotate = ops.Rotate(device=mode, fill_value=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># gridmask，类似于cutout这种随机遮挡块操作</span></span><br><span class="line">        self.gridmask = ops.GridMask(device=mode)</span><br><span class="line">       </span><br></pre></td></tr></table></figure>
<p>如果需要自定义数据处理的函数，可参考一下方式。以cutout为例：cutout使用的是cpu处理了，如果是gpu处理的话，<strong>需要将numpy改成cupy，DALI原生支持的操作和数据增强挺丰富的。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CUTOUT</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_holes, length</span>):</span><br><span class="line">        self.n_holes = n_holes</span><br><span class="line">        self.length = length</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, imgs</span>):</span><br><span class="line">        c, h, w = imgs.shape</span><br><span class="line">        mask = np.ones((h, w), np.float32)</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(self.n_holes):</span><br><span class="line">            y = np.random.randint(h)</span><br><span class="line">            x = np.random.randint(w)</span><br><span class="line">            y1 = np.clip(y - self.length // <span class="number">2</span>, <span class="number">0</span>, h)</span><br><span class="line">            y2 = np.clip(y + self.length // <span class="number">2</span>, <span class="number">0</span>, h)</span><br><span class="line">            x1 = np.clip(x - self.length // <span class="number">2</span>, <span class="number">0</span>, w)</span><br><span class="line">            x2 = np.clip(x + self.length // <span class="number">2</span>, <span class="number">0</span>, w)</span><br><span class="line">            mask[y1: y2, x1: x2] = <span class="number">0.</span></span><br><span class="line">        mask = np.expand_dims(mask, <span class="number">0</span>).repeat(c, axis=<span class="number">0</span>)</span><br><span class="line">        imgs = imgs * mask</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> imgs</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 然后在上面的 TrainPipeline上加上下面这行,model 是 “cpu”</span></span><br><span class="line">    self.mask = ops.PythonFunction(device=<span class="string">&quot;cpu&quot;</span>, function=CUTOUT(n_holes, length), num_outputs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>图像分类数据加载的时候的调用方式：其他的Iterator可以参考 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/plugins/pytorch_tutorials.html">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/plugins/pytorch_tutorials.html</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nvidia.dali.plugin.pytorch <span class="keyword">import</span> DALIClassificationIterator</span><br><span class="line"><span class="keyword">from</span> nvidia.dali.plugin.base_iterator <span class="keyword">import</span> LastBatchPolicy</span><br><span class="line"> </span><br><span class="line">pipe_train = TrainPipeline(batch_size, num_threads, device_id, data_root, img_size, n_holes, length,custom_cutout=custom_cutout)</span><br><span class="line">pipe_train.build()</span><br><span class="line">         </span><br><span class="line"><span class="comment"># DALIClassificationIterator: 返回pytorch tensor 形式是 (data and label) , 即DataLoader</span></span><br><span class="line">train_loader = DALIClassificationIterator(pipe_train, size=pipe_train.epoch_size(<span class="string">&#x27;Reader&#x27;</span>),last_batch_policy=LastBatchPolicy.PARTIAL, auto_reset=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/11/resnet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/11/resnet/" class="post-title-link" itemprop="url">resnet解析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-04-11 21:03:00" itemprop="dateCreated datePublished" datetime="2022-04-11T21:03:00+08:00">2022-04-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-11 23:03:00" itemprop="dateModified" datetime="2023-04-11T23:03:00+08:00">2023-04-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="resnet"><a href="#resnet" class="headerlink" title="resnet"></a>resnet</h1><h3 id="1-1-batch-normalization原理"><a href="#1-1-batch-normalization原理" class="headerlink" title="1.1 batch normalization原理"></a>1.1 batch normalization原理</h3><p>提出背景：在训练层数较深的神经网络时，参数的变化导致每一层的输入分布会发生变化，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难——internal Covariate Shift</p>
<p>问题：上层网络需要不停调整来适应输入分布的变化，导致网络学习速度的降低；网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度。</p>
<p>减缓internal covariate shift的方法：</p>
<ul>
<li>白化：对输入数据分布进行变换，具有相同的均值和方差；</li>
<li><strong>batch normalization</strong>：由于白化过程计算成本太高，且改变了网络每一层的分布。</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701153501142.png" alt="image-20220701153501142"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701153552148.png" alt="image-20220701153552148"></p>
<h3 id="1-2-resnet-结构"><a href="#1-2-resnet-结构" class="headerlink" title="1.2  resnet 结构"></a>1.2  resnet 结构</h3><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701154852179.png" alt="image-20220701154852179"></p>
<p><strong>Stage 0</strong></p>
<ul>
<li><code>(3,224,224)</code>指输入<code>INPUT</code>的通道数(channel)、高(height)和宽(width)，即<code>(C,H,W)</code>。现假设输入的高度和宽度相等，所以用<code>(C,W,W)</code>表示。</li>
<li>该stage中第1层包括3个先后操作</li>
</ul>
<ol>
<li><code>CONV</code><br> <code>CONV</code>是卷积（Convolution）的缩写，<code>7×7</code>指卷积核大小，<code>64</code>指<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=卷积核&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">卷积核</a>的数量（即该卷积层输出的通道数），<code>/2</code>指卷积核的步长为2。</li>
<li><code>BN</code><br> <code>BN</code>是Batch Normalization的缩写，即常说的BN层。 </li>
<li><code>RELU</code><br> <code>RELU</code>指ReLU激活函数。</li>
</ol>
<ul>
<li><p>该stage中第2层为<code>MAXPOOL</code>，即最大池化层，其kernel大小为<code>3×3</code>、步长为<code>2</code>。</p>
</li>
<li><p><code>(64,56,56)</code>是该stage输出的通道数(channel)、高(height)和宽(width)，其中<code>64</code>等于该stage第1层卷积层中卷积核的数量，<code>56</code>等于<code>224/2/2</code>（步长为2会使输入尺寸减半）。 </p>
</li>
</ul>
<p>总体来讲，在<em>Stage 0</em>中，形状为<code>(3,224,224)</code>的输入先后经过卷积层、<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=BN层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">BN层</a>、ReLU激活函数、<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=MaxPooling层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">MaxPooling层</a>得到了形状为<code>(64,56,56)</code>的输出。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/20/Inception%E7%B3%BB%E5%88%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/20/Inception%E7%B3%BB%E5%88%97/" class="post-title-link" itemprop="url">Inception系列</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-03-20 11:03:00" itemprop="dateCreated datePublished" datetime="2022-03-20T11:03:00+08:00">2022-03-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-03-22 13:03:00" itemprop="dateModified" datetime="2022-03-22T13:03:00+08:00">2022-03-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>一般来说，提升神经网络性能最直接的办法就是增加网络的尺寸，包括增加网络的深度和宽度两个方面。但这种方式存在一些问题：</p>
<ol>
<li><p>参数太多，如果训练数据集有限，很容易产生过拟合；</p>
</li>
<li><p>网络越大，参数越多，计算复杂度越高，难以应用；</p>
</li>
<li><p>网络越深，容易出现梯度弥散问题，难以优化模型；</p>
</li>
</ol>
<h2 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception-v1"></a>Inception-v1</h2><p>深度研究 Inception Net模型之前，必须了解 Inception 网络的一个重要概念：</p>
<p><strong>1×1卷积</strong>：1×1 卷积简单地将输入像素及其所有相应通道映射到输出像素。1×1卷积作为降维模块，一定程度上减少的计算量。</p>
<ul>
<li>例如，在不使用1×1卷积的情况下执行5×5卷积，如下所示：</li>
</ul>
<p><img title="" src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/inceptionNet.png" alt="" data-align="center"></p>
<p>FLOPs运算次数：（14×14×48）×（5×5×480）= 112.9M</p>
<ul>
<li>使用 1×1 卷积：</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/inceptionNet2.png" title="" alt="" data-align="center"></p>
<p>FLOPs运算次数</p>
<p>1×1卷积的操作数= （14×14×16）×（1×1×480）=1.5M<br>5×5卷积的操作数= （14×14×48）×（5×5×16 ) = 3.8M<br>相加后，1.5M + 3.8M = 5.3M</p>
<p>因此1×1卷积可以帮助减少模型大小，这可以在某种程度上帮助减少过度拟合问题；</p>
<p><strong>降维初始模型</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/modified.png" title="" alt="" data-align="center"></p>
<h2 id="Inception-v2"><a href="#Inception-v2" class="headerlink" title="Inception v2"></a>Inception v2</h2><h3 id="Inception-v1架构的问题"><a href="#Inception-v1架构的问题" class="headerlink" title="Inception v1架构的问题"></a>Inception v1架构的问题</h3><p>Inception V1有时会使用5×5等卷积，导致输入维度大幅下降。这导致神经网络使用一些精度下降。其背后的原因是，如果输入维度下降得太快，神经网络容易丢失信息。 此外，与3×3相比，<br>当我们使用更大的卷积（如5×5 ）时，复杂度也会降低. 我们可以在因式分解方面走得更远，即我们可以将3×3卷积分成<strong>1×3</strong>的非对称卷积，然后是3×1卷积。这相当于滑动一个具有与3×3卷积相同感受野但比*3×3便宜33%的两层网络。当输入维度很大但仅当输入大小为mxm时，这种分解不适用于早期层（m 在 12 到 20 之间）。根据 Inception V1 架构，辅助分类器提高了网络的收敛性。他们认为，通过将有用的梯度推到较早的层（以减少损失），它可以帮助减少深层网络中梯度消失问题的影响。但是，这篇论文的作者发现这个分类器在训练的早期并没有很好地提高收敛性。</p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>在BN的论文里，作者提出了Internal Covariate Shift这个问题，即在训练神经网络的过程中，因为前一层的参数变化而导致每层的输入分布都在不断变化（the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change）。这使得我们需要更低的学习率和更小心地进行参数初始化，导致我们难以充分构建一个具有饱满地非线性结构的模型，而这个现象就被称作Internal Covariate Shift。为了解决这个问题，Google提出了Batch Normalization（批规范化）。即在每次梯度下降前，对每个mini-batch做归一化操作来降低数据分布的影响。</p>
<h3 id="小卷积和代替大卷积核"><a href="#小卷积和代替大卷积核" class="headerlink" title="小卷积和代替大卷积核"></a>小卷积和代替大卷积核</h3><p><img title="" src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/5x5replaced3x3.png" alt="灯箱" data-align="center"></p>
<p>该架构还将 nXn 分解转换为<em>1xn</em>和 nx1 分解。正如我们上面讨论的，3×3 卷积可以转换为<em>1×3 ，然后是 3×1 卷积，与<em>*3×3</em></em>相比，计算复杂度降低了 33% 。</p>
<p><img title="" src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/1xn.png" alt="灯箱" data-align="center"></p>
<h2 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception v3"></a>Inception v3</h2><p>Inception v3 类似于 Inception v2，并做了以下改动：</p>
<ul>
<li><p>使用 RMSprop 优化器；</p>
</li>
<li><p>辅助分类器全连接层的BN；</p>
</li>
<li><p>使用 7×7 分解卷积；</p>
</li>
</ul>
<h2 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception v4"></a>Inception v4</h2><p>将 Inception 模块和残差连接 结合起来。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA57qi6bKk6bG85LiO57u_6am0,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt=""></p>
<p>EmbOding!9196#CN360</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Jie Chu</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
