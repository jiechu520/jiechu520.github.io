<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="google-site-verification" content="BTo06tdvlac_Dho4-PFTLmDqjKXr1KtOzavpD8XDA5k" />
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="日常笔记">
<meta property="og:type" content="website">
<meta property="og:title" content="CJ blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="CJ blog">
<meta property="og:description" content="日常笔记">
<meta property="og:locale">
<meta property="article:author" content="Jie Chu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CJ blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">CJ blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">不知名算法工程师</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jie Chu</p>
  <div class="site-description" itemprop="description">日常笔记</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/30/%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/30/%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B/" class="post-title-link" itemprop="url">文生图模型演进</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-30 21:00:00 / Modified: 23:08:00" itemprop="dateCreated datePublished" datetime="2023-12-30T21:00:00+08:00">2023-12-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>此文是为了更好的理解 stable diffusion以及DALL-E 3等最新的图像生成模型，回顾一下在它们之前更早的模型。stable diffusion的作者也是 VQ-GAN的作者，DALL-E3之前还有 DALL-E，DALL-E2。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/640.png" alt="图片"></p>
<h2 id="2-AE"><a href="#2-AE" class="headerlink" title="2. AE"></a>2. AE</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-120f54237d6bc3529d54270139ea8276_r.jpg" alt="img"></p>
<p><strong>作用</strong>：可以用于学习<font color=red> 无标签数据的有效编码</font>, 学习对高维数据的进行低维表示，常用于<font color=red>降维</font>，数据去噪，特征学习。</p>
<p><strong>基本思想</strong>：AE（auto-encoder）是很早之前的技术了，思路也非常简单：用一个编码器（Encoder）把输入编码为 latent vector；然后用 Decoder 将其解码为重建图像，<font color=red>希望重建后的图像与输入图像越接近越好</font>。通常 latent vector 的维度比输入、输出的维度小，因此称之为 bottleneck。AE 是一个自重建的过程，所以叫做“自-编码器”。</p>
<p><strong>特点</strong>：但是模型在 Latent Space 没有增加任何的约束或者正则化，意味着<font color=red>不知道 Latent Space 是如何构建的</font>, 所以很难使用 latent space 来采样生成一个新的图像。</p>
<h2 id="DAE"><a href="#DAE" class="headerlink" title="DAE"></a>DAE</h2><p>DAE（Denoising autoencoder）将原始输入图像进行一定程度的打乱，得到 corrupted input。然后把后者输入AE，目标仍然是希望重建后的图像与原始输入越接近越好。DAE 的效果很不错，原因之一就是图像的冗余度太高了，即使添加了噪声，模型依然能抓取它的特征。而这种方式增强了模型的鲁棒性，防止过拟合。</p>
<h2 id="3-VAE"><a href="#3-VAE" class="headerlink" title="3. VAE"></a>3. VAE</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-9fdb71e7e0b359f900a54878be5f2477_r.jpg" alt="img"></p>
<p><strong>论文</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6114">[1312.6114] Auto-Encoding Variational Bayes</a></p>
<p><strong>作用</strong>：除了AE的作用之外，还广泛应用于生成新的、与训练数据相似但不完全相同的样本。</p>
<p><strong>基本思想</strong>：VAE（Variational autoencoder）仍然由一个编码器和一个解码器构成，并且目标仍然是重建原始输入。但中间不再是学习 latent vector z ，而是学习它的后验分布 $p(z|x)$ ，并假设它遵循<font color=red>多维高斯分布</font>。具体来说，编码器得到两个输出，并分别作为高斯分布的均值和协方差矩阵的对角元（假设协方差矩阵是对角矩阵）。然后在这个高斯分布中采样，送入解码器。实际工程实现中，会用到重参数化（reparameterization）的技巧。</p>
<p><strong>特点</strong>：VAE训练目标除了重构误差，还包括最小化隐空间的KL散度，以确保隐空间与标准正态分布接近。但VAE最大的问题也是这个，使用了固定的先验分布。</p>
<h2 id="4-VQ-VAE"><a href="#4-VQ-VAE" class="headerlink" title="4. VQ-VAE"></a>4. VQ-VAE</h2><blockquote>
<p><strong>VQ(vector quantization)</strong>是一种数据压缩和量化的技术，它可以将连续的向量映射到一组离散的具有代表性的向量中。在深度学习领域，VQ通常用来将连续的隐空间表示映射到一个有限的、离散的 <strong>codebook</strong> 中。</p>
</blockquote>
<p>VAE 具有一个最大的问题就是使用了固定的先验（高斯分布），其次是使用了<strong>连续的中间表征，导致模型的可控性差</strong>。为了解决这个问题，VQ-VAE（Vector Quantized Variational Autoencoder）选择使用<strong>离散的中间表征</strong>，同时，通常会使用一个自回归模型来学习先验（例如 PixelCNN），在训练完成后，用来采样得到 $z_e$。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-956ed05aed4340fd44c2a2853800edf0_r.jpg" alt="img"></p>
<p>图像首先经过encoder，得到$z_e$,它是$H\times W$个 $D$ 维向量。$e_1,e_2,…,e_k$是 $K$ 个 $D$ 维向量，称为codebook。 对于 $z_e$ 中的每个 $D$ 维向量，都可以在 codebook 中找到最接近的 $e_i$, 构成 $z_q$, 这就是decoder的输入。一般 $k=8192, D=512 or 768$。</p>
<p>从 $z_e(x)$ 到 $z_q(x)$ 这个变化可以看成一个聚类，$e_1,e_2,…,e_k$ 可以看作 K 个聚类中心。这样把 encoder 得到的 embedding 离散化了，只由聚类中心表示。</p>
<h3 id="4-1-VQ-VAE的训练"><a href="#4-1-VQ-VAE的训练" class="headerlink" title="4.1 VQ-VAE的训练"></a>4.1 VQ-VAE的训练</h3><p>在VQ中使用 Argmin来获取最小的距离，这一步是不可导的，因为无法将 Decoder 和 Encoder联合训练，针对这个问题，作者添加了一个trick，如上图红线所示：直接将 $z_q(x)$的梯度cooy给$Z_e(x)$, 而不是给 codebook里面的embedding。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E6%8E%A8%E6%88%AA%E5%9B%BE_20240112000325.jpg" alt="推推截图_20240112000325"></p>
<p>VQ-VAE的loss如上所示，分成三部分，第一项用来训练 encoder和decoder，第二项叫 codebook loss,只训练 codebook，让codebook中的embedding向各自最近的$Z_e(x)$靠近VQGAN。第三项叫 commitment loss,只训练encoder, 目的是encourage the output of encoder to stay close to the chosen codebook vector to prevent it from flucturating too frequently from one code vector to another, 即防止encoder的输出频繁在各个codebook embedding之间跳</p>
<p>具体代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VectorQuantizer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_embeddings, embedding_dim, commitment_cost</span>):</span><br><span class="line">        <span class="built_in">super</span>(VectorQuantizer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self._embedding_dim = embedding_dim</span><br><span class="line">        self._num_embeddings = num_embeddings</span><br><span class="line">        </span><br><span class="line">        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)</span><br><span class="line">        self._embedding.weight.data.uniform_(-<span class="number">1</span>/self._num_embeddings, <span class="number">1</span>/self._num_embeddings)</span><br><span class="line">        self._commitment_cost = commitment_cost</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># convert inputs from BCHW -&gt; BHWC</span></span><br><span class="line">        inputs = inputs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        input_shape = inputs.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Flatten input</span></span><br><span class="line">        flat_input = inputs.view(-<span class="number">1</span>, self._embedding_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate distances</span></span><br><span class="line">        distances = (torch.<span class="built_in">sum</span>(flat_input**<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">                    + torch.<span class="built_in">sum</span>(self._embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">                    - <span class="number">2</span> * torch.matmul(flat_input, self._embedding.weight.t()))</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Encoding</span></span><br><span class="line">        encoding_indices = torch.argmin(distances, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        encodings = torch.zeros(encoding_indices.shape[<span class="number">0</span>], self._num_embeddings, device=inputs.device)</span><br><span class="line">        encodings.scatter_(<span class="number">1</span>, encoding_indices, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Quantize and unflatten</span></span><br><span class="line">        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loss</span></span><br><span class="line">        e_latent_loss = F.mse_loss(quantized.detach(), inputs)</span><br><span class="line">        q_latent_loss = F.mse_loss(quantized, inputs.detach())</span><br><span class="line">        loss = q_latent_loss + self._commitment_cost * e_latent_loss</span><br><span class="line">        </span><br><span class="line">        quantized = inputs + (quantized - inputs).detach()</span><br><span class="line">        avg_probs = torch.mean(encodings, dim=<span class="number">0</span>)</span><br><span class="line">        perplexity = torch.exp(-torch.<span class="built_in">sum</span>(avg_probs * torch.log(avg_probs + <span class="number">1e-10</span>)))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert quantized from BHWC -&gt; BCHW</span></span><br><span class="line">        <span class="keyword">return</span> loss, quantized.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous(), perplexity, encodings</span><br></pre></td></tr></table></figure>
<h3 id="4-2-VQ-VAE-PixelCNN"><a href="#4-2-VQ-VAE-PixelCNN" class="headerlink" title="4.2 VQ-VAE + PixelCNN"></a>4.2 VQ-VAE + PixelCNN</h3><p>有了上述的 VQ-VAE 模型，可以很容易实现图像<strong>压缩、重建</strong>的目的，但是无法生成新的图像数据。当然可以随机生成 Index，然后对应生成量化后的 latent code，进而使用 Decoder 来生成输出图像。但是这样的 latent code 完全没有全局信息甚至局部信息，因为每个位置都是随机生成的。因此，作者引入了 PixelCNN 来自回归的生成考虑了全局信息的 latent code，进而可以生成更真实的图像，如下图所示：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240112001504547.png" alt="image-20240112001504547"></p>
<p>PixelCNN 和 VQ-VAE 的一作是同一个人，来自 Google DeepMind，对应的论文为：Conditional Image Generation with PixelCNN Decoders。此处我们不再对 PixelCNN 展开，只需要知道它是一个自回归生成模型，可以逐个像素的生成，因为其是自回归模型，所以每个位置都能看到之前位置的信息，这样生成的 latent code 能够更全面的考虑到空间信息，有助于提高模型生成图像的质量和多样性。</p>
<h3 id="4-3-VQ-VAE-2"><a href="#4-3-VQ-VAE-2" class="headerlink" title="4.3 VQ-VAE-2"></a>4.3 VQ-VAE-2</h3><p>VQ-VAE-2 的模型结构如下图所示，以 256x256 的图像压缩重建为例：</p>
<ul>
<li>训练阶段：其首先使用 Encoder 将图像压缩到 Bottom Level，对应大小为 64x64，然后进一步使用 Encoder 压缩到 Top Level，大小为 32x32。重建时，首先将 32x32 的表征经过 VQ 量化为 latent code，然后经过 Decoder 重建 64x64 的压缩图像，再经过 VQ 和 Decoder 重建 256x256 的图像。</li>
<li>推理阶段（图像生成）：使用 PixelCNN 首先生成 Top Level 的离散 latent code，然后作为条件输入 PixelCNN 以生成 Bottom Level 的更高分辨率的离散 latent code。之后使用两个 Level 的离散 latent code 生成最终的图像。</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/640-20240112002031130.png" alt="图片"></p>
<p>当然，基于这个思想作者也进一步验证了使用 3 个 Level 来生成 1024x1024 分辨率的图像，相应的压缩分辨率分别为 128x128、64x64、32x32。</p>
<h2 id="5-VQ-GAN"><a href="#5-VQ-GAN" class="headerlink" title="5 VQ-GAN"></a>5 VQ-GAN</h2><h3 id="5-1-概述"><a href="#5-1-概述" class="headerlink" title="5.1 概述"></a>5.1 概述</h3><p>paper:<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html">https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html</a></p>
<p>code:<a target="_blank" rel="noopener" href="https://github.com/CompVis/taming-transformers">https://github.com/CompVis/taming-transformers</a></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230512112944268.png" alt="image-20230512112944268"></p>
<p>VQ-GAN 相比较 VQ-VAE 的主要改变有以下几点：</p>
<ul>
<li>引入 GAN 的思想，将 VQ-VAE 当做生成器（Generator），并加入判别器（Discriminator），以对生成图像的质量进行判断、监督，以及加入感知重建损失（不只是约束像素的差异，还约束 feature map 的差异），以此来重建更具有保真度的图片，也就学习了更丰富的 codebook。</li>
<li>将 PixelCNN 替换为性能更强大的自回归 GPT2 模型（针对不同的任务可以选择不同的规格）</li>
<li>引入滑动窗口自注意力机制，以降低计算负载，生成更大分辨率的图像。</li>
</ul>
<p>VQ-GAN 也是 stable diffusion的作者。</p>
<h3 id="5-2-方法"><a href="#5-2-方法" class="headerlink" title="5.2 方法"></a>5.2 方法</h3><p>模型结构如上图所示，实际训练的时候是分为<font color=red>两阶段训练的</font>。</p>
<p>如下图所示，第一阶段训练，相比 VQ-VAE 主要是增加 Discriminator， 以及将重建损失替换成 <font color=red> LPIPS损失：</font></p>
<ul>
<li>Discriminator：对生成的图像块进行判别，每一块都会返回 True 和 False，然后将对应的损失加入整体损失中。</li>
<li>LPIPS：除了像素级误差外，也会使用 VGG 提取 input 图像和 reconstruction 图像的多尺度 feature map，以监督对应的误差（具体可参考 lpips.py - CompVis/taming-transformers · GitHub）。</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113215830827.png" alt="image-20240113215830827"></p>
<h2 id="6-DALL-E-dVAE-DALL-E"><a href="#6-DALL-E-dVAE-DALL-E" class="headerlink" title="6. DALL-E (dVAE, DALL-E)"></a>6. DALL-E (dVAE, DALL-E)</h2><h3 id="6-1-概述"><a href="#6-1-概述" class="headerlink" title="6.1 概述"></a>6.1 概述</h3><p>DALL-E 最主要的贡献是提供了不错的文本引导图片生成的能力，其不是在 VQ-VAE 基础上修改，而是首先引入 VAE 的变种 dVAE，然后在此基础上进一步训练 DALL-E。可惜的是，OpenAI 并不 Open，只开源了 dVAE 部分模型，文本引导生成部分并没有开源，不过 Huggingface 和 Google Cloud 团队进行了复现，并发布对应的 DALL-E mini 模型。</p>
<p>DALL-E 对应的论文为：[2102.12092] Zero-Shot Text-to-Image Generation。对应的代码库为：GitHub - openai/DALL-E: PyTorch package for the discrete VAE used for DALL·E.。</p>
<p>DALL-E mini 对应的文档为：DALL-E Mini Explained，对应的代码库为：GitHub - borisdayma/dalle-mini: DALL·E Mini - Generate images from a text prompt。</p>
<h3 id="6-2-dVAE"><a href="#6-2-dVAE" class="headerlink" title="6.2 dVAE"></a>6.2 dVAE</h3><p>dVAE（discrete VAE）与VQ-VAE的区别在于<font color=red>引入 Gumbel Softmax 来训练</font>, 避免 VQ-VAE 训练中 ArgMin 不可导的问题。 </p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113222402539.png" alt="image-20240113222402539"></p>
<h3 id="6-3-模型训练"><a href="#6-3-模型训练" class="headerlink" title="6.3 模型训练"></a>6.3 模型训练</h3><p>有了 dVAE 模型之后，第二阶段就是就是训练 Transformer（此阶段会固定 dVAE），使其具备文本引导生成的能力。DALL-E 使用大规模的图像-文本对数据集进行训练，训练过程中使用 dVAE 的 Encoder 将图像编码为离散的 latent code。然后将文本输入 Transformer，并使用生成的 latent code 来作为 target 输出。以此就可以完成有监督的自回归训练。推理时只需输入文本，然后逐个生成图像对应的 Token，直到生成 1024 个，然后将其作为离散的 latent code 进一步生成最终图像。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113222517785.png" alt="image-20240113222517785"></p>
<p>最终作者在 1024 个 16G 的 V100 GPU 上完成训练，batch size 为 1024，总共更新了 430,000 次模型，也就相当于训练了 4.3 亿图像-文本对（训练集包含 250M 图像-文本对，主要是 Conceptual Captions 和 YFFCC100M）。</p>
<h3 id="6-4-DALL-E-mini-模型概述"><a href="#6-4-DALL-E-mini-模型概述" class="headerlink" title="6.4 DALL-E mini 模型概述"></a>6.4 DALL-E mini 模型概述</h3><p>如下图所示，DALL-E mini 中作者使用 VQ-GAN 替代 dVAE，使用 Encoder + Decoder 的 BART 替代 DALL-E 中 Decoder only 的 Transformer。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113222725076.png" alt="dalle-e mini infer"></p>
<p>在推理过程中，不是生成单一的图像，而是会经过采样机制生成多个 latent code，并使用 VQ-GAN 的 Decoder 生成多个候选图像，之后再使用 CLIP 提取这些图像的 embedding 和文本 embedding，之后进行比对排序，挑选出最匹配的生成结果。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113222818173.png" alt="image-20240113222818173"></p>
<h3 id="6-5-DALL-E-mini-和-DALL-E-对比"><a href="#6-5-DALL-E-mini-和-DALL-E-对比" class="headerlink" title="6.5 DALL-E mini 和 DALL-E 对比"></a>6.5 DALL-E mini 和 DALL-E 对比</h3><p>DALL-E mini 和 DALL-E 在模型、训练上都有比较大的差异，具体体现在：</p>
<ul>
<li>DALL-E 使用 12B 的 GPT-3 作为 Transformer，而 mini 使用的是 0.4B 的 BART，小 27 倍。</li>
<li>mini 中使用预训练的 VQ-GAN、BART 的 Encoder 以及 CLIP，而 DALL-E 从头开始训练，mini 训练代价更小。</li>
<li>DALL-E 使用 1024 个图像 Token，词表更小为 8192，而 mini 使用 256 个图像 Token，词表大小为 16384。</li>
<li>DALL-E 支持最多 256 个文本 Token，对应词表为 16,384，mini 支持最多 1024 文本 Token，词表大小为 50,264。</li>
<li>mini 使用的 BART 是 Encoder + Decoder 的，因此文本是使用双向编码，也就是每个文本 Token 都能看到所有文本 Token，而 DALL-E 是 Decoder only 的 GPT-3，文本 Token 只能看到之前的 Token。</li>
<li>DALL-E 使用 250M 图像-文本对训练，而 mini 只使用了 15M。</li>
</ul>
<h2 id="7-CLIP-VQ-GAN-VQGAN-CLIP"><a href="#7-CLIP-VQ-GAN-VQGAN-CLIP" class="headerlink" title="7 CLIP+VQ-GAN(VQGAN-CLIP)"></a>7 CLIP+VQ-GAN(VQGAN-CLIP)</h2><p>Katherine 等人将 VQ-GAN 和 OpenAI 发布的 CLIP 模型结合起来，利用 CLIP 的图文对齐能力来赋予 VQ-GAN 文本引导生成的能力。其最大的优势是不需要额外的预训练，也不需要对 CLIP 和 VQ-GAN 进行微调，只需在推理阶段执行少量的迭代即可实现。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20240113223222191.png" alt="image-20240113223222191"></p>
<p>如上图所示：使用初始图像通过 VQ-GAN 生成一个图像，然后使用 CLIP 对生成图像和 Target Text 提取 embedding，然后计算相似性，并将其误差作为反馈对隐空间的 Z-vector 进行迭代更新，直到生成图像和 Target Text 对应的 embedding 很相似为止。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.00937">https://arxiv.org/abs/1711.00937</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.05328">https://arxiv.org/abs/1606.05328</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.00446">https://arxiv.org/abs/1906.00446</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.09841">https://arxiv.org/abs/2012.09841</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/CompVis/taming-transformers">https://github.com/CompVis/taming-transformers</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.12092">https://arxiv.org/abs/2102.12092</a></li>
<li><a target="_blank" rel="noopener" href="https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained--Vmlldzo4NjIxODA">https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained--Vmlldzo4NjIxODA</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/borisdayma/dalle-mini">https://github.com/borisdayma/dalle-mini</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.08583">https://arxiv.org/abs/2204.08583</a></li>
<li><a target="_blank" rel="noopener" href="https://python.plainenglish.io/variational-autoencoder-1eb543f5f055">https://python.plainenglish.io/variational-autoencoder-1eb543f5f055</a></li>
<li><a target="_blank" rel="noopener" href="https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/">https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/20/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/20/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-20 00:09:14" itemprop="dateCreated datePublished" datetime="2023-12-20T00:09:14+08:00">2023-12-20</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/21/EVA02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/21/EVA02/" class="post-title-link" itemprop="url">EVA02解析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-04-21 21:03:00" itemprop="dateCreated datePublished" datetime="2023-04-21T21:03:00+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-22 23:03:00" itemprop="dateModified" datetime="2023-04-22T23:03:00+08:00">2023-04-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="EVA-02-A-Visual-Representation-for-Neon-Genesis"><a href="#EVA-02-A-Visual-Representation-for-Neon-Genesis" class="headerlink" title="EVA-02: A Visual Representation for Neon Genesis"></a>EVA-02: A Visual Representation for Neon Genesis</h1><p>EVA02的目标是作为下一代的基于Transformer的视觉表示模型。</p>
<p>文章主要包含两个部分：1、对普通Vit的架构改进，2、MIM的预训练策略</p>
<p><strong>总结</strong></p>
<p>EVA02主要有两个改进，一是，通过实验的方法来观察采用哪些NLP方向关于Vit的改进；二是，增加视觉特征编码的容量以及增加训练的轮数和图像的size；</p>
<h2 id="1-Architecture"><a href="#1-Architecture" class="headerlink" title="1 Architecture"></a>1 Architecture</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230629182751431.png" alt="image-20230629182751431"></p>
<p>ViT主要由 MHSA（用于全局空间信息聚合）和 pointwise的FFNs（特征变换）交错组成。但是NLP方面很多针对ViT的修改没有在视觉上应用。作者做了一个实验来探索不同的修改带来的影响：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230703191529244.png" alt="image-20230703191529244"></p>
<p><strong>Gelu</strong>: $GELU(x)=x * \phi(x),x \sim N(0,1)$</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-0d8b979444f64ab49d4bc0f4199a15c2_r.jpg" alt="img"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%9B%BE20230914183157.png" alt="截图20230914183157"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/v2-e31b1e5b4333a90fbcca03c9a863a0c5_r.jpg" alt="img"></p>
<h2 id="2-pre-trainning-strategy"><a href="#2-pre-trainning-strategy" class="headerlink" title="2 pre-trainning strategy"></a>2 pre-trainning strategy</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230703195140612.png" alt="image-20230703195140612"></p>
<p><strong>MIM teacher model变大，训练的epoch也需要变多；</strong></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230703195449659.png" alt="image-20230703195449659"></p>
<p><strong>分辨率的增加以及在imgnet数据上的有监督ft也会增加性能</strong></p>
<p>预训练目标类似于 EVA [44]，即仅以可见图像块为条件回归屏蔽图像文本对齐的视觉特征。我们使用 [MASK] 标记破坏输入补丁，并按照 [5, 44] 使用掩码率为 40% 的分块掩码。</p>
<p>MIM 预训练的目标表示来自可公开访问的 EVA-CLIP [44] 视觉塔，具有 10 亿个参数。 EV A-02 的输出特征首先被归一化 [4]，然后通过线性层投影到与 EVA-CLIP 的视觉特征相同的维度。我们使用负余弦相似度作为损失函数。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/21/EMA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/21/EMA/" class="post-title-link" itemprop="url">EMA原理和pytorch实现</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-21 21:03:00" itemprop="dateCreated datePublished" datetime="2023-03-21T21:03:00+08:00">2023-03-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-22 23:03:00" itemprop="dateModified" datetime="2023-03-22T23:03:00+08:00">2023-03-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="EMA的原理和pytorch实现"><a href="#EMA的原理和pytorch实现" class="headerlink" title="EMA的原理和pytorch实现"></a>EMA的原理和pytorch实现</h1><h2 id="EMA-定义"><a href="#EMA-定义" class="headerlink" title="EMA 定义"></a>EMA 定义</h2><p>指数移动平均也叫权重移动平均，是一种给予近期数据更高权重的平均方法。</p>
<h2 id="在深度学习的优化中的EMA"><a href="#在深度学习的优化中的EMA" class="headerlink" title="在深度学习的优化中的EMA"></a>在深度学习的优化中的EMA</h2><p>在深度学习的优化过程中，$\theta_t$ 是 t 时刻的模型权重 weight， $\upsilon_t$ 是 t 时刻的影子权重。在梯度下降的过程中，会一直维护着这个影子权重，但是这个影子权重不会参与训练。基本的假设是<strong>模型权重在最后 n 步内，会在实际的最优点处抖动，所以我们去最后 n 步的平均，能使模型更加的鲁棒。</strong></p>
<h3 id="EMA为何有效"><a href="#EMA为何有效" class="headerlink" title="EMA为何有效"></a>EMA为何有效</h3><p>因为在训练的时候，会使用验证集来衡量模型精度，但是验证集和测试集并不完全一致，在训练后期阶段，模型可能已经在测试集最佳精度附近波动，所以使用ema的结果会比使用单一结果更可靠。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230307191607861.png" alt="image-20230307191607861"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20230307191620727.png" alt="image-20230307191620727"></p>
<h3 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EMA</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, decay</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.decay = decay</span><br><span class="line">        self.shadow = &#123;&#125;</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">register</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                self.shadow[name] = param.data.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.shadow</span><br><span class="line">                new_average = (<span class="number">1.0</span> - self.decay) * param.data + self.decay * self.shadow[name]</span><br><span class="line">                self.shadow[name] = new_average.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply_shadow</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.shadow</span><br><span class="line">                self.backup[name] = param.data</span><br><span class="line">                param.data = self.shadow[name]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">restore</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.backup</span><br><span class="line">                param.data = self.backup[name]</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">ema = EMA(model, <span class="number">0.999</span>)</span><br><span class="line">ema.register()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程中，更新完参数后，同步update shadow weights</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    optimizer.step()</span><br><span class="line">    ema.update()</span><br><span class="line"></span><br><span class="line"><span class="comment"># eval前，apply shadow weights；eval之后，恢复原来模型的参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>():</span><br><span class="line">    ema.apply_shadow()</span><br><span class="line">    <span class="comment"># evaluate</span></span><br><span class="line">    ema.restore()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/02/24/lora/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/24/lora/" class="post-title-link" itemprop="url">Lora</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-02-24 23:00:00 / Modified: 23:53:00" itemprop="dateCreated datePublished" datetime="2023-02-24T23:00:00+08:00">2023-02-24</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h1><p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a><br>代码链接：<a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a></p>
<p>LoRA是一种finetune扩散模型的训练技术。通过对标准的checkpoint模型微小的修改，可以比checkpoint模型小10到100倍。LoRA的原理比较简单，原始全量的finetune其实就是在原始模型参数基础上加入增量$W=W_0+\Delta W$，那么我们可以通过冻结原始参数 $W_0$，并且把增量部分通过低秩分解方式进一步降低参数量级$\Delta W = A*B^T$, 原始参数的维度是 $d*d$, 则低秩分解后的参数量级是 $2*r*d$, 这里 $r&lt;&lt;d$， 因此可以起到大幅降低微调参数量级的效果。</p>
<p>和textula inversion一样，不能直接使用LoRA模型，需要和checkpoint文件一起使用。</p>
<h2 id="How-does-LoRA-work-in-Stable-diffusion？"><a href="#How-does-LoRA-work-in-Stable-diffusion？" class="headerlink" title="How does LoRA work in Stable diffusion？"></a>How does LoRA work in Stable diffusion？</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/45f7e2ce00c48b8c96e938c9fe8ed3d4.png" alt="image"></p>
<p>LoRA通过在checkpoint上做小的修改替换风格，具体而言修改的地方是UNet中的cross-attention层。该层是图像和文本prompt交界的层。LORA的作者们发现微调该部分足以实现良好的性能。</p>
<p>cross attention层的权重是一个矩阵，LoRA fine tune这些权重来微调模型。那么LoRA是怎么做到模型文件如此小？LoRA的做法是将一个权重矩阵分解为两个矩阵存储，能起到的作用可以用下图表示，参数量由(1000<em> 2000)减少到(1000</em> 2+2000*2), 大概300多倍！</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/10bbf49b2e9042779941c600a692d74dtplv-k3u1fbpfcp-zoom-in-crop-mark1512000.webp" alt="img"></p>
<p><strong>核心代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 初始化低秩矩阵A和B</span></span><br><span class="line">self.lora_A.update(nn.ModuleDict(&#123;adapter_name: nn.Linear(self.in_features, r, bias=<span class="literal">False</span>)&#125;))</span><br><span class="line">self.lora_B.update(nn.ModuleDict(&#123;adapter_name: nn.Linear(r, self.out_features, bias=<span class="literal">False</span>)&#125;))</span><br><span class="line">self.scaling[adapter_name] = lora_alpha / r</span><br><span class="line"></span><br><span class="line"><span class="comment">## 向前计算</span></span><br><span class="line">result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)</span><br><span class="line">result += (</span><br><span class="line">    self.lora_B[self.active_adapter](</span><br><span class="line">        self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))</span><br><span class="line">    )</span><br><span class="line">    * self.scaling[self.active_adapter]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>alpha参数：alpha其实是个缩放参数，本质和learning rate相同。</p>
<h2 id="cross-attention"><a href="#cross-attention" class="headerlink" title="cross attention"></a>cross attention</h2><p>cross-attention是扩散模型中关键的技术之一，在LoRA中通过微调该模块，即可微调生成图片的样式，而在Hypernetwork中使用两个带有dropout和激活函数的全链接层，分别修改cross attention中的key和value，也可以定制想要的生成风格。可见cross attention的重要性。</p>
<p>在讲cross-attention之前，先看看经典的transformer中attention的含义，attnetion实际上用了三个QKV矩阵，来计算不同token之间的彼此的依赖关系，Q和K可以用来计算当前token和其他token的相似度，这个相似度作为权值对V进行加权求和，可以作为下一层的token。更通俗点说，Q和k的作用是用来在token之间搬运信息，而value本身就是从当前token当中提取出来的信息. 比较常见的是self-attention，该注意力是一个sequence内部不同token间产生注意力，而cross-attention的区别是在不同的sequence之间产生注意力。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/21/dali%E9%A2%84%E5%A4%84%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/01/21/dali%E9%A2%84%E5%A4%84%E7%90%86/" class="post-title-link" itemprop="url">DALI加速图像数据预处理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-01-21 23:03:00" itemprop="dateCreated datePublished" datetime="2023-01-21T23:03:00+08:00">2023-01-21</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="DALI预处理加速"><a href="#DALI预处理加速" class="headerlink" title="DALI预处理加速"></a>DALI预处理加速</h1><p>NVIDIA DALI 文档：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/general/data_loading/external_input.html">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/general/data_loading/external_input.html</a></p>
<p>安装：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html#pip-official-releases">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html#pip-official-releases</a></p>
<h2 id="1、DALI-pipeline"><a href="#1、DALI-pipeline" class="headerlink" title="1、DALI pipeline"></a>1、DALI pipeline</h2><p>DALI可以选择纯CPU加载和预处理或者CPU&amp;GPU混合加载，GPU加载。</p>
<p>在DALI中，任何数据处理任务都有一个称为 Pipeline 的对象， Pipeline 对象是类的实例<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline"><code>nvidia.dali.Pipeline</code></a>或派生类。</p>
<p>可以通过以下方式定义DALI Pipeline</p>
<ol>
<li>通过实现内部使用 DALI 运算符的函数并使用<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.pipeline_def"><code>pipeline_def()</code></a>装饰器对其进行装饰。</li>
<li>通过<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline"><code>Pipeline</code></a>直接实例化对象、构建图形并使用<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline.set_outputs"><code>Pipeline.set_outputs()</code></a>.</li>
<li>通过从<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline"><code>Pipeline</code></a>类继承并覆盖<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html#nvidia.dali.Pipeline.define_graph"><code>Pipeline.define_graph()</code></a>（这是定义 DALI Pipelines 的传统方式）</li>
</ol>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%9B%BE20230907195707.png" alt="截图20230907195707"></p>
<h2 id="2、图像分类的pipeline示例"><a href="#2、图像分类的pipeline示例" class="headerlink" title="2、图像分类的pipeline示例"></a>2、图像分类的pipeline示例</h2><p>所有操作均在GPU上，note：<strong>使用gpu进行预处理，会占用显存，模型越大占用越多，但是GPU利用率会一直保持在100%</strong>。模型较大不推荐使用GPU加载。</p>
<p><strong>使用纯CPU操作，数据处理的速度也比 torchvision快</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TrainPipeline</span>(<span class="title class_ inherited__">Pipeline</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_threads, device_id, data_root, img_size, n_holes, length, custom_cutout=<span class="literal">False</span></span>):</span><br><span class="line">         </span><br><span class="line">        <span class="built_in">super</span>(TrainPipeline, self).__init__(batch_size, num_threads, device_id, prefetch_queue_depth=<span class="number">4</span>)</span><br><span class="line">        mode = <span class="string">&#x27;gpu&#x27;</span></span><br><span class="line">        self.decode = ops.decoders.Image(device=<span class="string">&#x27;mixed&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">        self.img_size = img_size</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># readers.File类似torchvision.datasets.ImageFolder，dali还有其他高阶API，可自行研究使用</span></span><br><span class="line">        self.<span class="built_in">input</span> = ops.readers.File(file_root=data_root, random_shuffle=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># Resize</span></span><br><span class="line">        self.resize = ops.Resize(device=mode, resize_x=<span class="built_in">int</span>(img_size*<span class="number">1.2</span>), resize_y=<span class="built_in">int</span>(img_size*<span class="number">1.2</span>))</span><br><span class="line">        <span class="comment"># Randomcrop，类似于torchvision.transforms.RandomCrop</span></span><br><span class="line">        self.randomcrop = ops.RandomResizedCrop(device=mode, size=img_size, random_area=[<span class="number">0.3</span>, <span class="number">1.0</span>])</span><br><span class="line">        <span class="comment"># CropMirrorNormalize可以实现normalize和随机水平翻转，类似于torchvision.transforms.Normalize &amp; RandomHorizontalFlip</span></span><br><span class="line">        self.normalize = ops.CropMirrorNormalize(device=mode, mean=[<span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>],</span><br><span class="line">                                                 std=[<span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>, <span class="number">0.5</span>*<span class="number">255</span>])</span><br><span class="line">        <span class="comment"># 获取随机数</span></span><br><span class="line">        self.rng1 = ops.random.Uniform()</span><br><span class="line">        self.rng2 = ops.random.CoinFlip()</span><br><span class="line">        <span class="comment"># 实例化改变图片色彩的类，类似于torchvision.transforms.ColorJitter</span></span><br><span class="line">        self.colortwist = ops.ColorTwist(device=mode)</span><br><span class="line">        <span class="comment"># 实例化旋转图像的类，类似于torchvision.transforms.RandomRotation</span></span><br><span class="line">        self.rotate = ops.Rotate(device=mode, fill_value=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># gridmask，类似于cutout这种随机遮挡块操作</span></span><br><span class="line">        self.gridmask = ops.GridMask(device=mode)</span><br><span class="line">       </span><br></pre></td></tr></table></figure>
<p>如果需要自定义数据处理的函数，可参考一下方式。以cutout为例：cutout使用的是cpu处理了，如果是gpu处理的话，<strong>需要将numpy改成cupy，DALI原生支持的操作和数据增强挺丰富的。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CUTOUT</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_holes, length</span>):</span><br><span class="line">        self.n_holes = n_holes</span><br><span class="line">        self.length = length</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, imgs</span>):</span><br><span class="line">        c, h, w = imgs.shape</span><br><span class="line">        mask = np.ones((h, w), np.float32)</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(self.n_holes):</span><br><span class="line">            y = np.random.randint(h)</span><br><span class="line">            x = np.random.randint(w)</span><br><span class="line">            y1 = np.clip(y - self.length // <span class="number">2</span>, <span class="number">0</span>, h)</span><br><span class="line">            y2 = np.clip(y + self.length // <span class="number">2</span>, <span class="number">0</span>, h)</span><br><span class="line">            x1 = np.clip(x - self.length // <span class="number">2</span>, <span class="number">0</span>, w)</span><br><span class="line">            x2 = np.clip(x + self.length // <span class="number">2</span>, <span class="number">0</span>, w)</span><br><span class="line">            mask[y1: y2, x1: x2] = <span class="number">0.</span></span><br><span class="line">        mask = np.expand_dims(mask, <span class="number">0</span>).repeat(c, axis=<span class="number">0</span>)</span><br><span class="line">        imgs = imgs * mask</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> imgs</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 然后在上面的 TrainPipeline上加上下面这行,model 是 “cpu”</span></span><br><span class="line">    self.mask = ops.PythonFunction(device=<span class="string">&quot;cpu&quot;</span>, function=CUTOUT(n_holes, length), num_outputs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>图像分类数据加载的时候的调用方式：其他的Iterator可以参考 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/plugins/pytorch_tutorials.html">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/plugins/pytorch_tutorials.html</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nvidia.dali.plugin.pytorch <span class="keyword">import</span> DALIClassificationIterator</span><br><span class="line"><span class="keyword">from</span> nvidia.dali.plugin.base_iterator <span class="keyword">import</span> LastBatchPolicy</span><br><span class="line"> </span><br><span class="line">pipe_train = TrainPipeline(batch_size, num_threads, device_id, data_root, img_size, n_holes, length,custom_cutout=custom_cutout)</span><br><span class="line">pipe_train.build()</span><br><span class="line">         </span><br><span class="line"><span class="comment"># DALIClassificationIterator: 返回pytorch tensor 形式是 (data and label) , 即DataLoader</span></span><br><span class="line">train_loader = DALIClassificationIterator(pipe_train, size=pipe_train.epoch_size(<span class="string">&#x27;Reader&#x27;</span>),last_batch_policy=LastBatchPolicy.PARTIAL, auto_reset=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/06/21/DARN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/21/DARN/" class="post-title-link" itemprop="url">图片美观度模型</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-21 21:00:00" itemprop="dateCreated datePublished" datetime="2022-06-21T21:00:00+08:00">2022-06-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-06-22 23:08:00" itemprop="dateModified" datetime="2022-06-22T23:08:00+08:00">2022-06-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="360图片搜索—图片美观度模型"><a href="#360图片搜索—图片美观度模型" class="headerlink" title="360图片搜索—图片美观度模型"></a>360图片搜索—图片美观度模型</h1><p><a target="_blank" rel="noopener" href="https://github.com/lmm360/Image-aesthetic-assessment">code和数据集地址</a></p>
<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>在图片搜索中，用户希望搜索的图片不但和自己的查询相关，并且在视觉感官、图片质量等方面也要比较满意。此时，就需要我们计算图片图片的在美学上的特征分值，加到排序模型中，将高相关性、高美学质量的图像排在检索结果的前面。</p>
<p>但是现有的图片质量模型主要考虑图像的质量，如像素，清晰度、有无噪声，在图像的美学特征，如构图、色彩、内容和谐等考虑的权重偏低。</p>
<h2 id="二、相关研究"><a href="#二、相关研究" class="headerlink" title="二、相关研究"></a>二、相关研究</h2><p>大部分的工作将这种美观度评估的任务形式化为回归或者分类问题。开源的数据集有 AVA【1】, AADB【2】等。代表的工作有 NIMA模型【3】和DARN模型【4】。360美观度模型正是在DARN模型的基础上改进得到的。</p>
<p><strong>NIMA模型</strong>利用了AVA数据集，AVA数据集包含了约 25w张图像，由业余摄影师根据审美品质进行评分，200人评分的平均分就是该张图像的最终的分数。</p>
<p><strong>NIMA模型</strong>目标是预测与人类评分的相关性，而不是将图片分类或者回归到平均分。提出了<strong>EMD loss</strong>，它显示了有序类分类的性能提升。下面是模型结构。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026144748579.png" alt="image-20231026144748579"></p>
<p>开源的数据集AVA、AADB图像与图片搜索的图像在种类和美学质量分布上有很大的差异，所以不能再公开的数据集上训练模型。最好的方式使构建自己的数据集，但是像AVA数据集的构建，需要一批具有美学专业知识的标注人员对每张图片进行标注。标注成本和难度都有很大</p>
<p>Microsoft提出了<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.00309">《An Universal Image Attractiveness Ranking Framework》</a>，很好的解决美学数据集构建难的问题，并提出了新的美学模型——DARN模型。下面介绍DARN模型以及360美观度模型在此之上的改进。</p>
<h2 id="三、基于DARN的美观度模型"><a href="#三、基于DARN的美观度模型" class="headerlink" title="三、基于DARN的美观度模型"></a>三、基于DARN的美观度模型</h2><h3 id="1、数据集构建"><a href="#1、数据集构建" class="headerlink" title="1、数据集构建"></a>1、数据集构建</h3><p>数据集的构建过程如下：</p>
<p>1、在图片搜索中高中低频queey中各选择2000条query。</p>
<p>2、根据query在图片搜索返回结果中，top20里面，top40-60里面各抽取两张图片。</p>
<p>3、根据Swiss-system tournament规则、对这4张图像进行三轮标注。</p>
<p>4、每一轮标注，将同一query下4张图片组成两对，每一对图像由7个人标注人员去进行投票：</p>
<p>左边更好，左边好一点，一样好，右边好一点，右边更好。</p>
<p>具体标注可以参考下图：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026154542693.png" alt="image-20231026154542693"></p>
<p>这样标注的好处是，标注人员无需掌握专业知识，“比较”相对于给出一个直接的分值更简单，多人投票也降低主观偏好带来的偏差。7个标注人员对image1和image2进行投票的结果中，左边更好，左边好一点，一样好，右边好一点，右边更好的人数分别为0、1、1、4、1，则这对图像的label为：</p>
<script type="math/tex; mode=display">
l(image1,image2)=(0,1,1,4,1)</script><h3 id="2、DARN模型"><a href="#2、DARN模型" class="headerlink" title="2、DARN模型"></a>2、DARN模型</h3><p>我们的标注数据是pair对，模型结构如下：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026163835872.png" alt="image-20231026163835872"></p>
<p>左边的网络和右边的网络是参数共享的，图像对输入到网络中，经过 Deep CNN（如resnet50）的特征抽取，再经过三个fc层，然后计算最终输出的方差和均值。</p>
<p>假设每张图像都是由很多的专业的美学专家评分得到的，那么根据中心极限定理，图像美观度分值$X$服从正态分布，AVA数据基本就符合正态分布。所以<strong>最终计算得到的均值和方差就可以看作是模型计算的该图像的平均美观度分值和方差</strong>。</p>
<p>对于一个pair对 $[x_i,x_j]$, 它们的 label 记为${0,1,2,3,4}$ 分别对应着左边更好，左边好一点，一样好，右边好一点，右边更好。对于我们的标注数据有：</p>
<script type="math/tex; mode=display">
\sum_{y=0}^4{P_{ij}(xi-xj)=y}=1</script><p>直觉上，图像通过模型得到的美观度分值有以下两个特点：</p>
<ul>
<li>图像越美观，模型得到美观度分数均值越大。</li>
<li>图像对的label应该与美观度分数一致。如，$u_i&gt;&gt;u_j$,那么标注图像 $i$ 好一点或者更好的人数应该更多。如果 $u_i=u_j$,则大部分人标注的 label 是 “一样好”。</li>
</ul>
<p>上面说到我们假设美观度分值服从正态分布，那么两个美观度分值的差也服从正态分布，记$x_{i}^{left}$,$x_{j}^{right}$分别表示图像对中左边图像美观度和右边图像美观度（左边和右边只是为了对应我们数据的标注label），他们美观度的差值 $\Delta X$ 服从 $N(x,u_i-u_j,\sigma_{1}^{2}+\sigma_{2}^{2})$。</p>
<p>然后我们定义4个可以学习的边界值 ${b_i}_{i=0}^{3}$，这4个boundaries将 x 轴分成5部分，也就将正态分布和x轴围成的区域分成了5个部分，这5个部分就对应了 我们设定的 五个label {左边更好，左边好一点，一样好，右边好一点，右边更好}，我们定义 $p_{i}^{j}，j={0,1,2,3}$表示第$i$对图像对被标注lable $j$ 的概率。$\Delta u_i $和 $\Delta \sigma_i$ 表示第$i$对图像对美观度分值差的均值和方差。于是我们有：</p>
<script type="math/tex; mode=display">
p_{i}^{j}=\displaystyle \int^{b_j}_{b_{j-1}}N(x;\Delta u_i,\Delta \sigma_i)dx</script><p>形象点可以看下图：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026175218691.png" alt="image-20231026175218691"></p>
<p>上面这张图显示了标签如何随着图像对的分数差值系统的变化。当右侧图像的平均得分远高于左侧图像时，得分差的分布将向右移动，导致标签“右侧更好”的概率更高。当左右之间的分数差异变小时，分布向左移动并导致标签发生变化。因此，当两个图像的分数相等时，中间桶中的面积最大，这意味着这对图像最有可能被标记为“一样好”。</p>
<p>至此我们可以计算出模型对第 $i$ 对图像对的美观度分值的差的分布 $p_i=(p_{i}^{0},p_{i}^{1},p_{i}^{2},p_{i}^{3},p_{i}^{4})$ ，而我们</p>
<p>图像对的真实label 为 $l=(n_{i}^{0},n_{i}^{1},n_{i}^{2},n_{i}^{3},n_{i}^{4})$, 因此我们可以定义一个对数最大似然损失函数：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026180559585.png" alt="image-20231026180559585"></p>
<p>从上面的过程可以看出，由于预先假设图像的美学评分服从正态分布，而正态分布可以由均值和方差两个参数控制，所以严格的说，DARN模型预测的不是美学评分，而是美学评分分布。<strong>在实际应用时，可以直接用均值作为该图像的美学评分</strong>。</p>
<h3 id="3、实验改进"><a href="#3、实验改进" class="headerlink" title="3、实验改进"></a>3、实验改进</h3><p>实验的过程中，根据我们的数据集和训练情况对DARN模型做了一些改进和调整。</p>
<p>1、 原始DARN模型中，deep CNN以及以后所有FC层，激活函数都是 Relu。缺少带有归一化能力的激活函数和norm操作，则很可能随着<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=W方差&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;83896463&quot;}">W方差</a>的增大，出现个别节点数值过大的问题，导致训练时loss很可能出现nan的情况。所以将第一个FC层的激活函数改为 tanh。</p>
<p>2、在计算均值之前加上softplus激活函数。使得最后推理的结果都是正的，且减小方差。模型中计算正态分布在某一个区间的积分，需要保证 $\sigma &gt;0$，才能让正态分布函数有意义，但是原始论文中：</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026182412236.png" alt="image-20231026182412236"></p>
<p>使用的是Relu激活函数，有可能训练得到 $\sigma = 0$。</p>
<p>3、训练中对边界值进行截断，防止训练中部分 $\Delta u$过大，产生nan值。</p>
<p>4、最后一层维度通过实验调整为 16。</p>
<p>5、边界值的初始化值对于模型训练影响很大，我们数据集下初始化值为（-0.75，-0.25， 0.25, 0.75）</p>
<p>6、deep CNN 由 resnet50 调整为 swin transformer。</p>
<h2 id="四、结果展示"><a href="#四、结果展示" class="headerlink" title="四、结果展示"></a>四、结果展示</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image2022-11-3_15-17-31.png" alt="image2022-11-3_15-17-31"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image2022-11-3_15-21-28.png" alt="image2022-11-3_15-21-28"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image2022-11-3_15-24-26.png" alt="image2022-11-3_15-19-28"></p>
<p>该美观度模型计算的特征应用在360图片搜索的排序中，离线ndcg指标和在线Ctr均得到了提升。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>【1】N. Murray, L. Marchesotti, and F. Perronnin. Ava: A large-scale database for aesthetic visual analysis. In Computer Vision and Pattern Recognition (CVPR), pages 2408–2415,Providence, RI, USA, 2012. IEEE.</p>
<p>【2】S. Kong, X. Shen, Z. Lin, R. Mech, and C. Fowlkes. Photo aesthetics ranking network with attributes and content adapta-tion. In European Conference on Computer Vision (ECCV),<br>page 662679, Amsterdam, The Netherlands, 2016. Springer.</p>
<p>【3】H. Talebi and P . Milanfar. Nima: Neural image assess-ment. IEEE Transactions on Image Processing, 27:3998–4011, 2017</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/10/NIMA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/10/NIMA/" class="post-title-link" itemprop="url">NIMA——Neural Image Assessment</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-10 21:00:00" itemprop="dateCreated datePublished" datetime="2022-05-10T21:00:00+08:00">2022-05-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-05-11 23:08:00" itemprop="dateModified" datetime="2022-05-11T23:08:00+08:00">2022-05-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="NIMA：Neural-Image-Assessment"><a href="#NIMA：Neural-Image-Assessment" class="headerlink" title="NIMA：Neural Image Assessment"></a>NIMA：Neural Image Assessment</h1><h2 id="1、introduction"><a href="#1、introduction" class="headerlink" title="1、introduction"></a>1、introduction</h2><ol>
<li><p>基于CNN的方法相比较早期基于手工制作的特征的做法，性能有显著的提升；</p>
</li>
<li><p>AVA 数据集，图片审美视觉分析的标准数据集；</p>
</li>
<li><p>深度CNN非常适合审美评估任务【1】【2】；他们的双列 CNN 由四个卷积层和两个全连接层组成，其输入是调整大小的图像和大小为 224 × 224 的裁剪窗口；</p>
</li>
<li><p>之前大多是通过分类或者回归的方法预测人类评估的分数；</p>
</li>
<li><p>kong【3】训练了一个基于 AlexNet 的 CNN 来学习两个输入图像的审美分数差异，从而间接优化排名相关性。</p>
</li>
<li><p>NIMA的目标是预测与人类评分的相关性，而不是将图片分类或者回归到平均分。提出了EMD loss，它显示了有序类分类的性能提升。实验也表明，这种方法也更准确地预测了平均分。</p>
</li>
</ol>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h2 id="2、proposed-method"><a href="#2、proposed-method" class="headerlink" title="2、proposed method"></a>2、proposed method</h2><p>模型建立在图片分类模型的结构基础上。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20231026114849878.png" alt="image-20231026114849878"></p>
<p>在训练阶段，输入图像被重新缩放为256<em>256，然后随机提取 224\</em>224的裁剪，这减少潜在的过度拟合问题。</p>
<h2 id="3、实验"><a href="#3、实验" class="headerlink" title="3、实验"></a>3、实验</h2><p>基线CNN权重通过在ImageNet上训练来初始化，最后一个全连接层是随机初始化。权重和偏置动量设置为0.9,基线CNN最后一层dropout应用0.75。基线CNN层和最后一个全连接层的学习率分别设置为3<em>10e-7,3\</em>10e-6。在基线CNN层上设置较低的学习率会导致使用sgd时更容易和更快优化。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>【1】Deep multi-patch aggregation network for image style, aesthetics, and quality estimation</p>
<p>【2】Rating image aesthetics using deep learning</p>
<p>【3】S. Kong, X. Shen, Z. Lin, R. Mech, and C. Fowlkes, “Photo aesthetics ranking network with attributes and content adaptation,” in European Conference on Computer Vision. Springer, 2016, pp. 662–679. 1, 2, 6, 7</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/19/Vit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/19/Vit/" class="post-title-link" itemprop="url">VIT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2022-04-19 21:00:00 / Modified: 23:08:00" itemprop="dateCreated datePublished" datetime="2022-04-19T21:00:00+08:00">2022-04-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ViT-Vision-Transformer"><a href="#ViT-Vision-Transformer" class="headerlink" title="ViT(Vision Transformer)"></a>ViT(Vision Transformer)</h1><h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><p>受到NLP领域中Transformer成功应用的启发，Vit算法中尝试将标准的Transformer结构直接应用于图像，并对整个图像分类流程进行最少的修改。</p>
<p><strong>Vit原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破Transformer的归纳偏置的限制，可以在下游任务中获得较好的迁移效果。</strong></p>
<p>但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）， $f(g(x)=g(f(x)))$ ，其中g代表卷积操作，f代表平移操作。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。</p>
<h2 id="模型结构与实现"><a href="#模型结构与实现" class="headerlink" title="模型结构与实现"></a>模型结构与实现</h2><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/ViT.png" alt="图1 ViT算法结构示意图"></p>
<h3 id="1-图像分块嵌入"><a href="#1-图像分块嵌入" class="headerlink" title="1.图像分块嵌入"></a>1.图像分块嵌入</h3><p>transformer的输入是一个二维矩阵，因此在ViT算法中，首先要做的是如何将一个$H\times W\times C$ 的三维图像转换为$N\times D$ 的二维输入。</p>
<p>iT中的具体实现方式为：将 $H×W×C$ 的图像，变为一个 $N×(P^2\times C)$ 的序列。这个序列可以看作是一系列展平的图像块，也就是将图像切分成小块后，再将其展平。该序列中一共包含了 $N=HW/P^2$ 个图像块，每个图像块的维度则是 $(P^2\times C)$。其中 $P$ 是图像块的大小，$C$ 是通道数量。经过如上变换，就可以将 $N$ 视为sequence的长度了。</p>
<p>但是，此时每个图像块的维度是 $(P2\times C)$，而我们实际需要的向量维度是 $D$，因此我们还需要对图像块进行 Embedding。这里 Embedding 的方式非常简单，只需要对每个 $(P^2\times C)$的图像块做一个线性变换，将维度压缩为 $D$ 即可。</p>
<h3 id="2-多头注意力"><a href="#2-多头注意力" class="headerlink" title="2.多头注意力"></a>2.多头注意力</h3><p>将图像转换成序列之后，就可以将其输入到 Transformer 中结构中进行特征提取了。</p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/Multi-head_Attention.jpg" alt="图4 多头注意力"></p>
<h3 id="3-MLP"><a href="#3-MLP" class="headerlink" title="3.MLP"></a>3.MLP</h3><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/MLP.png" alt="图7 多层感知机"></p>
<h3 id="4-DropPath"><a href="#4-DropPath" class="headerlink" title="4.DropPath"></a>4.DropPath</h3><p>除了以上重要模块意外，代码实现过程中还使用了DropPath（Stochastic Depth）来代替传统的Dropout结构，DropPath可以理解为一种特殊的 Dropout。其作用是在训练过程中随机丢弃子图层（randomly drop a subset of layers），而在预测时正常使用完整的 Graph。</p>
<p><strong>参考文献</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/11/resnet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jie Chu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CJ blog">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CJ blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/11/resnet/" class="post-title-link" itemprop="url">resnet解析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-04-11 21:03:00" itemprop="dateCreated datePublished" datetime="2022-04-11T21:03:00+08:00">2022-04-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-11 23:03:00" itemprop="dateModified" datetime="2023-04-11T23:03:00+08:00">2023-04-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="resnet"><a href="#resnet" class="headerlink" title="resnet"></a>resnet</h1><h3 id="1-1-batch-normalization原理"><a href="#1-1-batch-normalization原理" class="headerlink" title="1.1 batch normalization原理"></a>1.1 batch normalization原理</h3><p>提出背景：在训练层数较深的神经网络时，参数的变化导致每一层的输入分布会发生变化，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难——internal Covariate Shift</p>
<p>问题：上层网络需要不停调整来适应输入分布的变化，导致网络学习速度的降低；网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度。</p>
<p>减缓internal covariate shift的方法：</p>
<ul>
<li>白化：对输入数据分布进行变换，具有相同的均值和方差；</li>
<li><strong>batch normalization</strong>：由于白化过程计算成本太高，且改变了网络每一层的分布。</li>
</ul>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701153501142.png" alt="image-20220701153501142"></p>
<p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701153552148.png" alt="image-20220701153552148"></p>
<h3 id="1-2-resnet-结构"><a href="#1-2-resnet-结构" class="headerlink" title="1.2  resnet 结构"></a>1.2  resnet 结构</h3><p><img src="https://vino-1316924433.cos.ap-beijing.myqcloud.com/image-20220701154852179.png" alt="image-20220701154852179"></p>
<p><strong>Stage 0</strong></p>
<ul>
<li><code>(3,224,224)</code>指输入<code>INPUT</code>的通道数(channel)、高(height)和宽(width)，即<code>(C,H,W)</code>。现假设输入的高度和宽度相等，所以用<code>(C,W,W)</code>表示。</li>
<li>该stage中第1层包括3个先后操作</li>
</ul>
<ol>
<li><code>CONV</code><br> <code>CONV</code>是卷积（Convolution）的缩写，<code>7×7</code>指卷积核大小，<code>64</code>指<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=卷积核&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">卷积核</a>的数量（即该卷积层输出的通道数），<code>/2</code>指卷积核的步长为2。</li>
<li><code>BN</code><br> <code>BN</code>是Batch Normalization的缩写，即常说的BN层。 </li>
<li><code>RELU</code><br> <code>RELU</code>指ReLU激活函数。</li>
</ol>
<ul>
<li><p>该stage中第2层为<code>MAXPOOL</code>，即最大池化层，其kernel大小为<code>3×3</code>、步长为<code>2</code>。</p>
</li>
<li><p><code>(64,56,56)</code>是该stage输出的通道数(channel)、高(height)和宽(width)，其中<code>64</code>等于该stage第1层卷积层中卷积核的数量，<code>56</code>等于<code>224/2/2</code>（步长为2会使输入尺寸减半）。 </p>
</li>
</ul>
<p>总体来讲，在<em>Stage 0</em>中，形状为<code>(3,224,224)</code>的输入先后经过卷积层、<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=BN层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">BN层</a>、ReLU激活函数、<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=MaxPooling层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;353235794&quot;}">MaxPooling层</a>得到了形状为<code>(64,56,56)</code>的输出。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Jie Chu</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
